
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page not found!Please use the search bar from the bottom left or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "Hello there... It's our pleasure to meet you!",
    "body": "The primary objective of thoughtful works is to encourage the exchange of ideas among Rust developers, who each possess a unique set of skills and expertise, through virtual collaboration. We explore an extensive range of concepts here. We strive to present information in the clearest and most straightforward manner possible, covering all the grounds needed to grasp it completely. Our commitment is to provide a clutter-free experience, free of advertisements on the blogs, and without requiring a subscription—any information that is beneficial must be invariably democratized for all, thereby enabling readers to fully understand the subject matter at hand without having to rely on additional sources for further clarification. Why Scala?: Scala is a powerful and versatile programming language that combines object-oriented and functional programming paradigms. Here are the strengths of Scala:  Expressiveness: Scala’s syntax is more expressive than Java’s, enabling us to write the same functionality in a clear, concise, and understandable manner. Expressiveness in a programming language refers to its ability to facilitate the expression of the programmer’s intent with the least amount of verbosity possible.  Functional Programming: Adopting functional programming can bring about improved code quality, maintainability, and scalability. Scala’s combination of features makes it well-suited for a wide range of applications, from small scripts to large-scale, particularly distributed systems. Feel free to get in touch with us if you like reading our posts, have feedback for us, or would like to contribute. Happy learning! Senthil NayaganFounder and content creator, towardsdata. dev and rustinaction. dev. Email: hello@thoughtfulworks. devTwitter: thoughtfulwxInstagram: thoughtful. worksGitHub: thoughtfulworks "
    }, {
    "id": 2,
    "url": "http://localhost:4000/archive",
    "title": "Archive",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/authors",
    "title": "Meet our contributors!",
    "body": "                                                                   Senthil Nayagan:                   I am a Data Engineer by profession and a Rustacean by interest. Founder and Content Creator, towardsdata. dev.                        "
    }, {
    "id": 4,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 5,
    "url": "http://localhost:4000/comment-policy.html",
    "title": "Comment Policy",
    "body": "Written by Disqus. Refer the official comment policy here. This comment policy is a place for the author to set the tone of his/her community and lay a foundation for what is acceptable. They also provide a reference for making moderation decisions. If the author ever needs to ban a member or remove a comment, the author will be able to refer those members to the pre-determined community guidelines that they may have failed to follow. What to consider: When developing community guidelines, it is good to consider the type of community the author is trying to cultivate. Guidelines can cover topics like:  Etiquette - “Be polite and stay on topic”. “No self-promotion”. “Don’t flag/downvote comments because you disagree with a user. ” Expectations - “Your comment will be removed for reason X, Y, and Z” Privacy - “Don’t post personal information” Moderation Settings - “Comments containing links are pre-moderated”. “Discussions auto-close after 7 days”.  Anything else that you’d like members to keep in mind while commenting"
    }, {
    "id": 6,
    "url": "http://localhost:4000/contact",
    "title": "Contact us",
    "body": "    Please feel free to send your message to us. We will respond as soon as we can.                       "
    }, {
    "id": 7,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "                                                                                               How To Set SLA in Apache Airflow              :       Apache Airflow enables us to schedule tasks as code. In Airflow, a SLA determines the maximum completion time for a task or DAG. Note that SLAs are established based on the DAG execution date, not the task start time.                                                             airflow                                  service-level-agreement                                  sla                                  workflow-engine                                                                                                                                         Introduction to gRPC              :       gRPC is an open-source, high-performance RPC framework that can run in any environment. gRPC builds on HTTP/2 protocol and the protobuf message-encoding protocol to provide high performance, low-bandwidth communication between applications and services.                                                             grpc                                  inter-process-communication                                  remote-procedure-call                                  rpc                                                                                                                                         Terraform Basics              :       Terraform is an open source infrastructure-as-code tool that allows us to programmatically provision the physical resources required for an application to run.                                                             iac                                  infrastructure-as-code                                  terraform                                                                                                                                         Singleton Pattern              :       Design Patterns and Coding Principles                                                            coding-principles                                  design-patterns                                  singleton-pattern                                                                                                                                         Anti-Pattern              :       Anti-patterns at first seem to be quick and reasonable, they typically have adverse effects in the future. They are design and code smells. It affects our software badly and adds technical debt. We should avoid them at all costs.                                                             anti-pattern                                  design-patterns                                                                                                                                         Apache Pinot joins hands with Kafka and Presto to provide low-latency, high-throughput user-facing analytics              :       Apache Pinot is a real-time, distributed OLAP datastore that was built for low-latency, high-throughput analytics, making it perfect for user-facing analytical workloads. Pinot joins hands with Kafka and Presto to provide user-facing analytics.                                                             apache-pinot                                  olap                                  olap-datastore                                  pinot                                                                                                                                         Data Product vs. Data as a Product              :       A data product is not the same as data as a product. A data product aids the accomplishment of the product's goal by using the data, whereas in data as a product, the data itself is seen as the actual product.                                                             data-as-a-product                                  data-engineering                                  data-management                                  data-product                                                                                                                                         Data Catalog              :       A data catalog is an ordered inventory of an organization's data assets that makes it easy to find the most relevant data quickly.                                                             big-data                                  data-catalog                                  data-engineering                                  data-inventory                                  data-lake                                                                                                                                         An Introduction to Algorithms and Data Structures              :       An algorithm is a series of instructions in a particular order for performing a specific task.                                                             algorithms                                  algorithms-and-data-structures                                  data-structures                                                                                                                                         AWS Command Line Interface (AWS CLI)              :       AWS CLI is an open-source tool that allows us to interact with AWS services using command-line shell commands.                                                             aws                                  aws-cli                                                                                                                                         Overview of Amazon EMR              :       Amazon EMR is a managed cluster platform that makes it easier to run big data frameworks like Apache Hadoop and Apache Spark on AWS to process and analyze huge amounts of data.                                                             amazon-emr                                  aws                                                                                                                                         Data Governance              :                                                                   big-data                                  data-engineering                                  data-goverance                                  data-security                                                 &laquo;          1        2       &raquo; "
    }, {
    "id": 8,
    "url": "http://localhost:4000/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "Your privacy really matters! {{ site. name }} takes your privacy seriously. To better protect your privacy, we provide this privacy policy notice explaining the way your personal information is collected and used. Please do not hesitate to contact us if you have further concerns or seek extra information about our Privacy Policy. Cookies: This website DOES NOT use cookies. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Consent: By using our website, you consent to and accept our Privacy Policy and its terms and conditions. "
    }, {
    "id": 9,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 10,
    "url": "http://localhost:4000/page2/",
    "title": "Home",
    "body": "  {% for post in paginator. posts %}    {% include postbox. html %}  {% endfor %}  {% include pagination. html %}"
    }, {
    "id": 11,
    "url": "http://localhost:4000/orchestration/2024/airflow-setting-sla",
    "title": "How To Set SLA in Apache Airflow",
    "body": "2024/01/16 - A quick overview of AirflowApache Airflow is, at its core, an open-source batch task scheduler. It’s a platform for programmatically authoring (creating), scheduling, and monitoring workflows via Directed Acyclic Graphs (DAGs). Airflow is distinguished from other schedulers primarily by its ability to schedule tasks as code. It’s gaining popularity, especially among developers, because it’s written in Python and it’s easy to create code-based orchestration with it. To put it all together, Airflow is a batch-oriented open-source framework that enables us to build data pipelines and monitor data workflows efficiently. How to set SLA in AirflowSLA stands for service-level agreement. It’s a contract between a service provider and its customer that outlines the level of service expected by the customer and guaranteed by the vendor.   In the context of Airflow, an SLA is the maximum amount of time a task or a DAG should take to complete. It serves to notify us if our DAG or tasks are taking longer than expected to complete.  Note: SLAs are set to the DAG execution date rather than the task start time. SLA missesAn SLA miss occurs if a task or DAG is not finished within the allotted time for the SLA. When SLAs are breached, Airflow can perform the following:  Sends an email alert notification Logs the event in the database - the event is also logged in the database and made available in the web UI. The web UI gives us broad details about which task failed to meet the SLA and when. Additionally, it shows if an email was sent after the SLA failed.  Invokes the callback function via sla_miss_callback.  Note: It should be noted that only scheduled tasks will be checked against the SLA. An SLA miss won’t be triggered by manually initiated tasks! Defining SLAsThere are several ways to define an SLA that include:  Defining SLA on the task itself Defining SLA on DAG levelDefining SLA on the task itself: Note that defining SLAs for a task is optional. As a result, in a DAG with several tasks, some tasks may have SLAs and others may not. To set an SLA for a task, pass a datetime. timedelta object to the task/operator’s sla parameter as shown below: from airflow import DAGfrom airflow. operators. bash_operator import BashOperatorfrom datetime import datetime, timedeltawith DAG(&#39;setting_sla&#39;,     start_date=datetime(2002, 1, 1),     schedule_interval=&#39;@daily&#39;,    catchup=False) as dag:      bash_task = BashOperator(    task_id=&#39;bash_task&#39;,    bash_command=&#39;sleep 10&#39;,    sla=timedelta(seconds=5) # SLA for the task  ) When a task exceeds its SLA, it is not cancelled. It’s free to continue running till the end. The timedelta object is found in the Python’s datetime library. It takes the following arguments:  microseconds (not applicable to Airflow) milliseconds (not applicable to Airflow) seconds minutes hours days weeksNote that milliseconds and microseconds available, but those wouldn’t apply to Airflow. Cancelling or failing the task: Timeouts can help cancel or fail a task if it takes longer than the allotted time. If we want to cancel a task after a certain runtime is reached, we can set the execution_timeout attribute to a datetime. timedelta value that represents the longest permitted duration. Having said that, the maximum duration permitted for each execution is managed by the property execution_timeout. If it’s breached, the task times out and AirflowTaskTimeout is raised. Implementing our own logicWe can use callbacks to trigger our own logic if we choose to do so. In this case, we can use sla_miss_callback that will be called when the SLA is not met.  Only one callback function is permitted for tasks or a DAG. The function signature of an sla_miss_callback requires the following five parameters:  dag task_list blocking_task_list slas blocking_tisLet’s define a callback method: def _sla_missied_take_action(*args, **kwargs):  logger. info(args)Pass the callback method to DAG as shown below: with DAG('setting_sla',   start_date=datetime(2002, 1, 1),  default_args=default_args,  schedule_interval='@daily',  sla_miss_callback=_sla_missied_take_action # callback function  catchup=False) as dag:Disabling SLAIf we want to disable SLA checking entirely, we can set check_slas = False in Airflow’s [core] configuration. [core]check_slas = False"
    }, {
    "id": 12,
    "url": "http://localhost:4000/rpc/2024/introduction-to-grpc",
    "title": "Introduction to gRPC",
    "body": "2024/01/16 - OverviewgRPC is an open-source, high-performance Remote Procedure Call (RPC) framework that can run in any environment. With pluggable support for load balancing, tracing, health checking, and authentication, it can efficiently connect distributed services in and across data centers. gRPC builds on HTTP/2 protocol and the protobuf (protocol buffers) message-encoding protocol to provide high performance, low-bandwidth communication between applications and services. gRPC can use protocol buffers as both its Interface Definition/Description Language (IDL) and as the format for exchanging messages. It can generate code for both the server and the client in the most common programming languages and platforms, such as. NET, Java, Python, Node. js, Go, and C++. With gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object. This makes it easier to create distributed applications and services. As with most RPC systems, gRPC is based on the idea of defining a service by specifying the methods that can be called remotely, along with their parameters and return types. On the server side, the server implements this interface and runs a gRPC server to handle calls from clients. On the client side, the client has a stub that provides the same methods as the server.                    Figure 1: gRPC - Communication between clients and server. (Image courtesy: grpc. io)      gRPC clients and servers can run and talk to each other in any platform (Linux, macos, etc. ) and can be written in any of gRPC’s supported languages. For example, we can easily create a gRPC server in Java with clients in Go, Python, or Ruby. Also, the latest Google APIs will have gRPC versions of their interfaces, which will make it easy to add Google features to our applications. Protocol buffersLet’s talk about what protocol buffers (also called protobuf) are and how to use them. Protocol buffers is an open-source protocol developed by Google to serialize structured data in a way that works in different languages (language-agnostic) and on different platforms. When working with protocol buffers, the first step is to define the structure of the data that we want to serialize in a proto file. Proto file is an ordinary text file with a . proto extension. Protocol buffer data is organized into messages, where each message is a small logical record of information with a set of name-value pairs called fields. Here’s a simple proto definition example: message Person { string name = 1; int32 id = 2; bool is_married = 3;}Once we’ve specified our data structures (shown above), we use the protocol buffer compiler protoc to generate data access classes in our preferred language from our proto definition. These provide simple accessors (name()) and mutators (set_name()) for each field, as well as methods to serialize/parse the whole structure to/from raw bytes. For instance, if our chosen language is Java or Python, running the compiler on the example above will generate a class called Person. We can then use this class in our application to populate, serialize, and retrieve Person protocol buffer messages. gRPC usecasesEveryone agrees that gRPC is the best way for internal microservices to talk to each other because it is fast and can be used in different gRPC supported languages. Microservices must agree on the API, data format, error patterns, load balancing, and other things in order to exchange data. Since gRPC lets us describe a service contract in a binary format, programmers can have a standard way to describe these contracts that works with any language and ensures interoperability. Benefits of gRPCTODO Weakness of gRPCTODO Frequently asked questions (FAQ)What is “g” in gRPC?: We know that Google developed gRPC, but the “g” at the beginning of gRPC doesn’t stand for Google. At first, the “g” in “gRPC” was a recursive acronym (an acronym that refers to itself) that stood for “gRPC. ” Later, each release meant something different. In version 1. 49, it stands for “gamma,” and in version 1. 50, it stands for “galley. ” What is protocol buffers?: Protocol buffers (aka protobuf) is an open-source way to serialize structured data that works across languages and platforms and can be expanded. Google developed protocol buffers, which are widely used at Google for storing and exchanging all kinds of structured information. It is the foundation for a custom remote procedure call (RPC) system that Google uses for almost all inter-machine (machine-to-machine) communication. Protocol Buffers are similar to the Apache Thrift, Ion (created by Amazon), or Microsoft Bond protocols. What is “stub” in distributed computing?: In distributed computing, a stub is a piece of code that is used to convert parameters passed between client and server during a remote procedure call (RPC). An RPC allows a client or local computer to remotely call procedures on a server or different computer. Since the client and server use different address spaces, parameters used in a function (procedure) call have to be converted. Otherwise, the values of those parameters couldn’t be used because pointers to parameters in one computer’s memory would point to different data on the other computer. The client and server may also use different data representations, even for simple parameters (e. g. , big-endian versus little-endian for integers). Stubs do this conversion so that the remote server computer perceives the RPC as a local function call. Stub libraries must be installed on both the client and server side. Client stubs (sometimes called proxy) convert (marshalling) parameters used in function calls and reconvert the result obtained from the server after execution of the function. Server stubs, on the other hand, reconvert parameters passed by clients and convert results back after function execution. Stubs are generated either manually or automatically. Automatic stub generation (more commonly used) uses Interface Definition Language (IDL) to define client and server interfaces. What does address space mean?: An address space is a range of valid memory addresses that a program or process can use. That is, a program or process can access the memory. The memory can be physical or virtual, and it is used to store data and run instructions. A memory management technique called “virtual memory” can be used to make the size of an address space bigger than the size of physical memory. A virtual memory, which is also called a page file, is a physical file on the hard drive that acts like extra RAM or a RAM module. Thus, an address space consists of both physical memory and virtual memory. What is endianness?: Texts in different languages are read in different orders. For example, English is read from left to right, but Arabic is read from right to left. In the same way, in computing, endianness is a way to specify the order in which bytes of a word are stored in memory. If one computer reads bytes from left to right and another from right to left, it will be hard for them to talk to each other. Endianness is represented two ways:  Big-endian (BE) Little-endian (LE)TODO What is interface definition language (IDL)?: An interface description or definition language (IDL) is a language that is used to define the interface between a client and server process in a distributed system. It makes it possible for two programs written in different languages to talk to each other. IDLs describe an interface in a way that doesn’t depend on the language being used (language-independent way). This lets software components that don’t use the same language talk to each other. How HTTP/2 differs from HTTP/1. 1 in terms of performance?: At a high level, HTTP/2:  It’s binary, instead of textual     Binary protocols are more efficient to parse   It’s more compact on the wire    It’s fully multiplexed, instead of ordered and blocking     Unlike HTTP/1. 1, HTTP/2 allows multiple request and response messages to be in flight at the same time    Uses one connection for parallelism     With HTTP/1, browsers open between four and eight connections per origin and each connection will start a flood of data in the response   HTTP/2 allows a client to use just one connection per origin to load a page    Uses header compression to reduce overhead Allows servers to push responses proactively into client caches     Server push potentially allows the server to avoid this round trip of delay by “pushing” the responses it thinks the client will need into its cache   "
    }, {
    "id": 13,
    "url": "http://localhost:4000/terraform/2024/terraform-basics",
    "title": "Terraform Basics",
    "body": "2024/01/16 - Introducing infrastructure as code (IaC)Infrastructure as Code (IaC) refers to the process of managing and provisioning infrastructure using code rather than manual processes. When we talk about “infrastructure as code,” we mean that we manage our IT infrastructure with code in the form of configuration files. Configuration files that describe our infrastructure are produced by IaC. Changes and sharing of configurations are made simpler as a result.  An IaC process produces the same environment every time it deploys, just as the same source code always generates the same binary. It’s a version control: Version control is an important aspect of IaC, and our configuration files, like any other software source code file, can be under source control. The difficulty of manually managing infrastructure: Managing IT infrastructure was traditionally a manual process where the deployment team would physically install and configure servers. Unsurprisingly, this manual process would frequently result in several issues. Uses “declarative” definition files: IaC uses declarative definition files, which are simply configuration files. It’s worth noting that in the declarative approach, we only specify the “what” but not the “how” in our definition files. A definition file specifies the parts and settings required by an environment, but it does not always specify how to obtain those settings. For example, the definition file may specify the required server version and configuration but not the process for installing and configuring the server—we specify the “what” part but not the “how” part. Benefits of infrastructure as code: Because IaC is text-based, we can easily edit, copy, version, and distribute it. Speed: The first major benefit of IaC is its speed. We can quickly set up your entire infrastructure by running a script with infrastructure as code. That is something we can do for any environment, from development to production. Consistency with reduced errors: Manual processes are prone to errors. Infrastructure as a code solves this problem by making the configuration files the single source of truth. That way, we can be certain that the same configurations will be deployed repeatedly and without error. Traceability with the help of a version control system: We have full traceability of the changes made to each configuration because we can version IaC configuration files like any other source code file, which allows us to save time troubleshooting the problem. Introduction to TerraformTerraform is an open-source infrastructure as a code tool from HashiCorp. It enables us to define both cloud and on-premises resources in human-readable declarative definition files (aka configuration files) that can be versioned, reused, and shared. These definition files contain the steps required to provision and maintain our infrastructure. We can edit, review, and version these definition files just like code. Terraform can create infrastructure on a variety of cloud platforms such as AWS, Azure, Google Cloud, etc. Download and install: Terraform distribution consists of a single binary file, which can be obtained for free from Hashicorp’s download page. There are no dependencies, so we can just copy the executable binary to a folder of our choice and run it from there. HashiCorp also offers a managed solution known as Terraform Cloud. After we finish the installation, we can run the following command to ensure that everything is working properly: $ terraform -vTerraform v1. 2. 7on darwin_amd64Terraform core concepts: Below are the core Terraform concepts: Blocks: Blocks are containers for other content, and they typically represent the configuration of an object, such as a resource. The block body is delimited by the { and } characters. Blocks have:  A block type Zero or more labels A body that contains any number of arguments and nested blocks. Top-level blocks in a configuration file control the majority of Terraform’s features. Block syntax: This is the Terraform block syntax: &lt;BLOCK TYPE&gt;  &lt;BLOCK NAME&gt;   &lt;BLOCK LABEL&gt;  { # Block body &lt;IDENTIFIER&gt; = &lt;EXPRESSION&gt; # Argument}These are some of the Terraform blocks:  providers resources variable output local module dataBlock example: resource  aws_instance   demo  { ami =  ami-1fc93908a9403  network_interface {  # . . .  }}In the above example, a block has a type as resource. The resource block type in our case expects two labels: aws_instance and demo. The Terraform language uses a limited number of top-level block types, which are blocks that can appear outside of any other block in a configuration file. The majority of Terraform’s features, such as resources, input variables, output values, data sources, and so on, are implemented as top-level blocks. Providers: Terraform makes use of providers to connect the Terraform engine to the supported cloud platform. Other than a cloud platform, other things can be considered a provider, such as platform-as-a-service (PaaS) (e. g. , Kubernetes) and other software-as-a-service (SaaS). It’s a Terraform plugin that serves as a translation layer, allowing Terraform to communicate with a variety of cloud providers, databases, and services. Terraform’s most popular providers, including major cloud providers:  AWS Azure Google Cloud Platform Kubernetes Oracle Cloud InfrastructureEach provider adds a set of resource types and/or data sources that Terraform can manage. Every resource type is implemented by a provider; Terraform cannot manage any infrastructure without providers. Where do providers come from?: The Terraform Registry is the primary directory of publicly available Terraform providers, hosting providers for the majority of major infrastructure platforms. Provider syntax: The general syntax is as follows: resource  &lt;PROVIDER&gt;_&lt;TYPE&gt;   &lt;NAME&gt;  { [CONFIG. . . ]}The PROVIDER above is the name of the provider (e. g. , aws). The TYPE is the type of resource to create in that provider (e. g. , instance, which represents an EC2 instance). The NAME is the identifier we can use throughout the Terraform code to refer to this resource. The CONFIG consists of one or more arguments that are specific to that resource (e. g. , ami = “ami-1fc93908a9403”). For instance, the aws_instance resource has many different arguments, but for now, we only need to set the following:  ami: The Amazon Machine Image (AMI) to run on the EC2 instance.  instance_type: The type of EC2 Instance that will be used. Each EC2 Instance type has a different amount of CPU, memory, disk space, and networking capacity. We use t2. micro instance. T2 instances are available to use in the AWS Free Tier, which includes 750 hours of Linux and Windows t2. micro instances each month for one year for new AWS customers. An example of a simple configuration is as follows: resource  aws_instance   demo  { ami =  ami-1fc93908a9403  instance_type =  t2. micro }Variables and outputs: Variables in Terraform are an excellent way to define centrally managed, reusable values. Information contained in Terraform variables is stored independently of deployment plans. Terraform supports multiple variable formats. Based on their usage, the variables are generally divided into:  Input variables - They serve as parameters for a Terraform module, allowing users to modify behavior without having to edit the source code.  Output values - They serve as return values for a Terraform module.  Local values - They are a convenience feature for assigning a short name to an expression. Modules: Modules are small, reusable Terraform configurations that allow us to manage a collection of related resources as if they were one. A module serves as a container for multiple resources that are used together. To put it simply, a module is a set of Terraform configuration files (. tf files) in a single directory as shown below: . ├── main. tf├── outputes. tf└── variables. tfEven a single Terraform configuration file (. tf) in a directory becomes a module. When Terraform commands are executed directly from such a directory, the contents of that directory are considered the root module. Having said that, a module is a method for packaging and reusing configurations of resources. What does a module do?: A module enables us to group related resources together and reuse them in the future, multiple times. Let’s assume we need to repeatedly create a virtual machine with a set of resources such as IP, firewall, storage, etc. This is where modules come in handy; we don’t want to repeatedly write the same configuration code over and over again. The below example demonstrates how our “server” module may be invoked. Here, we create two instances of “server” using a single set of configurations: module  server  { count = 2 source =  . /module/server  . . .  . . . }State: Terraform must keep track of what infrastructure it creates in a terraform. tfstate Terraform state file-local state stored on the provisioning machine. Terraform uses this state to map real-world resources to our configuration. Terraform state is stored locally by default, but it can also be stored remotely, which is preferable in a team environment. This state file contains a custom JSON format that records a mapping from the Terraform resources in our templates to their real-world representation.  Avoid directly manipulating the state file! The state file is meant only for internal use within Terraform. This implies that we should never manually edit the Terraform state files or write code that directly reads them. If for some reason we need to manipulate the state file, use the terraform import or terraform state commands. Terraform makes use of this local state to create plans and modify our infrastructure. Terraform performs a refresh prior to any operation to update the state with the real infrastructure. Remote state: When working with Terraform in a team, using a local file complicates Terraform usage because each user must ensure that they always have the most recent state data before running Terraform and that no one else runs Terraform at the same time. A remote state comes to our aid. Terraform writes the state data to a remote data store, which can then be shared by all team members. Terraform can store state in a variety of storage platforms, including, but not limited to:  Terraform Cloud HashiCorp Consul Amazon S3 Azure Blob Storage Google Cloud Storage Alibaba Cloud OSSRemote state is implemented by a backend or by Terraform Cloud, both of which can be configured in the root module of our configuration. Data sources: Terraform can use data sources to get information about resources that are set up outside of Terraform and use that information to set up Terraform resources.  It’s a way of getting data from the outside world and making it available to your Terraform configuration. A data source is accessed through a special type of resource called a data resource, which is declared using a data block. Data source example: data  azurerm_role_definition   example  { name =  Developer }resource  azurerm_role_assignment   example  { role_definition_id = data. azurerm_role_definition. example. id}In the above example, we have defined two blocks: data and resource blocks. We are sending the data from the data source directly into a role assignment resource. Refreshing data sources: By default, before creating a plan, Terraform will refresh all data sources. Additionally, we can explicitly refresh all data sources by executing terraform refresh command. Resources: The most important element of the Terraform language is resources. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, and so on. Resource syntax: resource  aws_instance   web  { ami      =  ami-a1b2c3d4  instance_type =  t2. micro }In the above example, a resource block declares a resource of a given type (aws_instance) with a given local name (web). The name is used to refer to this resource from within the same Terraform module, but it has no significance outside of the module. The combination of the resource type and name serves as an identifier for a given resource and must therefore be unique within a module. Command line interface: We can use the Terraform command line interface (CLI) to manage infrastructure, and interact with Terraform state, providers, configuration files, and Terraform Cloud. Terraform CLI commands: Following shows some of the Terraform CLIs:  terraform version terraform init [option] - This command is used to initialize a working directory containing Terraform configuration files and install the required plugin. Policy libraries: A library of policies that can be used within Terraform Cloud to accelerate our adoption of policy as code. HashiCorp Configuration Language (HCL): The HashiCorp Configuration Language (HCL) is a one-of-a-kind configuration language created by HashiCorp. HCL was created to work with HashiCorp tools, specifically Terraform. The Terraform language’s primary function is to declare resources, which represent infrastructure artifacts. Configuration syntax: HCL is intended to support multiple syntaxes for configuration, but the native syntax is the primary format and is optimized for human authoring and maintenance rather than machine generation. The native configuration syntax of HCL is relatively easy for humans to read and write. The file extension of the configuration file written in HCL syntax is . tf. The Terraform language also allows us to define resource dependencies. A configuration can consist of multiple files and directories. Declarative configuration: As previously stated, this configuration language is declarative, which means that we only describe the infrastructure we want, and Terraform will figure out how to build it. In other words, it means that we describe our intended goals (what to be done) rather than the steps (how to be done) to achieve those goals. The configuration files we write in the Terraform configuration language tell Terraform:  What plugins to install? What infrastructure to create? What data to fetch?Basic configuration elements: The syntax of the Terraform language consists of only a few basic elements:  Blocks Arguments ExpressionsBlocks: Refer above for more details. Arguments: Arguments assign a value to a name. They can be found within blocks. image_id =  abc123 The identifier before the equals sign is the argument name (image_id in our case), and the expression after the equals sign is the argument’s value (abc123 in our case). Expressions: Expressions represent a value in one of two ways: directly or by referencing and combining other values. CDK for Terraform (CDKTF): The Cloud Development Kit for Terraform (CDKTF) enables us to define and provision infrastructure using familiar programming languages. You might wonder why we need CDKTF when HCL can do the same thing. As mentioned above, CDKTF enables us to define and provision infrastructure using familiar programming languages other than HCL or JSON syntax. At the time of writing this post, Terraform supports the following programming languages:  Typescript Python Java C# GoCDKTF uses the Cloud Development Kit from AWS, which provides a set of language-native frameworks for defining infrastructure, as well as adapters that allow underlying provisioning tools to use those definitions. Install CDKTF: We can install CDKTF with npm on most operating systems. We can also install CDKTF with Homebrew on MacOS. Install CDKTF with npm: To install the most recent stable release of CDKTF, use npm install with the @latest tag as shown below: npm install --global cdktf-cli@latestInstall CDKTF using Homebrew: We can also use the Homebrew package manager to install CDKTF on MacOS systems. brew install cdktfVerify the CDKTF installation: Verify that you have CDKTF installed by running the cdktf help command to show the available subcommands. How does CDK for Terraform work?: On a high level, we will follow these steps:    Step 1 - Define infrastructure: As a first step, define the infrastructure we want to provision on one or more providers using any of the supported programming languages. CDKTF works by translating configurations defined in an imperative programming language to JSON configuration files for Terraform.     Step 2 - Deploy: Use cdktf CLI commands to provision infrastructure with Terraform or synthesize (translate to other form) our code into a JSON configuration file that others can use with Terraform directly.  CDKTF to HCL: The cdktf synth command synthesizes (generate code/configuration) Terraform configuration for the given app. CDKTF stores the synthesized configuration in the cdktf. out directory, unless you use the --output flag to specify a different location. The output folder is ephemeral (short lived) and might be erased for each synth that we run manually or that happens automatically when we run deploy, diff, or destroy. HCL to CDKTF: Use the cdktf convert command to translate Terraform configuration written in HCL to the equivalent configuration in our preferred language. Convert a local file: cat main. tf | cdktf convert &gt; imported. tsCDKTF CLI commands: The following are some of the CDKTF CLI commands:  cdktf versionHow does Terraform work?As stated above, Terraform uses definition files to create and manage resources on cloud or on-premises platforms, as well as other services via their APIs. Terraform’s primary function is to create, modify, and destroy infrastructure resources as described in a Terraform configuration. Definition or configuration files: Terraform requires infrastructure configuration or definition files written in either HashiCorp Configuration Language (HCL) or JSON syntax. Working directories: Terraform expects to be invoked from a working directory containing Terraform language configuration files. Terraform uses configuration content from this working directory to store settings, cached plugins and modules, and occasionally state data. Working directory contents: A Terraform working directory typically contains:  A Terraform configuration (. tf files) that defines the resources that Terraform should manage. This configuration is likely to change over time.  A hidden . terraform directory, which Terraform uses to manage cached provider plugins and modules, record which workspace is currently active, and record the last known backend configuration in case it needs to migrate state on the next run. This directory is automatically managed by Terraform, and is created during initialization.  State data, if the configuration uses the default local backend. This is managed by Terraform in a terraform. tfstate file (if the directory only uses the default workspace) or a terraform. tfstate. d directory (if the directory uses multiple workspaces). The following shows all the files in a working directory, including the hidden ones: $ tree -a├── . terraform│   └── providers│     └── registry. terraform. io│       └── hashicorp│         └── local│           └── 1. 4. 0│             └── darwin_amd64│               └── terraform-provider-local_v1. 4. 0_x4├── . terraform. lock. hcl├── hello. txt├── main. tf└── terraform. tfstateProvisioning infrastructure with Terraform: The provisioning workflow (Terraform’s life cycle) in Terraform is based on the following commands:  init plan apply destroy                   Terraform life cycle.       Init: Init command initializes the working directories. A working directory must be initialized before Terraform can perform any operations on it, such as provisioning infrastructure or changing state. Why do we need to initialize the working directory, and what happens during initialization? The terraform binary contains the basic functionality of Terraform, but it does not include the code for any of the providers (e. g. , the AWS provider, Azure provider, GCP provider, and so on), so when we first start using Terraform, we must run the terraform init command to instruct Terraform to scan the code (configuration file), determine which providers we are using, and download the code from the Terraform Registry. The provider code is downloaded by default into a . terraform directory.  Note: It’s safe to run init multiple times because the command is idempotent. After initialization, we will be able to perform other commands, like terraform plan and terraform apply. Plan: After we’ve initialized the working directory, we’ll use the plan command to see what actions Terraform will take to create our resources. This step works similar to the “dry run” feature found in other build systems. This is an excellent way to double-check our changes before releasing them. The plan command’s output is similar to the diff command’s output:  Resources with a plus sign (+) will be created.  Resources with a minus sign (-) will be deleted.  Resources with a tilde sign (~) will be modified in-place. $ terraform planTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + createTerraform will perform the following actions: # local_file. hello will be created + resource  local_file   hello  {   + content       =  Hello, Terraform    + directory_permission =  0777    + file_permission   =  0777    + filename       =  hello. txt    + id          = (known after apply)  }Plan: 1 to add, 0 to change, 0 to destroy. Terraform’s plan output indicates that it needs to create a new resource, which is expected given that it does not yet exist. We can also see the provided values that we’ve specified, as well as a pair of permission attributes. Because we did not include them in our resource definition, the provider will use the default values. Apply: We can now use the apply command to create actual resources: $ terraform applyTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + createTerraform will perform the following actions: # local_file. hello will be created + resource  local_file   hello  {   + content       =  Hello, Terraform    + directory_permission =  0777    + file_permission   =  0777    + filename       =  hello. txt    + id          = (known after apply)  }Plan: 1 to add, 0 to change, 0 to destroy. ╷│ Warning: Version constraints inside provider configuration blocks are deprecated││  on main. tf line 2, in provider  local :│  2:  version =  ~&gt; 1. 4 ││ Terraform 0. 13 and earlier allowed provider version constraints inside the provider configuration block, but that is now deprecated and│ will be removed in a future version of Terraform. To silence this warning, move the provider version constraint into the│ required_providers block. ╵Do you want to perform these actions? Terraform will perform the actions described above.  Only 'yes' will be accepted to approve.  Enter a value: yeslocal_file. hello: Creating. . . local_file. hello: Creation complete after 0s [id=392b5481eae4ab2178340f62b752297f72695d57]Apply complete! Resources: 1 added, 0 changed, 0 destroyed. We can now confirm that the file was created with the desired content: $ cat hello. txtHello, TerraformAlso, after running terraform apply, the terraform. tfstate file has been created with the following content: $ cat terraform. tfstate{  version : 4,  terraform_version :  1. 2. 7 ,  serial : 1,  lineage :  bd8530c6-6781-a52a-2c26-dd1164afc7cd ,  outputs : {},  resources : [  {    mode :  managed ,    type :  local_file ,    name :  hello ,    provider :  provider[\ registry. terraform. io/hashicorp/local\ ] ,    instances : [    {      schema_version : 0,      attributes : {       content :  Hello, Terraform ,       content_base64 : null,       directory_permission :  0777 ,       file_permission :  0777 ,       filename :  hello. txt ,       id :  392b5481eae4ab2178340f62b752297f72695d57 ,       sensitive_content : null     },      sensitive_attributes : [],      private :  bnVsbA==     }   ]  } ]}Destroy: When we’re finished experimenting with Terraform, we should delete all of the resources we created so the cloud service provider doesn’t charge us for them. Cleanup is simple because Terraform keeps track of what resources we created. All we have to do is execute the destroy command. $ terraform destroyWhen we type “yes” and press enter, Terraform will create the dependency graph and delete all the resources in the correct order, using as much parallelism as possible. Terraform in action: In this section, let’s perform a series of exercises. Exercise #1 - Create an AWS EC2 instance: Create a Terraform configuration file with the content as shown below and save it in the main. tf file: provider  aws  { profile  =  default  region   =  us-east-1 }resource  aws_instance   example  { ami      =  ami-2757f631  instance_type =  t2. micro }The above definition file uses two blocks: provider followed by resource blocks. In the provider block, we use AWS as our cloud provider and the default AWS profile, which indicates the use of the default credentials (ACCESS KEY ID and SECRET ACCESS KEY) from the /. aws/credentials file. In the resource block, the AWS resource (resource type) to be created has been specified. In this example, we specified aws_instance, which represents the EC2 instance. Also, we specified additional parameters such as ami and instance_type that might be required for the creation of an EC2 instance. The following steps will be performed in order to create an EC2 instance:  Step 1: Initialize the working directory by executing the command terraform init.  Step 2: (Optional) Validate the configuration file (main. tf) by executing the command terraform validate.  Step 3: Perform a dry run to see what changes occur when the command terraform plan is executed.  Step 4: Execute Terraform by running the command terraform apply. This is the actual execution step where Terraform, after successfully authenticating, creates an EC2 instance.  Step 5: (Optional) Destroys the instance we created in the above step by executing the command terraform destroy -target aws_instance. example.  Note: As previously mentioned, Terraform keeps track of the infrastructure it creates in a state file called terraform. tfstate, which is stored locally on the provisioning machine by default. This state file is generated during the execution of Terraform, i. e. , when the apply command is executed. Once Terraform has been successfully executed, a new EC2 instance is created in our AWS account as shown below:                    A new EC2 instance created.       As shown in the screenshot, an EC2 instance was created with the given instance type t2. mirco and the AMI ID specified in our definition file. Exercise #2 - Create a compute engine in GCP: In this exercise, a compute engine instance will be created. Pre-requisites:  GCP account gcloud CLI must be installed.  Ensure we have a IAM service account with role as Owner. To verify, use the following command to list the service-accounts: gcloud iam service-accounts list Create a key for the service account so that Terraform can connect to GCP using this key. Use the following command for the same: gcloud iam service-accounts keys create google-key. json --iam-account &lt;service account email id&gt;. This would create a new key file named google-key. json in the current directory. We can also generate key via Google Cloud web console.  We must enable Compute Engine API by visiting the Google Cloud console’s Compute Engine page. Create a Terraform configuration file with the content as shown below and save it in the main. tf file: provider  google  {	credentials = file( proud-sweep-359704-f460da53c31e. json )	project =  proud-sweep-359704 	region =  us-central1 	zone =  us-central1-c }resource  google_compute_network   vpc_network  {	name =  demo-network }resource  google_compute_instance   dev_instance  {	name =  dev-instance 	machine_type =  f1-micro 	zone =  us-central1-c 	boot_disk {		initialize_params {			image =  centos-cloud/centos-7 		}		}	network_interface {		network =  default  // This enable private IP address		access_config { // This enable private IP address		}	}}To create a Compute Engine instance (it’s a VM) in GCP, the following steps will be performed:  Step 1: terraform init.  Step 2: (Optional) terraform validate.  Step 3: terraform plan.  Step 4: terraform apply.  Step 5: (Optional) terraform destroy -target google_compute_instance. dev_instance. Once Terraform has been successfully executed, a new Google Compute Engine (GCE) instance is created in our GCP account as shown below:                    A new GCE instance created in GCP.       ConclusionThis should help you understand the fundamentals of Terraform and how infrastructure as code can be set up quickly and efficiently. The declarative language simplifies the process of describing the infrastructure we intend to build, as opposed to describing how to build it. Before putting our changes into action, we use the plan command to test them and look for potential bugs. This is merely a summary of how efficiently infrastructure as code can be implemented with Terraform. I attempted to make this post as clear as possible while still providing sufficient detail for you to understand the ideas. For more details, check out their online documentation. "
    }, {
    "id": 14,
    "url": "http://localhost:4000/design-patterns-and-coding-principles/2024/solid",
    "title": "SOLID Design Principles",
    "body": "2024/01/16 - OverviewThe term “SOLID” in software engineering refers to a group of five design principles that are meant to improve the readability, flexibility, and maintainability of object-oriented designs. The SOLID design principle includes the following five principles:  Single-responsibility principle Open-closed principle Liskov substitution principle Interface segregation principle Dependency inversion principleAmerican software engineer and instructor Robert C. Martin (popularly known as Uncle Bob) introduced these design principles in his 2000 paper Design Principles and Design Patterns. Michael Feathers later transformed these principles into an acronym. We will examine each of them to learn how these principles can aid in the creation of software that is well-designed. Single-Responsibility Principle (SRP)Robert C. Martin, the originator of this principle, put it this way: “A class should have only one reason to change. ” It means that every module, class, or method should only have one (single) responsibility or job.                    SOLID - Single-Reponsibility Principle.       Uncle Bob defines responsibility as a reason to change, and he suggests breaking down components such as classes, modules, and methods until each has a single reason to change. Why do we need to do this, and what benefits will be gained by breaking a component into subcomponents where each must have a single responsibility? The idea is that each class or method is responsible for a single part of the product’s functionality, i. e. , each class or method does just one job. This keeps the design as simple as possible and makes it easier and less disruptive to make changes in the future. Furthermore, it improves the quality of the codebase in general. What if our class has multiple responsibilities to fulfill? A class with multiple responsibilities has a higher chance of having bugs since altering one of those responsibilities might have unintended effects (side-effects) on the others. Having said that, utilising SRP principle makes code easier to maintain and test. Motivation: "
    }, {
    "id": 15,
    "url": "http://localhost:4000/design-patterns-and-coding-principles/2024/singleton-pattern-or-object",
    "title": "Singleton Pattern",
    "body": "2024/01/16 - A singleton pattern limits the number of instances of a class to one. It falls under a creational design pattern because it deals with an object creation procedure. The singleton pattern ensures that a class has only one instance and provides a global access point to it.  Design patterns: In software engineering, a design pattern is a general, reusable solution to a commonly occurring problem in software design. It’s not a fully finished design that can be put into source code right away. Instead, it serves as a description, model, or template for problem-solving that may be used in a variety of situations. Except for the special creation method (magic method in Python), the singleton pattern prevents the creation of objects via any other means. If an object has already been created, this method either returns it or creates a new one if needed.  Dunder methods: The dunder methods are special methods that start and end with double underscores. These double underscores are referred by the acronym “dunder. ” Use casesWho would want to limit the number of instances a class has? The most common justification for this is to manage access to a shared resource, such a file or database. Creation of a singleton objectSingleton in Python: The following shows the creation of a singleton object in Python. Here, __new__ is a special or magic method (aka the dunder method). This method is invoked every time a class is instantiated. Note that the __new__ method is invoked before the __init__ method gets called.  When we attempt to create an object inside of the __new__ method in the usual way (ClassName()), it runs recursively. It loops back on itself until the maximum recursion depth is reached and a RecursionError error is thrown. class Singleton(object):     Represents Singleton class.      __instance = None  # This __new__ method is invoked every time a class is instantiated.   # It's invoked before the __init__ method gets called.   def __new__(cls, *args, **kwargs):    if not cls. __instance:      print( Creating singleton object. . .  )      cls. __instance = super(Singleton, cls). __new__(cls, *args, **kwargs)      # cls. __instance = object. __new__(cls, *args, **kwargs) # Other way of creating an object      # cls. __instance = Singleton() # Will get into a RecursionError error    return cls. __instancedef main():  singleton_object_1 = Singleton()  singleton_object_2 = Singleton()if __name__ == '__main__':  main()Output: Creating singleton object. . . The output shows that the new object is created just once. "
    }, {
    "id": 16,
    "url": "http://localhost:4000/design-patterns-and-coding-principles/2024/anti-pattern",
    "title": "Anti-Pattern",
    "body": "2024/01/16 - What is an anti-pattern?: The term “anti-pattern” was first used in 1995 by a computer programmer, Andrew Koenig, in an article called “Journal of Object-Oriented Programming. ”  An antipattern is just like a pattern, except that instead of a solution it gives something that looks superficially like a solution but isn’t one. - Andrew Koenig Anti-patterns in software engineering are a commonly used, simple-to-implement solution to recurring issues that is often inefficient and has the potential of being incredibly counterproductive. It demonstrates how to go from a problem to a bad solution. We just call these bad ideas or design smells. Avoid them at all costs! Anti-patterns are undesirable counterparts to design-patterns: Anti-patterns are the undesirable opposites of design patterns, which are formalized solutions to frequent issues and are usually viewed as acceptable development practice. Although anti-patterns at first seem to be quick and reasonable, they typically have adverse effects in the future. We will eventually come to understand that anti-pattern results in more negative outcomes than positive ones. Anti-patterns might result in technical debt: It affects our software badly and adds technical debt! Technical debt is a development decision taken for short-term gains that has long-term consequences. If we let our unintentional technical debt to grow wildly, it may result in dissatisfied developers and encourage staff attrition. IT leaders see technical debt as a significant threat to their organizations’ capabilities. What do anti-patterns reveal to us?: A well formulated anti-pattern reveals us:  Why does the poor solution seem appealing? Why it ends up being bad? What best patterns are available to replace it? The issue that the anti-pattern is attempting to solve already has a better alternative solution in place that is proven to be effective in contrast to the anti-pattern. In one aspect, the bad is perceived as the good: In general, developers are not too concerned about identifying design and code smells. However, it’s important to keep in mind that identifying bad practices can be as valuable as identifying good practices. Examples of anti-patterns: Here are some examples of anti-patterns:  Spaghetti code - It’s an unstructured and difficult-to-maintain source code.  Golden hammer - It is the practice of applying a known strategy to a variety of issues.  God object/class - This class or object is responsible for too many things.  Boat anchor - It is future code that has no use inside the present context.  Magic numbers and strings - Using unnamed numbers or string literals instead of named constants in code. Lastly, it should be noted that the same solution may act as both a pattern and an anti-pattern, depending on the context. "
    }, {
    "id": 17,
    "url": "http://localhost:4000/apache-pinot/2024/introduction-to-apache-pinot",
    "title": "Apache Pinot joins hands with Kafka and Presto to provide low-latency, high-throughput user-facing analytics",
    "body": "2024/01/16 - This post is for you if you are curious about Apache Pinot but are just getting started with it. We’ll go a little deeper into Apache Pinot to help you understand the components that make up a Pinot cluster. We will also get hands-on experience by running samples that include both streaming and batch data imports into Pinot and then analytical query executions. The evolution of analyticsBatch analytics: In the past, analytics were often performed in batches, resulting in high-latency analytics where queries returned responses based on data that was at least minutes, hours, or even days old, depending on the volume of data and available computing resources. The generation of business intelligence (BI) reports is one use case of batch analytics. Business intelligence uses historical data to report on business trends and answer strategic questions. In batch-style analytics, jobs are generally scheduled to run at night or during non-business hours. So, it often provides us with insights after the fact. Most of the time, these batch-type insights are based on stale data (old information), so we can’t rely on them. So no one anymore wants to do analytics in batches. Real-time analytics: Real-time analytics has now become something that every business ought to do. It’s the process of applying logic to data to get insights or draw conclusions right away so that better decisions can be made at the right time. Put simply, it aids in helping us make the right decisions at the right time. “Real-time” in real-time analytics means being able to get business insights as soon as possible after data (transactions) enters the system. Having access to analytics in real-time is important for day-to-day operations, financial intelligence, triaging incidents, and allowing businesses to act quickly. The most crucial benefit of real-time analytics is that it allows us to take opportunities and stop problems before they happen. User-facing analytics: In the world we live in now, everyone needs real-time analytical data, not just business analysts or top-level executives. We call this kind of analytics “user-facing analytics” also known as “customer-facing analytics. ” One good example of this is LinkedIn’s “Who viewed your profile” feature, which lets all of its more than 700 million users slice and dices the information about who looked at their pages. Uber created the UberEats Restaurant Manager app to give restaurant owners real-time insights into their order data. This is yet another excellent example of how the best use of user-facing real-time analytics improved the end-user experience.   In user-facing analytics, users won’t put up with painfully slow analytics. When they can find insights in real-time, they are more open to a data-driven culture. So, we need a solution that can scale to millions of users and offer fast, real-time insights. Businesses are working hard to speed up the steps needed to get enough data to answer everyone’s questions. One such solution that comes to our rescue is “Apache Pinot. ”                    Figure 1: The Evolution of Analytics.       A brief introduction to Apache PinotApache Pinot is a real-time, distributed OLAP datastore that was built for low-latency, high-throughput analytics, making it perfect for user-facing analytical workloads. It can ingest directly from streaming data sources like Apache Kafka and Amazon Kinesis and make the events available for querying right away. It can also ingest from batch data sources such as Hadoop HDFS, Amazon S3, Azure ADLS, and Google Cloud Storage. At the heart of the system is a columnar store equipped with advanced indexing and pre-aggregation techniques for low latency.  This makes Pinot the best choice for real-time analytics. One of the best things about Pinot is that it has a pluggable architecture. The plugins make it easy to add support for any third-party system, such as an execution framework, a filesystem, or an input format. For example, some plugins make it easy to ingest data and push it to our Pinot cluster:  pinot-batch-ingestion-spark pinot-s3 pinot-parquet                   Figure 2: Apache Pinot Overview. Image Courtesy: https://docs. pinot. apache. org.       Pinot joins hands with Presto and KafkaNow, the question comes: Can’t Pinot just do what it’s supposed to do without help from Kafka and Presto? Due to some limitations, Pinot depends on Kafka and Presto to provide user-facing analytics with high throughput and low latency. We’ll go through the reasons for using Kafka and Presto with Pinot, and how they complement each other. Need for Presto: Presto and Pinot are distinct technologies, yet they complement each other quite well for conducting and storing ad-hoc data analytics. Presto supports SQL, but users can’t use it to get fresh aggregated data. Pinot, on the other hand, can give us second-level data freshness, but it doesn’t support flexible queries.  Second-level data freshness: Second-level data freshness is the amount of time between when the organisation gets the data and when it can be used for in-depth analytics.                    Figure 3: Pinot vs. Presto.       Need for Kafka: Apache Kafka has become the industry standard for real-time event streaming because it flawlessly addresses the issue of real-time ingestion of data with high velocity, volume, and variability. Integrating Kafka with Pinot makes the event streams available for querying in real-time. This allows segments to be available for query processing as they’re being built. In the following sections, we will go over how Pinot leverages Kafka and Presto, making it perfect for user-facing analytical workloads at scale. Taking a closer look into Pinot and its components                   Figure 4: Pinot Architecture.       Pinot has two kinds of components:  Logical components Architectural componentsLogical components: Pinot cluster has the following logical components:  Tenant Table SegmentA logical view is another way to see what the cluster looks like:                    Figure 5: Pinot Cluster’s Logical View.        A cluster contains tenants Tenants contain tables Tables contain segmentsTenant: A tenant is a logical component of Apache Pinot. It’s simply a logical grouping of resources (servers and brokers) with the same Helix tag. So, tenant enable us to group resources that are used for isolation. In our cluster, we have a default tenant called “default tenant. ” When nodes are created in the cluster, they automatically get added to the default tenant. Pinot has top-notch support for tenants so that multi-tenancy can work. Every table is associated with a server tenant and a broker tenant. This sets the nodes that will be used as servers and brokers by this table. This lets all of the tables for a certain use case be put together under a single tenant name. The idea of tenants is very important when Pinot is used for many different purposes and there is a need to set limits or separate tenants (isolation) in some way. Table: A table is a logical abstraction that represents a collection of related data. It is made up of rows and columns (known as documents in Pinot). This concept is similar to that of other databases. A schema defines the table’s columns, data types, and other metadata. Pinot supports the following types of table:  Offline - Offline tables ingest pre-built pinot-segments from external data stores. This is generally used for batch ingestion.  Real-time - Real-time tables ingest data from streams such as Kafka and build segments from the consumed data.  Hybrid - Under the hood, a hybrid Pinot table is made up of both real-time and offline tables. All tables in Pinot are of the hybrid type by default.  The user who is querying the database doesn’t need to know what kind of table it is. In the query, they only need to say the name of the table. Regardless of whether we have an offline table myTable_OFFLINE, a real-time table myTable_REALTIME, or a hybrid table containing both of these, the query will be:select count(*) from myTable. Table configuration is used to define the table properties, such as name, type, indexing, routing, retention, etc. It is written in JSON format and is stored in ZooKeeper, along with the table schema. Segment: Pinot breaks a table into multiple small chunks of data known as segments that are distributed across Pinot servers. These segments can also be stored in a deep store such as HDFS. Pinot segment is a unit of partitioning, replication, and query processing that represents a subset of the input data along with the specified indices. By using Apache Helix, the Pinot controller defines how data is partitioned and replicated across servers. Using this information, the Pinot broker disperses queries to individual servers and gather them back together. There are two types of segments:  Mutable segments Immutable segmentsMutable segments: Mutable segments are those that are being stored in memory in the CONSUMING state. Each mutable segment organizes the incoming data into a columnar format and updates the needed indices, such as inverted or text indexes, in real time. The mutable segments are available for query processing right away, as they’re being built. Therefore, given the low ingestion overhead, Pinot provides the same level of data freshness as Kafka. Immutable segments: Each server decides on its own when it is time to save the in-memory segments to disk, based on a set of criteria.  Such on-disk segments are referred to as immutable segments. The criteria used for creating immutable segments can either be:  the amount of time elapsed since the segment was initially created the number of rows consumed Note: For offline tables, segments are built outside of pinot and uploaded using a distributed executor such as Apache Spark or Hadoop. For real-time tables, segments are built in a specific interval inside Pinot. A table is represented as a Helix resource in the Pinot cluster, and each segment of a table is represented as a Helix Partition.                    Figure 6: Tenant -&gt; Tables -&gt; Segments.       Architectural components: A Pinot cluster is a set of nodes comprising of:  Server Broker Controller MinionPinot uses Apache Helix for cluster management. Helix handles resources that are replicated and partitioned in a distributed system. Helix uses ZooKeeper to store cluster state and metadata. Server: Servers host (store) the data segments and serve queries based on the data they host. Pinot servers consume data directly from the Kafka topic. They are also indexing the data as they ingest it, and they serve queries on the data that they have. There are two types of servers:  Offline server Real-time serverOffline server: Offline servers download segments from the segment store so that they can host and serve queries. When a new segment is uploaded to the controller, the controller decides which servers will host the new segment and notifies them to download the segment from the segment store. When the servers get this notification, they download the segment file and put the segment on the server so that they can handle queries.                    Figure 7: Offline Server shows Batch Ingestion.       Real-time server: Real-time servers ingest directly from a real-time stream like Kafka. Based on certain thresholds, they make segments of the data that has been stored in-memory from time to time. Then, this segment is saved to the segment store.                    Figure 8: Real-time Ingestion.       Broker: Broker handles Pinot queries. They accept queries from clients and forward them to the right Pinot servers. They gather results from the servers and combine them into a single response to send back to the client.                    Figure 9: Broker interaction with other components.       Controller: The node that observes and controls the participant nodes. In other words, the Pinot controller manages all the components of the Pinot cluster with the help of Apache Helix. It is in charge of coordinating all cluster transitions and making sure that state constraints are met while keeping the cluster stable. Pinot controllers are modeled as controllers. The Pinot controller is really the brains of the cluster, and it takes care of cluster membership, figuring out what data is located on which server, and performing query routing. Pinot controller hosts Apache Helix (for cluster management), and together they are responsible for managing all the other components of the cluster. The Pinot controller has a user interface that lets us access and query Pinot tables. We can also use this user interface to add, edit, or delete schema and table configurations. We can access the user interface via controller’s port (http://localhost:9000); port 9000 is the default controller’s port. Minion: I’m not going into Minion details here. If you’d like to learn more about Minion, check out the official document here. Pinot’s use cases and limitationsUse cases: Pinot was built to execute real-time OLAP queries on massive amounts of streaming data and events with low latency. Pinot also supports batch use cases with a similar guarantee of low latency. It works well in situations where we need to do quick analytics, such as aggregations, on immutable data that’s being received via real-time data ingestion. Also, Pinot is a great choice for querying time series data with lots of dimensions and metrics. Limitations:  Pinot is not a replacement for a database and should not be used as a source of truth. Pinot is not a replacement for a search engine. SQL query limitations::  Pinot does not fully support ANI-SQL at this time.  It doesn’t allow cross-table queries (using JOINs) as well as nested queries.  It also cannot handle a large amount of data shuffle.   Table joins and other operations, such as a large amount of data shuffling, may be accomplished using either the Trino-Pinot connector or the Presto-Pinot connector. Getting started with PinotThis section describes how to launch a Pinot cluster and ingest data into it. Running Pinot components: Apache Pinot can be run in any of the following environments:  locally on our own computer in Docker in KubernetesRead more to find out how to deploy and run Apache Pinot locally on our computer. Getting data into Pinot: There are multiple ways of importing data into Pinot.  Read more. Frequently asked questions (FAQ)What are the challenges that user-facing analytics have to deal with?:  Latency: Must be in the sub-second range.  Throughput: Should support millions of concurrent users querying the data.  Data freshness: Insights must be fresh (current), up to date, and relevant. What is queries per second (QPS)?: Queries per second (QPS) is a way to measure how many searches a system for obtaining information, like a search engine or a database, gets in one second. The term is often used for any request-response system, in which case it should be called requests per second (RPS). What is the Ideal State and Exteral View?: Ideal State represents the desired state of the table resource. The Exteral View represents the actual state. Any cluster action usually updates the Ideal State, and then Helix sends state transitions to the right components. How does Pinot use ZooKeeper?: The cluster management in Pinot is made with Apache Helix, which is built on top of Zookeeper. Helix uses Zookeeper to store the cluster state, including Ideal State, External View, Participants, etc. Besides that, Pinot uses Zookeeper to store other information such as Table configs, schema, Segment Metadata, etc. What is Apache Helix?: Apache Helix is a generic open-source cluster management framework developed by LinkedIn. It is used for the automatic management of partitioned, replicated and distributed resources hosted on a cluster of nodes. Helix automates:  Reassignment of resources when a node fails Cluster expansion ReconfigurationWhat is Helix task?: In Helix terminology, a task is referred to as a resource. Helix is based on the idea that every task has the following attributes associated with it:  Location (e. g. it is available on Node N1) State (e. g. it is running, stopped etc. )Is Pinot support open distributed data file format like Parquet?: No. A closed-file format is used by Pinot. In the future, they might be able to work with open file formats like Parquet. What input file formats can be used to send data to Pinot?:  CSV Parquet ORC AVRO JSON Thrift Protocol BuffersHow can data from Pinot be retrieved?: There are different ways to query for data from Pinot using:  Broker endpoint (REST API) Query console Pinot-admin Pinot clientsBroker endpoint: We can use the /query/sql endpoint on a broker to access the Pinot REST API using the POST operation with a JSON body that includes the parameter sql. $ curl -H  Content-Type: application/json  -X POST \  -d '{ sql : select foo, count(*) from myTable group by foo limit 100 }' \  http://localhost:8099/query/sqlQuery console: Query Console can be used for running ad-hoc queries. The Query Console can be accessed by entering the &lt;controller host&gt;:&lt;controller port&gt; in our browser. Pinot-admin: We can also query using the pinot-admin script (pinot-admin. sh). cd pinot/pinot-tools/target/pinot-tools-pkgbin/pinot-admin. sh PostQuery \ -queryType sql \ -brokerPort 8000 \ -query  select count(*) from baseballStats 2020/03/04 12:46:33. 459 INFO [PostQueryCommand] [main] Executing command: PostQuery -brokerHost localhost -brokerPort 8000 -queryType sql -query select count(*) from baseballStats2020/03/04 12:46:33. 854 INFO [PostQueryCommand] [main] Result: { resultTable :{ dataSchema :{ columnDataTypes :[ LONG ], columnNames :[ count(*) ]}, rows :[[97889]]}, exceptions :[], numServersQueried :1, numServersResponded :1, numSegmentsQueried :1, numSegmentsProcessed :1, numSegmentsMatched :1, numConsumingSegmentsQueried :0, numDocsScanned :97889, numEntriesScannedInFilter :0, numEntriesScannedPostFilter :0, numGroupsLimitReached :false, totalDocs :97889, timeUsedMs :185, segmentStatistics :[], traceInfo :{}, minConsumingFreshnessTimeMs :0}Pinot clients: Here’s a list of the clients available to query Pinot from your application:  Java client Go client JDBC client (coming soon)What are the key takeaways for Apache Pinot?:  We may not need it at all.  This isn’t our system of record. "
    }, {
    "id": 18,
    "url": "http://localhost:4000/data-management/2024/data-product-vs-data-as-a-product",
    "title": "Data Product vs. Data as a Product",
    "body": "2024/01/16 - OverviewWe live in a world full of data, but how can we best use it? It should come as no surprise that, when used wisely, data is the most precious thing on the planet. Be aware that a “data product” is not the same as “data as a product”. We might hear the term “data as a product” more often these days due to the current hot trend in the data industry known as “data mesh,” which claims to be able to solve many of the problems of its predecessors. One of the principles of the data mesh paradigm is to consider data as a product. This principle is sometimes shortened to “data product,” which leads to a misunderstanding between data product and data as a product.                    Data everywhere!      Let’s explore the distinctions in detail between these two notions. What is a data product?Any product is called a data product if data is the key enabler for its primary goal. This means that any digital product or feature that relies on data to achieve its ultimate purpose or goal may be referred to as a data product. The former chief data scientist of the United States, DJ Patil, defined a data product in his 2012 book Data Jujitsu: The Art of Turning Data into Product as “a product that facilitates an end goal though the use of data. ” In general, a data product is any tool or application that processes data and produces outcomes. These outcomes may provide businesses with valuable insights. Various types of data products: Typically, data products are categorized by type:  Raw data Derived data Algorithms Insights (Decision support) Automated decision-makingExamples of data products: Examples of data products are:  Any online shopping page may be a data product if the featured products are dynamically displayed depending on my previous purchases and searches.  Google analytics is a data product since the insights it presents to the end user are derived from data.  A data warehouse - This data product is a mix of raw data, derived data and insights.  A self-driving car - It’s of the type automated decision-making. What value does it provide?: Data products can help organisations extract insight from their data in order to develop more accurate forecasts, reduce expenses, and increase revenue. Data as a productData as a product. In other words, data as a first-class product. This implies that data is considered as a true product, as opposed to a by-product. The data being discussed is organizational analytical data generated by several domains. The goal is to make this data discoverable, addressable, trustworthy, and secure so that other domains can make good use of it. This principle implies that there are data consumers outside of the domain. In other words, each domain team considers the other domains as internal customers of their data, and they take on additional stewardship responsibilities for their data in order to meet the needs of other domains by providing high-quality data. This principle applies a product-thinking mindset to analytic data.  Product-thinking: When it comes to identifying solutions, the design team must consider the whole picture in order to make the product effective for the user. Product-thinking places the focus on the product rather than the features. An example of data as a product: Well, what does data as a product look like? Consider data as a product to be a microservice for analytics or for the data world. Like a microservice, data as a product comprises the code (to perform data computation), its data and metadata, and the infrastructure required for its operation. Data product vs. data as a productAfter understanding each of these concepts, it becomes clear that they all substantially depend on meticulously derived data. However, “data product” is a broad term, while “data as a product” is a subset of all possible “data products”. In other words, “data as a product” is formed from the data type “data product”. "
    }, {
    "id": 19,
    "url": "http://localhost:4000/data-engineering/2024/data-mesh",
    "title": "Data Mesh",
    "body": "2024/01/16 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! Before we go any farther in defining data mesh, let’s first understand why it’s necessary. Existing data warehouse or data lake pain points:  It does not scale well to an increasing variety of data sources (addressed in data lake).  The effort required for central cleaning and quality control increases in lockstep with the growing amount of data.  Existing centralized analytical data repositories, such as data warehouses and data lakes, create distance and anonymity between data producers and data consumers. Why are we not yet ready to adopt data mesh?: Data mesh is generating both excitement and concern at the moment. Some of us aren’t brave enough to change the current centralized data paradigm because we need a holistic view of data. What is data mesh?Data mesh is more than just another version of a centralization-based analytics data architecture. Rather, it is a new approach for acquiring, maintaining, and accessing data for large-scale analytical use cases. The kind of data that we are referring to is analytical data.  Analytical data: It’s a collection of data used for decision-making and research. It is historical data that is curated and optimised for data analysis. Zhamak Dehghani came up with the term “data mesh” in 2019. It is based on four basic principles that bring together well-known concepts:  Decentralized domain ownership Data as a product Self-serve data infrastructure platform Federated computational data governanceData mesh demands a fundamental transformation in our assumptions, design, technical solutions, and social structure of our organisations in terms of how we handle, utilise, and own analytical data. It shifts the organisation’s data ownership model from centralised to decentralised. Typically, the centralised data model is administered by data platform management specialists. This change returns data ownership and accountability to the business domains from where it originated. Data consolidationEnterprises manage a wide variety of data from various sources to run their operations effectively. As enterprises expand, the size of their data stores grows, resulting in data silos inside them. In such enterprises, data is often siloed among departments or domains, making it difficult to get overall visibility while making business-critical decisions. One way to get comprehensive visibility of the data is through “data consolidation”. Data consolidation is the act of gathering data from several systems and storing it in a single repository, where it may be utilized as the foundation for business analytics to derive strategic and operational insights. This method is most commonly associated with data warehousing. However, more modern variations on this concept include the data lake, which is best suited for storing unstructured data that presents itself more easily for analysis using AI and ML technologies. Unfortunately, the data consolidation approach is incapable of providing real-time insight into what is going on in the organization. Data federationData federation takes a different approach. Instead of bringing data from various sources together in one place, federation leaves an organization’s data where it is but gives a unified view of all through virtualization. "
    }, {
    "id": 20,
    "url": "http://localhost:4000/data-management/2024/data-catalog",
    "title": "Data Catalog",
    "body": "2024/01/16 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! What is a data catalog?A catalog, in its literal sense, refers to a book or document containing a complete list of things, usually arranged systematically. A data catalog, in the context of data management, is an organised inventory of all data assets inside an organisation to help data professionals quickly and easily locate the most relevant data with the aim of gaining business insights. With a data catalog, the entire organization is given the ability to see all the data it has. Data catalogs have emerged as a powerful tool for data management and data governance. What is data assets?: The data assets consist of, but are not limited to:  Files and folders Structured (tabular) data Unstructured data Reports and query results Data visualisations and dashboards Machine learning modelsMetadata catalog: A data catalog, at its core, is a collection of metadata combined with data management (e. g. , access permissions) and search tools. We can also call it a metadata catalog. When the data catalog gathers metadata about a data asset, it obtains information such as the asset’s creation date, owner, column names, schema name, and so on. Put it simply, the metadata summarises or describes the underlying data assets.  Metadata Indexing: The underlying data assets are not indexed by the data catalog. Only the metadata describing these data assets is indexed. Organizes metadata: We mentioned in the definition of a data catalog that it is an “organized” inventory of all data assets. When it says “organized,” it signifies that the data catalog’s metadata is well organized based on domains and other parameters. A domain is a collection of data assets that are conceptually related. Finance, sales, inventories, and so forth are examples of domains. It is up to the domain owners to specify which data assets get into their domain in a data catalog. Data catalog doesn’t reveal actual data: The data catalog only provides an overview at the metadata level and does not expose any actual data. As a result, we can let everyone see everything without worry of exposing confidential or sensitive data. What kind of queries may be sent to a data catalog?: A data catalog should, at the very least, respond to:  Where can I get my data? Are these data relevant and important? What do these data indicates? How can I make use of this data?What can be accomplished with a data catalog?: Generally, the data catalog gives users access to tools that let them accomplish the following:  Search the catalog with flexible searching and filtering options Data discovery Data governance in compliance with organisational or governmental rulesHow does a data catalog get created?: A data catalog can retrieve metadata from our IT landscape (data sources) using a built-in crawler. Crawler is a pull-based system, however data catalog may also be push-based.  The data catalogs that crawl the data sources (i. e. , that pull, not push) must have a wide variety of connectors to support various data sources. The IT landscape that is reflected in our data catalog will get business terminology added to it as “tags”. We can also enhance our data catalog with additional information such as roles, supplementary descriptions, classifications, and so on. A data catalog has various roles built into it, such as data steward, data owner, and others that all perform certain functions in the data catalog. Searchable data catalog: Search is one of the most important features of a data catalog. A data catalog enables all data specialists in an organization to search for any data.  Data discovery: Searching and finding relevant data is called data discovery. It determines if the data exists or not, not what is contained inside it. "
    }, {
    "id": 21,
    "url": "http://localhost:4000/algorithms-and-data-structures/2024/dsa_intro",
    "title": "An Introduction to Algorithms and Data Structures",
    "body": "2024/01/16 - What is an algorithm?An algorithm is a set of steps or instructions in a particular order for performing a specific task. If we have ever written code before, we have written an algorithm. The code can be considered an algorithm. There are multiple problems in computer science, but some of them are pretty common regardless of the project we’re working on. Different people have developed various solutions to these common problems, and the field of computer science has, over time, identified several that perform well for a particular task. What characteristics or guidelines do algorithms possess?:  An algorithm should have a clearly defined problem statement, input, and output.  The algorithm’s steps must be performed in a very specific order.  The steps of an algorithm must be distinct; it should not be possible to subdivide them further.  The algorithm should produce a result; each instance of a given input must produce the same output every time.  The algorithm must finish within a finite amount of time. These guidelines not only aid in defining what an algorithm is, but also in validating its correctness. Big-O notation: Big O notation uses algebraic terms to describe the complexity of our code. It describes an asymptotic upper bound i. e. , worst case scenario. No mathematical knowledge is necessary to understand Big-O notation. Each Big-O notation defines a curve’s (XY graph) shape. The shape of the curve represents the relationship between the size of a dataset (amount of data) and the time it takes to process that data. The performance of the following Big-O notations is ranked from best (`O(1)`) to worst (`O(n!)`). Here, the letter n represents the input size.  O(1) - Constant complexity that takes the same amount of space regardless of the input size.  O(log n) - Logarithmic complexity. It’s better than O(n) and close to that of O(1). Every time n (input size) increases by an amount k, the time or space increases by k/2.  O(n) - Linear complexity.  O(n log n) - Log linear complexity O(n^2) - Quadratic complexity with a growth rate of n2 i. e. , n x n times.  O(2^n) - Exponential complexity O(n!) - Factorial complexityEfficiency of an algorithm: A problem can be solved in more than one way. So, it is possible to come up with more than one solution or algorithm for a given problem. In such a case, we must compare them to determine which is the most effective. There are two measures of efficiency when it comes to algorithms:  Time (aka time complexity) Space (aka space complexity)Time complexity (Speed): The efficiency measured in terms of time is known as time complexity. Time complexity is a measure of how long an algorithm takes to execute. Time complexity is defined as a function of the input size n using Big-O notation as O(n), where n is the size of the input and O is the growth rate function for the worst-case scenario.  Big-O notation: The Big-O notation enables a standard comparison of the worst-case performance (aka upper bound) of our algorithms. Constant time complexity: If an algorithm’s time complexity is constant, its execution time remains the same regardless of the input size and the number of times it is executed. The constant time is denoted by O(1) in Big-O notation. If we can create an algorithm that solves the problem in O(1), we are likely at our top performance. Note that O(1) has the least complexity. Space complexity (Memory): In an algorithm, time is not the only thing that matters. We may also worry about the amount of memory or space an algorithm requires. Space complexity is the amount of memory space that an algorithm or a problem takes during execution. Put simply, it corresponds to the amount of memory a program or algorithm uses. The memory here is required for storing the variables, data, temporary results, constants and many more. In other words, the space complexity is not only determined by the amount of storage space consumed by the algorithm or program, but it also takes into account the amount of storage space required for the input data. For example, creating an array of size n will require O(n) space. O(n2) space is required for a two-dimensional array of size n x n.   Space complexity vs. auxiliary space: People have a tendency to get space complexity and auxiliary space confused with one another. Let’s get one thing out of the way first: they are two different terminology. Auxiliary space is the temporary space that our program allocates in order to solve the problem with respect to the amount of data that is supplied as input. Note that the size of the supplied input data does not account for this auxiliary space in any way. However, space complexity includes both auxililary space and space used input data.  Auxiliary space = Total space - Size of input dataSpace complexity = Auxiliary space + Size of input data An efficient algorithm must be able to achieve a balance between both time and space complexity measures. For example, it might not matter if we have a super-fast algorithm if it uses up all the memory we have.  Ideal efficiency: The most efficient algorithm could be one that performs all of its operations in the shortest time possible and uses the least amount of memory. Linear search algorithm: For a given value of n, linear search will require n attempts to locate the value in the worst case. By evaluating a worst-case scenario, we avoid having to perform the necessary work. We are aware of the actual or potential outcome. As the value gets really large, the running time of the algorithm gets large as well. What is data structure?A data structure is a way to organize, manage, and store data that makes it easy to access its values and change them. To efficiently access or utilize data, we must first store it efficiently. Data structures are essential ingredients in creating fast and powerful algorithms. They make code cleaner and easier to understand. The following five operations are commonly performed on any data structure:  Insertion Deletion Traversal Searching SortingTypes of data structure: Generally, data structures fall into two categories:  Linear data structure Non-linear data structureLinear data structure: Data elements are arranged sequentially in a linear data structure. These elements can be ordered in any manner (ascending or descending). In a linear data structure, each element must have an element before it, and if it is not the last element, it may also have an element after it.   Non-linear data structure: TODO Asymptotic analysis: Frequently asked questions (FAQ)Array vs. linked list: Inserting an element at the beginning of the sequential list is way faster in the linked list than in the array. O(1) vs. O(log(n)): O(log(n)) is more complex than O(1). O(1) indicates that the execution time of an algorithm is independent of the size of the input, whereas O(log n) denotes that as input size n increases exponentially, the running time increases linearly. In some instances, O(log(n)) may be faster than O(1), but O(1) will outperform O(log(n)) as n grows, because O(1) is independent of the input size n. What is asymptotic analysis?: Asymptotic analysis is the process of calculating the execution or running time of an algorithm in mathematical units in order to determine the program’s limitations or run-time performance. Using asymptotic analysis, we can determine the worst case, average case, and best case execution times of an algorithm. Following are the asymptotic notations for each of the three execution time cases:  Worst case is represented by Ο(n) notation; represents upper bound.  Average case is represented by Θ(n) notation aka Big-Theta; represents tight bound.  Best case is represented by Ω(n) notation aka Big-Omega; represents lower bound. What are the common approaches for developing algorithms?: There are three common approaches for developing algorithms:  Greedy approach     It’s a simple and straightforward approach.    Identifying a solution by selecting the next component that provides the most obvious and immediate benefit, i. e. the next-best option.    It identifies the feasible solution, which may or may not be the optimal solution.    In this method, a decision is made based on the information currently available, without consideration for the long-term consequences.     Divide and conquer approach     The divide strategy involves dividing the problem into smaller subproblems until those subproblems are simple enough to be solved directly.    The conquer strategy solves subproblems by calling recursively until all subproblems have been resolved.    After resolving all subproblems, combine them to arrive at the final solution for the entire problem.    The divide-and-conquer approach is often used to find an optimal solution to a problem.    The standard algorithms that follow the divide and conquer algorithm/approach are listed below:         Binary search     Quicksort     Merge sort     Closest pair of points     Strassen’s algorithm     Cooley–tukey fast fourier transform (FFT) algorithm     The karatsuba algorithm           Dynamic programming     It is similar to the divide and conquer strategy because it divides the problem into smaller sub-problems.    In dynamic programming, sub-problems are interdependent; therefore, the results of subproblems are stored for future use so that they do not need to be recalculated.    Before calculating the solution to a current subproblem, the previous solutions are examined. If any of the previously solved sub-problems are similar to the current one, the result of that sub-problem is utilized. Finally, the solution to the original problem is obtained by combining the solutions to the subproblems.    How constant time differs from linear time?: When it comes to time complexity, no matter how big the constant is and how slow the linear increase is, linear will at some point surpass (exceed) the constant.                    Figure 1: Time complexity - O(1) vs. O(s).       What is amortized time complexity?: Amortized time complexity occurs when an operation is extremely slow once in a while (occasionally) and has expensive worst-case time complexity, but the operations are faster the majority of the time.   In the case of a dynamic array, if its size reaches capacity, it immediately expands to double its original size. For example, if an array’s starting size is 10, and it fills up, the size will be increased to 20. The following steps are taken when the dynamic array is filled:  Allocate memory for a larger array size, generally twice the size of the previous array. Its time complexity is O(2n), where n is the size.  Copy all the contents of the old array to a new one. Its time complexity is O(n).  Insert the new element. Its time complexity is O(1) for each insertion. What operations are available for stacks?: The operations listed below can be performed on a stack:  push() - Adds an item to stack pop() - Removes the top item in the stack peek() - Gives top item’s value without removing it isempty() - Checks if stack is empty isfull() - Checks if stack is full"
    }, {
    "id": 22,
    "url": "http://localhost:4000/aws/2024/aws-cli",
    "title": "AWS Command Line Interface (AWS CLI)",
    "body": "2024/01/16 - AWS command line interfaceThe AWS command line interface (AWS CLI) is an open-source tool that allows us to interact with AWS services (S3, EC2, etc. ) using commands in our command-line shell. It’s an interface between users and AWS services. The AWS CLI lets us start running commands that do the same things as the browser-based AWS Management Console from the command prompt in our terminal application with minimal configuration. The AWS CLI provides direct access to the public APIs of AWS services. AWS CLI version 2: The AWS CLI version 2 is the most current major version and supports all of the most recent features. Some features included in version 2 are not backported to version 1, and we must update to use them.  Note: The AWS CLI version 2 is only available as a packaged installer. While it may be available via package managers, these are unsupported and unofficial packages that are not produced or maintained by AWS. To install the AWS CLI version 2, refer here. Use the following command to verify the currently installed version: aws --versionHow to configure AWS CLI to interact with AWS services?: Let’s take a look at how to quickly configure the basic settings that the AWS CLI uses to interact with AWS. In general, the aws configure command is the quickest approach to get our AWS CLI installation up and running. When we enter this (aws configure) command with no arguments, the AWS CLI prompts us for four pieces of information:  Access key ID (sensitive information) Secret access key (sensitive information) AWS Region (non-sensitive information) Output format (non-sensitive information)The AWS CLI stores sensitive credential information that we specify with aws configure command in a local file named credentials, in a folder named . aws in our home directory-the default location is ~/. aws/credentials. The non-sensitive configuration options that we specify with aws configure are stored in a local file named config, stored under the same . aws folder in our home directory i. e. , ~/. aws/config. $ ls -ltr ~/. aws-rw------- 1 user staff 116 Sep 24 18:02 credentials-rw------- 1 user staff  10 Sep 24 18:02 configConfigure AWS CLI to connect to AWS: Here is how we configure the AWS CLI to connect to our AWS account. These are only sample values. Replace them with our own values. % aws configureAWS Access Key ID [None]: AKIA4S2AHSC2UKNSQCFMAWS Secret Access Key [None]: TCORDiSPQ5o4B7+SGerdEud1wZHQEQ7N0zhbX7m9Default region name [None]: us-east-1Default output format [None]: jsonThe AWS CLI stores the given details in a profile named [default] in both the credentials and config files as shown below: % cat ~/. aws/credentials[default]aws_access_key_id = AKIA4S2AHSC2UKNSQCFMaws_secret_access_key = TCORDiSPQ5o4B7+SGerdEud1wZHQEQ7N0zhbX7m9% cat ~/. aws/config[default]region = us-west-2output = table Profile: A profile is a collection of settings that include access key id, secret access key, region name and output format. By default, when we run an AWS CLI command that doesn’t say which profile to use, the information in the [default] profile is used. Managing multiple AWS accounts: If we have multiple AWS accounts to connect using AWS CLI from a single computer, we may use the —-profile parameter to create a named profile. We can specify a --profile profilename and use the credentials and settings stored under that name. Creating a new named profile: The following example creates a profile named guestuser: aws configure --profile guestuserAWS Access Key ID [None]: AKIAI44QH8DHBEXAMPLEAWS Secret Access Key [None]: je7MtGbClwBF/2Zp9Utk/h3yCo8nvbEXAMPLEKEYDefault region name [None]: us-west-2Default output format [None]: tableHow to update existing settings?: To update these settings, run aws configure again (with or without the --profile parameter, depending on which profile we want to update) and enter new values as appropriate. $ aws configureAWS Access Key ID [****]:AWS Secret Access Key [****]:Default region name [us-west-1]: us-west-2Default output format [None]:list flag: The list flag will show us the current configuration data. $ aws configure list [--profile profile-name]Example: $ aws configure list   Name          Value       Type  Location   ----          -----       ----  --------  profile        &lt;not set&gt;       None  Noneaccess_key   ****************XIEM shared-credentials-filesecret_key   ****************OqwS shared-credentials-file  region        us-east-1       env  ['AWS_REGION', 'AWS_DEFAULT_REGION']get flag: The get flag gets a configuration value from both the credential and the config file, depending on what value we have given. $ aws configure get varname [--profile profile-name]Examples: $ aws configure get aws_access_key_id$ aws configure get default. aws_access_key_id$ aws configure get region$ aws configure get aws_secret_access_key --profile guestuser"
    }, {
    "id": 23,
    "url": "http://localhost:4000/aws-glue/2024/aws-glue-overview",
    "title": "Overview of AWS Glue",
    "body": "2024/01/16 - What is AWS Glue?AWS Glue is a “serverless” data integration service (commonly known as ETL) that makes it easier to discover, prepare, move, and integrate data from multiple sources for analytics, machine learning (ML), and application development. In three significant ways, AWS Glue stands apart from the other ETL tools on the market:  Serverless - We don’t need to provision, configure, or spin up any servers, and we don’t need to manage the servers’ lifecycle.  Schema inference - Glue provides crawlers with automatic schema inference for our structured and semi-structured data sets.  Autogen ETL scripts - In order to save us the trouble of starting from scratch, Glue will automatically generate the scripts required to extract, transform, and load our data from source to target. Event-driven ETL: AWS Glue can perform our ETL operations as fresh data arrives. For instance, we can enable AWS Glue to start our ETL operations running as soon as fresh data is made available in Amazon S3. AWS Glue componentsAWS Glue relies on the interaction of several components to create and manage our ETL workflow. AWS Glue Data Catalog: A centralized metadata catalog known as the AWS Glue Data Catalog is where the table definitions (schemas) that are inferred by AWS Glue are stored. After the data has been cataloged, it is immediately available for search and query using Amazon Athena, Amazon EMR, and Amazon Redshift Spectrum. Each AWS account has one AWS Glue Data Catalog per AWS Region. Each Data Catalog is a highly scalable collection of tables organized into databases. A table is metadata representation of a collection of structured or semi-structured data stored in sources such as Amazon RDS, HDFS, Amazon OpenSearch Service, and others. The AWS Glue Data Catalog offers a standardized repository in which disparate systems can store and find information for the purpose of maintaining a record of data that is stored in data silos. After that, we can make use of the metadata that is kept in this catalog to query and transform the actual data in a consistent manner across a broad range of applications. We can use the metadata from AWS Glue Data Catalog to orchestrate ETL jobs that transform data sources and load our data warehouse or data lake. For the purpose of controlling access to the tables and databases, we make use of the Data Catalog in conjunction with AWS IAM and Lake Formation. By doing this, we’ll be able to give the rest of the organization a safe way to share data while also protecting sensitive information in a very detailed way.   Crawler: A crawler accesses our data store (source or target), extracts metadata, and creates table definitions in a centralized metadata catalog known as the AWS Glue Data Catalog for later querying and analysis. The ETL jobs that we define in AWS Glue use these Data Catalog tables as sources and targets. Clasifier: A classifier is part of a crawler that is triggered during a crawl task. A classifier reads the data in a data store and checks whether a given file is in a format the crawler can handle. If it recognizes the format of the data, it generates a schema in the form of a StructType object that corresponds to the structure of the data. AWS Glue provides a set of built-in classifiers, but we can also create custom classifiers. Classifiers are provided by AWS Glue for a variety of commonly used file formats, including CSV, JSON, AVRO, XML, and others. In addition to this, it offers classifiers for common relational database management systems using a JDBC connection. Connection: An AWS Glue connection is a Data Catalog object that stores login credentials, URI strings, information about virtual private cloud (VPC), and other relevant data for a particular data store. AWS Glue crawlers, jobs, and development endpoints use connections in order to access certain types of data stores. AWS Glue supports the following connection types:  JDBC Amazon Relational Database Service (Amazon RDS) Amazon Redshift Amazon DocumentDB Kafka MongoDB Network (designates a connection to a data source that is in an Amazon VPC)Database: A set of associated Data Catalog table definitions organized into a logical group. Table: The schema of our data, which is often referred to as the metadata definition that describes our data, It is important to keep in mind that a table only stores the schema representation of our data, which includes the names of columns, definitions of data types, information on partitions, and other metadata about the actual data. The actual data remains in its original data store, whether it be in a file or a relational database table. Dynamic Frame: A distributed table which can handle nested data, such as structures and arrays. Each record includes both the data itself as well as the schema that specifies those data. In our ETL scripts, we are able to use both dynamic frames and Apache Spark DataFrames, as well as convert between the two types. Dynamic frames provide a set of advanced transformations for data cleaning and ETL. Job: The necessary business logic to carry out the tasks associated with ETL. It is composed of a transformation script, data sources, and data targets. Job runs are initiated by triggers that can be scheduled or triggered by events. Script: Script is the code that reads data from sources, transforms it as necessary, and then loads it into targets.  Using the metadata in the Data Catalog, AWS Glue can automatically generate Scala or PySpark scripts with AWS Glue extensions that we can use and modify to perform various ETL operations. Trigger: Initiates an ETL job. Triggers can be defined based on a scheduled time or an event. Development endpoint: It creates a development environment where the ETL job script can be tested, developed, and debugged. Worker: With AWS Glue, we only pay for the time your ETL job takes to run. There are no resources to manage, no upfront costs, and we are not charged for startup or shutdown time. We are charged an hourly rate based on the number of Data Processing Units (or DPUs) used to run our ETL job. A single Data Processing Unit (DPU) is also referred to as a worker. AWS Glue comes with three worker types to help us select the configuration that meets our job latency and cost requirements. Workers come in Standard, G. 1X, G. 2X, and G. 025X configurations. Populate data catalog with crawlersWe can use a crawler to populate the AWS Glue Data Catalog with tables. A crawler is capable of crawling multiple data stores in a single run. When it’s done, the crawler will either generate new table(s) in our Data Catalog or update the ones it already has. ETL jobs that we define in AWS Glue use these Data Catalog tables as sources and targets. Crawler prerequisites: When we create the crawler, the crawler will automatically take on the permissions of the IAM role that we specify. This IAM role must have permissions to extract data from your data store and write to the Data Catalog. For our crawler, we can create a role and attach the following policies:  The AWSGlueServiceRole AWS managed policy, which grants the required permissions on the Data Catalog.  An inline policy that grants permissions on the data source. Which data stores can we crawl?: Crawlers can crawl the following file-based and table-based data stores:  Native client     Amazon S3   Amazon DynamoDB   Delta Lake    JDBC     Amazon Redshift   Amazon RDS (Amazon Aurora, MariaDB, MS SQL Server, MySQL, Oracle, PostgreSQL)    MongoDB client     MongoDB   Amazon DocumentDB (with MongoDB compatibility)   How crawlers work?: When a crawler runs, it takes the following actions:  Classification of data - Classifies data to determine the format, schema, and associated properties of the raw data.  Grouping of data - Groups data into tables or partitions based on crawler’s principles.  Writing metadata to the Data CatalogWhen we are defining a crawler, we choose one or more classifiers that analyze the structure of our data in order to infer a schema. AWS Glue has built-in classifiers that can determine schemas based on the files that are commonly used. However, we are also able to create custom classifiers in a separate operation, before we define the crawlers. The metadata tables that a crawler creates are contained in a database when we define a crawler. If our crawler does not specify a database, our tables are placed in the default database. Frequently asked questions (FAQ)"
    }, {
    "id": 24,
    "url": "http://localhost:4000/amazon-emr/2024/emr-overview",
    "title": "Overview of Amazon EMR",
    "body": "2024/01/16 - What is Amazon EMR?Amazon EMR, formerly known as Amazon Elastic MapReduce, is a managed cluster platform that makes it simpler to process and analyze massive volumes of data (big data) on AWS using big data frameworks such as Apache Hadoop and Apache Spark. Using these big data frameworks and related open-source projects, we can process data for analytics purposes and business intelligence applications. EMR deployment options: Using Amazon EMR, we can deploy our workloads (applications) using:  Amazon EC2 instance(s) Amazon Elastic Kubernetes Service (EKS) On-premises AWS OutpostsWe can run and manage our workloads with the EMR Console, API, SDK or CLI and orchestrate them using Amazon Managed Workflows for Apache Airflow (MWAA) or AWS Step Functions. For an interactive experience, EMR Studio or SageMaker Studio can be used. Amazon EMR also enables the transformation and movement of huge quantities of data into and out of other AWS data storage and databases, such as Amazon S3 and Amazon DynamoDB. EMR file systems: The following are the two primary file systems that are used with Amazon EMR. We specify which file system to use by the prefix of the URI used to access the data.  HDFS - Uses either hdfs:// as prefix or no prefix EMRFS - Uses s3:// as prefix EMRFS file system: EMRFS is an implementation of the Hadoop file system used for reading and writing regular files from Amazon EMR directly to Amazon S3. Amazon EMR use casesThe following are the use cases:  Perform big data analytics Build scalable data pipelines Process real-time data streams Accelerate data science and ML adoptionGetting started with Amazon EMRIn this post, we’ll look at how to set up and launch an Amazon EMR cluster on EC instances. It is beyond the scope of this post to look at how to set up an EMR cluster on EKS or on-premises with AWS Outposts. Prerequisite: Before launching an Amazon EMR cluster, the following actions must be completed:  Sign up for AWS Create an Amazon EC2 key pair for SSH into it - To authenticate and connect to the nodes in a cluster over the SSH protocol, we need to create an EC2 key pair before we launch the cluster.  Amazon EC2 key pairs: A key pair, consisting of a public key and a private key that we use to prove our identity when connecting to an Amazon EC2 instance. Amazon EC2 stores the public key on our EC2 instance (in an entry within ~/. ssh/authorized_keys file), and we (client apps) store the private key. Note that Amazon EC2 doesn’t keep a copy of our private key. Creating Amazon EMR cluster: EMR is built upon a cluster, which is a collection of E2 instances. These instances are generally called nodes. There are different types of nodes in the cluster, each of which has different roles. Amazon EMR node types: There are three types of nodes in an EMR cluster:  Master node (At least one master node) Core node (one or more core nodes) Task node (optional i. e. , zero or more task nodes)Let’s go over each node type in detail. Master node: The master node supervises the cluster and typically runs master application components of distributed applications. For instance, the master node runs the YARN ResourceManager service to manage application resources. It also runs the HDFS NameNode service, tracks the status of jobs submitted to the cluster, and monitors the health of the instances. To monitor the progress of a cluster and interact directly with applications, we can connect to the master node over SSH as the Hadoop user. Connecting to the master node enables direct access to directories and files, such as Hadoop log files. Key takeaways Master node distribute data and tasks among nodes.  Tracks the status of tasks.  Monitors the health of the cluster.  Master node is a must-have node type. Multiple master nodes: With Amazon EMR 5. 23. 0 or later, it is possible to configure a cluster with three master nodes to offer high availability for applications such as YARN Resource Manager, HDFS NameNode, etc. With this capability, the master node is no longer a potential single point of failure. Core nodes: Core nodes are managed by the master node. Core nodes run the Data Node daemon to coordinate data storage as part of the Hadoop Distributed File System (HDFS). In addition, they run the Task Tracker daemon and perform other parallel processing tasks on data required by installed apps. For example, a core node runs YARN NodeManager daemons, Hadoop MapReduce tasks, and Spark executors. There is only one core instance group per cluster, however the instance group may include many nodes operating on several Amazon EC2 instances. With instance groups, we can add and remove EC2 instances while the cluster is running. We can also set up automatic scaling to add instances based on the need. Key takeaways Core node runs the actual data processing tasks via Hadoop aplication services such as Hive, Pig, HBase, and Hue.  Core nodes are responsible for storeing the data.  In case of multi-node cluster, we should have at least one core node. Task nodes: We can utilize task nodes to increase the processing capability of parallel computing jobs on data, such as Hadoop MapReduce tasks and Spark executors.  Note: Task nodes neither execute the Data Node daemon nor store data in HDFS. Key takeaways Task nodes are optional.  Task node runs the actual data processing tasks.  Task nodes are useful when we plan to increase our cluster capacity for a specific job and scale down after its completion.  But doesn’t store the data i. e. , do not have HDFS storage. We’ve learned about the various types of nodes. Launching an Amazon EMR cluster can be accomplished in one of two methods, depending on our preferences and needs:  Launch via the AWS Management Console Launch via the AWS Command Line Interface (AWS CLI)Launch via the AWS Management Console: Follow the key steps outlined below to create an EMR cluster in AWS Management Console:  Login into AWS Management Console.  Go to Services and select EMR. It will take us to the EMR’s home screen.  Click on Create cluster.  Click Go to advanced options if we want to select specific frameworks or tools as shown in Figure 1 below.  Select the type of step to be submitted to the EMR cluster (Optional).  Configure instance group or instance fleet - We specify the configuration of the master, core and task nodes as an instances group or instance fleet.  Select either one of the instance group configurations: Uniform instance groups or instance fleet.  Configure cluster nodes and instances.  Give a name to the cluster.  Under Security Options, choose the EC2 key pair we created earlier.  Click on Create cluster button. It may take a few minutes to launch the EMR cluster after we have clicked the “Create cluster” button. The launch time is determined by a variety of factors, including the number of cluster nodes, the number of applications and frameworks that we have chosen, and so on.                    Figure 1: Amazon EMR - Adanced options.                          Figure 2: Amazon EMR - Adanced options: Cluster Nodes and Instances.       Create a cluster with instance fleets or uniform instance groups: When creating an EMR cluster and specifying the configuration of the master node, core nodes, and task nodes, we have one of the two available configuration choices:  Uniform instance groups Instance fleetsThe configuration option we choose is applicable to all nodes for the lifetime of the cluster. It’s important to note that in a cluster, instance fleets and instance groups cannot coexist. We can only choose one of these. Instance fleetThe instance fleets configuration offers the widest variety of provisioning choices for Amazon EC2 instances. Each node type (master/core/task node) has a single instance fleet, and it is optional to use task instance fleet. Up to 5 EC2 instance type (General purpose instance type, compute optimized instance type, etc. ) per fleet. If the cluster is created using the AWS CLI or Amazon EMR API, we can have up to 30 EC2 instance types per fleet. Uniform instance groupsUniform instance groups offer a simpler setup than instance fleets. When we are creating an EMR cluster, we have the flexibility to group different instance types and assign core or task node roles to them. Because of this, we are not limited to choosing a single instance type for our whole cluster. In general, we are able to use a variety of EC2 instance types for the master node, the core nodes, and the task nodes. This could benefit by autoscaling our cluster in a more effective manner. For example, the task nodes won’t have HDFS, so the instances they use will have greater compute and memory capacity but lower disk capacity. This is because the task nodes won’t have HDFS. Each Amazon EMR cluster can include up to 50 instance groups:  One master instance group that contains one Amazon EC2 instance.  A core instance group that contains one or more EC2 instances.  Up to 48 optional task instance groups. Each core and task instance group can contain any number of Amazon EC2 instances. We can scale each instance group manually by adding and removing Amazon EC2 instances, or we can set up automatic scaling. Having said that, instance groups give us much control and flexibility. Launch via the AWS CLI: We are now aware of how to create and launch an Amazon EMR cluster using the AWS Management Console. Now, let’s create the same using the AWS CLI. Prerequisites:  IAM user AWS CLI must be installed and configured via aws configure command on the client machine to connect to AWS services. Refer here to learn more about how to install and configure the AWS CLI. On the termical, type the following aws command: $ aws emr create-cluster --name emr-cluster-demo \--use-default-roles \--release-label emr-6. 7. 0 \--instance-count 3 \--instance-type c4. large \--applications Name=Spark Name=Hadoop Name=Spark Name=Livy \--ec2-attributes KeyName=&lt;NAME OF KEY PAIR WITHOUT . pem or . ppk EXTENSION&gt; \--log-uri s3://&lt;S3 BUCKET NAME&gt;Arguments: Refer here for a complete list of arguments.  --name: Name of the EMR cluster.  --user-default-roles: Uses EMR_DefaultRole as default EMR role and uses EMR_EC2_DefaultRole as default EC2 instance profile.  --release-label: Create EMR cluster with the given EME version.  --instance-count: Build 1 master node and n number of core nodes. For instance, if the instance-count is 3, then it builds with 1 master node and 2 core nodes.  --instance-type: EC2 instance type e. g. m5. xlarge.  --applications Name=&lt;application name&gt;: Installs the given applications or frameworks.  --ec2-attributes KeyName=&lt;key pair&gt;: KeyName is the EC2 key pair to connect to EC2 instance. Do not provide the key pair file extension such as . pem or . ppk.  --log-uri: Specifies the location in Amazon S3 to which log files are periodically written. If a value is not provided, logs files are not written to Amazon S3 from the master node and are lost if the master node terminates. When we run the above AWS CLI command, we get similar output shown below, which includes the cluster id and cluster arn that were created:  {   ClusterId :  j-2KMYIITE20AWG ,   ClusterArn :  arn:aws:elasticmapreduce:us-east-1:811587835183:cluster/j-2KMYIITE20AWG }Using the ClusterId that was obtained by the previous emr create-cluster command, we can use the following command to check the status of the newly created cluster and find out more about it: aws emr describe-cluster --cluster-id &lt;ClusterId&gt;Authorize inbound traffic: Before we connect to an Amazon EMR cluster, we must authorize inbound SSH traffic (port 22) from trusted clients. In order to do so, edit the managed security group rules for the nodes (master or core nodes) to which we want to connect. The steps that are shown below demonstrate how to include an inbound rule for SSH access inside the default ElasticMapReduce-master security group:  Open the Amazon EMR console at https://console. aws. amazon. com/elasticmapreduce/.  Choose Clusters.  Choose the Name of the cluster we want to modify.  Choose the Security groups for Master link under Security and access. Shown in Figure 3 below.  Choose ElasticMapReduce-master from the list. Shown in Figure 4 below.  Choose the Inbound rules tab and then Edit inbound rules.  Check for an inbound rule that allows public access with the following settings. If it exists, choose Delete to remove it.      Type SSH   Port 22   Source Custom 0. 0. 0. 0/0    Scroll to the bottom of the list of rules and choose Add Rule.  For Type, select SSH. Selecting SSH automatically enters TCP for Protocol and 22 for Port Range. Shown in Figure 5 below.  For source, select My IP to automatically add your IP address as the source address. You can also add a range of Custom trusted client IP addresses, or create additional rules for other clients. Many network environments dynamically allocate IP addresses, so you might need to update your IP addresses for trusted clients in the future.  Choose Save.  Optionally, choose ElasticMapReduce-slave from the list and repeat the steps above to allow SSH client access to core and task nodes.                    Figure 3: Security groups for Master under Security and access.                          Figure 4: Edit ElasticMapReduce-master                         Figure 5: Added SSH 22 to inbound rules      Now we should be able to SSH into the master node using the below command. We can find the master node’s public DNS from the Summary tab in the EMR console as shown in Figure 7 below. ssh -i &lt;key pair&gt; hadoop@&lt;EMR master public DNS&gt;Example: ssh -i my-emr-key. pem hadoop@ec2-xxxxxxx. compute-1. amazonaws. com If we see the screen as shown below in Figure 7 with the name EMR in huge letter, it is wonderful because it indicates that we have successfully constructed and connected to the EMR cluster by using the AWS CLI.                    Figure 6: SSHed into EMR EC2 node                         Figure 7: Finding master node’s public DNS      Understanding Amazon EMR security optionsIn the Security Options section, we are provided with settings that allow us to select EC2 key pairs, IAM roles, security groups, and more. With these configurations, we are able to control who has access to the resources of our cluster and what privileges they have.  EC2 key pair: A key pair, consisting of a public key and a private key that we use to prove our identity when connecting to an Amazon EC2 instance. Amazon EC2 stores the public key on our EC2 instance (in an entry within ~/. ssh/authorized_keys file), and we (client apps) store the private key.  Permissions: By default, EMR creates three roles:     EMR_DefaultRole as the EMR role. It calls or interacts with other AWS services, such as EC2, while creating the cluster.    EMR_EC2_DefaultRole as the EC2 instance profile. It provides access to cluster EC2 instances to access other AWS services such as Amazon DynamoDB, Amazon S3, and more.    EMR_AutoScaling_DefaultRole as the Auto Scaling role. It provides access to add or remove EC2 instances from the cluster when scaling up or down happens through managed scaling or auto scaling policies.     Security configuration: This allows us to specify encryption and authentication options for our cluster. We need to create the configuration before creating the EMR cluster.  EC2 security group: EC2 security groups provide firewall security for our AWS services’ inbound and outbound access. EMR establishes two security groups by default: one for the master node and another for the core and task nodes. Amazon EMR and Apache YARNYARN is one of the most important parts of Hadoop clusters since it not only helps to manage the resources of the cluster but also organizes the execution of jobs across multiple nodes. The core concept behind YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. YARN is used by the majority of Hadoop applications and frameworks in Amazon EMR by default; however, there are a few other applications and frameworks that do not utilize YARN to manage their resources. On each and every node in the cluster, EMR runs an agent that manages YARN components and communicates with Amazon EMR.  Note: It is important to keep in mind that while using spot instances, the chances of task failure are high as they get terminated because of resource unavailability. EMR allows running the ApplicationMaster only on core nodes in order to make jobs more fault-tolerant. This is done so that the ApplicationMaster will not be terminated if a spot node is terminated, and the ApplicationMaster will be able to trigger the failed task in another node. EMR added a built-in YARN node label feature with the 5. 19. 0 release. With this feature, it labels core nodes with the CORE label, configures yarn-site and capacity-schedulers to utilize these labels, and ensures that the ApplicationMaster runs only these nodes. Frequently asked questions (FAQs)What exactly is meant by the term “key pair” when referring to Amazon EC2?: A key pair, also called an EC2 key pair, consists of a public key and a private key that we use to prove our identity when connecting to an Amazon EC2 instance using SSH. We can create a key pair using the AWS Management Console or the AWS CLI. Amazon EC2 stores the public key on our EC2 instance (in an entry within ~/. ssh/authorized_keys file), and we (client apps) store the private key. What is termination protection?: When termination protection is enabled, the cluster cannot be terminated. Before terminating the cluster, termination protection must first be removed explicitly. This helps ensure that EC2 instances are not accidentally terminated. It prevents accidental termination of the cluster. Having said, to shut down the cluster, we must turn off termination protection. Termination protection is especially helpful if our cluster stores data on local disks that we need to retrieve back before we terminate the instances. What is auto-termination?: With auto-termination, we can set a timer to terminate the cluster after a period of inactivity or after out steps are done. This would allow us to save cost on unsed instances. How to archive Amazon EMR cluster log files?: We an configure a cluster to periodically archive the log files stored on the master node to Amazon S3. This guarantees that the log files are available after the cluster has terminated, whether normally or as a result of an error. Amazon EMR archives the log files to Amazon S3 at 5 minute intervals. To have the log files archived to Amazon S3, we must enable this feature when we launch the cluster:                    Figure 3: Amazon EMR - Archiving cluster logs to S3.       Which cluster storage(S3 vs. HDFS) should we use?: We know that EMR has the freedom to choose either HDFS or S3 as the persistent storage for the cluster. HDFS as cluster persistent storage: When our cluster utilizes HDFS as its permanent storage, there are certain aspects that need to be taken into consideration:  To ensure that our data is protected against loss in the event of a core node failure, we must always keep three copies of it.  Since an EMR cluster is only ever deployed in a single Availability Zone (AZ) within a Region, any failure that affects the whole AZ might result in the loss of data.  Our storage cost will vary depending on the EBS volumes we use since HDFS is constructed using the EBS volumes of the core nodes.  Since the data is kept locally, the cluster needs to be accessible 24x7 even if there are no processes currently running that make use of the cluster’s capacity. Amazon S3 as a persistent data storage: When we utilize Amazon S3 as the persistent storage layer, we gain a number of advantages, some of which are shown below:  With S3 being a persistent storage, we get increased reliability as a result of S3’s multi-AZ replication.  Our cluster is protected against the possibility of data loss as a result of a failed node, cluster, or AZ.  Compared to EBS-based HDFS, S3 is substantially less expensive, which brings our total costs down.  We are now able to terminate clusters when they are not in use and multiple clusters can point to the same dataset in S3 as a result of the decoupling of compute and storage.  Note: Amazon S3 places a limit on the number of write or read requests that may be processed in one second. We are able to do 3,500 GET/HEAD requests or 5,500 PUT/COPY/POST/DELETE requests per second in an Amazon S3 bucket for each prefix. There are no limits to the number of prefixes that we can have in our bucket. So while writing output, we may consider adding more S3 prefixes in order to prevent exceeding the maximum limit. What is step execution?: A step is a unit of work we submit to the cluster. It’s optional. For instance, a step might contain one or more Hadoop or Spark jobs. We an also submit additional steps to a cluster after it is running. There are various step types:  Streming program Hive program Spark application Pig program Custom JARWhat is bootstrap actions?: Bootstrap actions are scripts that are executed on every cluster node during the setup process before Hadoop starts. We can use them to install additional software and customize our applications. "
    }, {
    "id": 25,
    "url": "http://localhost:4000/data-quality/2023/gx-takes-great-care-with-data-quality",
    "title": "Great Expectations takes great care of your data quality",
    "body": "2023/04/01 - Data qualityInformation is only valuable if it is of high quality. How can we get data of such high quality, then? The answer is simple: testing the data quality is what assures high quality. We know the “what” part of it gets us quality, but the “how” part is much more crucial in producing high-quality data. Data quality dimensions: Data quality focuses on ensuring that the data conforms to the six data quality dimensions listed below. These data quality dimensions are useful guidelines for enhancing the quality of data assets.  Accuracy - What degree of fact does a piece of information have? Completeness - The state of being complete and entire.  Consistency - Does information stored in one place match relevant data stored elsewhere? Timeliness - Is our information available when it’s needed? Validity - Invalid data affects the accuracy: Is information is a certain format, do business standards or rules apply to the information, or is it in an unusable format? Uniqueness - Ensures duplicate or overlapping data is identified and removed. Testing data: Similar to unit testing in software engineering, data testing has to become a regular practice in data engineering. A data acceptance procedure may be established by the organization, according to which data cannot be utilized unless its owners provide evidence that it satisfies the organization’s quality standards. Data quality testing stages:  First stage: The first place that we need to test data is at the point of ingestion. When ingesting data, we want to be sure that all of the data has successfully moved from its source to the target destination.  Second stage: The second place that we need to test data is at the point of transformation. With transformation testing, we will typically check pre- and post-conditions, such as:     Null checks   Valid value checks   Schema checks   Referential integrity checks, and so on.    Data quality tools: There are various data quality tools—both commercial and open source—that are currently on the market. This blog focuses on one of the hand-picked open source data quality tools called Great Expectations, among other open source tools. We’ve taken into account the following factors while evaluating the open source data quality tools:  Is the tool able to deal with all six data quality dimensions? How easy is it for both data engineers and data analysts to learn how to write data quality checks or tests and get good at them quickly? How well the documentation and API guides are supported? How flexible and extensible the tool is How active and responsive is the respective community in Slack? In general, Slack is considered to be far more helpful than submitting our questions elsewhere. We could ask questions and receive straight answers from committers, which is one of the best things about Slack.  The rate at which the tool is evolving and gaining new capabilities.  Last but not least, which one of them is more widely used across enterprises?Let’s take a closer look at Great Expectations to see how it might assist us in obtaining reliable data. Introduction to Great Expectations (GX)Great Expectations, shortly referred to as GX, is a powerful and flexible open-source data quality solution on the market today. We’ll take a close look at it with examples to demonstrate how powerful and flexible GX tool is. Unlike traditional unit tests, GX applies tests to data instead of code. To put it simply, in GX, testing is performed on data rather than code. It’s a Python library that enables us to verify that our data is accurate by validating, documenting, and profiling it.  Profiling, or data profiling, is the process of examining, analyzing, and creating useful summaries of data that aid in the discovery of data quality issues. The best part about Great Expectations is that, unlike other data quality tools, we do not need to write the configuration. Instead, Great Expectations comes with the Jupyter Notebook, which will help us generate various configurations for us. At a high level, there are four stages in Great Expectations:  Setup Connect to Data Create Expectations Validate Data                   Figure 1: Four stages of Great Expectations.       Various activities go into each stage as shown below. We will go into great detail on each of these activities later on. For now, it’s crucial to understand the various concepts and terms used in Great Expectations.                    Figure 2: Various activities go into each stage.       Data Context: A Data Context is the primary entry point for a Great Expectations. Our Data Context provides us with methods to configure our Stores, plugins, and Data Docs. It also provides the methods needed to create, configure, and access our Datasources, Expectations, Profilers, and Checkpoints.  In addition to all of that, it internally manages our Metrics, Validation Results, and the contents of your Data Docs for us. Expectations, Profilers, Checkpoints, Metrics, and Validation Results will all be covered in greater depth later on. Data Context can be initialized using the CLI, created, loaded, and saved for future use. Initialize our Data Context with the CLI: The simplest way to create a new Data Context is by using Great Expectations’ CLI. Run the following command from the directory where we wish to initialize Great Expectations: great_expectations initThe above command causes Great Expectations to create the directory structure and configuration files required for us to go on. Great Expectations will create a new directory with the following structure: great_expectations  |-- great_expectations. yml  |-- expectations  |-- checkpoints  |-- plugins  |-- . gitignore  |-- uncommitted    |-- config_variables. yml    |-- data_docs    |-- validationsCreate our Data Context: Use the following Python statements to create a new Data Context: import great_expectations as gxcontext = gx. get_context() # Creating a DataContext object# The below statement is the same as above with a variable-type annotation. # It's a more clean way of coding. # context: gx. DataContext = gx. get_context()Load the existing Data Context: Load an on-disk Data Context via: import great_expectations as gxcontext = gx. get_context(  context_root_dir='path/to/my/context/root/directory/great_expectations')Save the Data Context for future use: We obtained a temporary, in-memory Ephemeral Data Context from gx. get context() since we had not previously initialized a Filesystem Data Context (using great_expectations init) or specified a path at which to create one (via gx. get_context(context_root_dir='path/great_expectations')). To save this Data Context for future use, we will convert it to a Filesystem Data Context: context = context. convert_to_file_context()We can also provide the path to a specific folder where we want the Filesystem Data Context to be initialized. Datasource: GX provides better connectivity with a wide variety of data sources and data manipulation frameworks like Apache Spark and Pandas. It provides a unified Datasource API that connects and interacts across multiple data sources. The term “unified” denotes that the Datasource API remains the same across all data sources, such as PostgreSQL, CSV filesystems, and others. This unified Datasource API makes working with all data sources very convenient. Having said that, our primary tool for connecting to data is the Datasource. Under the hood, Datasources uses a Data Connector and an Execution Engine to connect to a wide variety of external data sources and perform computation, respectively. The Datasource provides an interface for a Data connector and an Execution Engine to work together. Each Datasource must have an Execution Engine and one or more Data Connectors configured. Thanks to the unified Datasource API, once a Datasource is configured, we will be able to operate with the Datasource’s API rather than needing a different API for each data source we may be working with. Data Connector: Datasource leverages the Data Connector, which facilitates access to external data sources such as databases, filesystems, and cloud storage. A Data Connector is an integral element of a Datasource. Great Expectations provides three types of DataConnector classes:  InferredAssetDataConnectors ConfiguredAssetDataConnectors RuntimeDataConnectorInferredAssetDataConnectors:  Infers data_asset_name by using a regex that takes advantage of patterns that exist in the filename or folder structure. ConfiguredAssetDataConnector:  It allows us to specify that we have multiple Data Assets in a Datasource, but also requires an explicit listing of each Data Asset we want to connect to.  Allows users to have the most fine-tuning, and requires an explicit listing of each Data Asset we want to connect to. There are different Data Connector classes that exist both for InferredAssetDataConnectors and ConfiguredAssetDataConnector:  InferredAssetFilesystemDataConnector and ConfiguredAssetFilesystemDataConnector InferredAssetFilePathDataConnector and ConfiguredAssetFilePathDataConnector InferredAssetAzureDataConnector and ConfiguredAssetAzureDataConnector InferredAssetGCSDataConnector and ConfiguredAssetGCSDataConnector InferredAssetS3DataConnector and ConfiguredAssetS3DataConnector InferredAssetSqlDataConnector and ConfiguredAssetSqlDataConnector InferredAssetDBFSDataConnector and ConfiguredAssetDBFSDataConnectorInferredAssetDataConnectors and ConfiguredAssetDataConnectors are used to define Data Assets and their associated data_references. A Data Asset is an abstraction that can consist of one or more data_references. For instance, we might have a yellow_tripdata Data Asset containing information about taxi rides, which consists of twelve data_references. RuntimeDataConnector:  A RuntimeDataConnector is a special kind of Data Connector that enables us to use a RuntimeBatchRequest to provide a Batch’s data directly at runtime.  The RuntimeBatchRequest can wrap either an in-memory dataframe, filepath, or SQL query, and must include batch identifiers that uniquely identify the data. For example, a run_id from an AirFlow DAG run.  The batch identifiers that must be passed in at runtime are specified in the RuntimeDataConnector’s configuration. Execution Engine: Execution Engine provides computing resources that will be used to perform Validation. Great Expectations can take advantage of different Execution Engines, such as Pandas, Spark, or SqlAlchemy. Various Execution Engine’s class names are listed below. We will discuss in the later section where we will use these Execution Engine classes.  Pandas - PandasExecutionEngine Spark - SparkDFExecutionEngine SqlAlchemy - SqlAlchemyExecutionEngineThe following shows the high-level workflow:                    Figure 3: Datasource - How it works?      Create a new Datasource through the CLI: Run the below command to create a new Datasource: great_expectations datasource newThe above command will bring up the following prompt: What data would you like Great Expectations to connect to?  1. Files on a filesystem (for processing with Pandas or Spark)  2. Relational database (SQL): 1We can get data either way:  From the filesystem (a file-based Datasource) using Pandas or Spark From a relational databaseFile-based Datasource: For file-based Datasource, the configuration contains an InferredAssetFilesystemDataConnector, which will add a Data Asset for each file in the base directory we provided. It also contains a RuntimeDataConnector, which can accept file paths. Note that we can customize it as we wish. In the case of Spark as the processing engine, the following is the Datasource configuration (as a Python string): datasource_yaml = f   name: { my_datasource }class_name: Datasourceexecution_engine: class_name: SparkDFExecutionEnginedata_connectors: default_inferred_data_connector_name:  class_name: InferredAssetFilesystemDataConnector  base_directory: . . /data  default_regex:   group_names:    - data_asset_name   pattern: (. *) default_runtime_data_connector_name:  class_name: RuntimeDataConnector  assets:   my_runtime_asset_name:    batch_identifiers:     - runtime_batch_identifier_name   Create a Filesystem Datasource (Python): A Filesystem Datasource can be created with two pieces of information:  name: The name by which the Datasource will be referenced in the future base_directory: The path to the folder containing the files the Datasource will be used to connect todatasource_name =  my_spark_datasource raw_data_files =  . /data Next, pass both data source name and data directory path as parameters when we create our Datasource: datasource = context. sources. add_spark_filesystem(  name = datasource_name,   base_directory = raw_data_files )This creates a SparkFilesystemDatasource object shown below: SparkFilesystemDatasource(	type='spark_filesystem', 	name='my_spark_datasource', 	id=None, 	assets={}, 	base_directory=PosixPath('data'), 	data_context_root_directory=None)Test our Datasource configuration: Use context. test_yaml_config(. . . ) to test our Datasource configuration as shown below: context. test_yaml_config(yaml_config = datasource_yaml)In the above Python statement, context is the Data Context object, which can be created as follows: import great_expectations as gxcontext = gx. get_context()# The below statement is the same as above with a variable-type annotation. # It's a more clean way of coding. # context: gx. DataContext = gx. get_context()Save our Datasource configuration: Here we save our Datasource in our Data Context once we are satisfied with the configuration using the following Python statement: from great_expectations. cli. datasource import sanitize_yaml_and_save_datasourcesanitize_yaml_and_save_datasource(context, datasource_yaml, overwrite_existing=False)Note that overwrite_existing defaults to False, but we can change it to True if we wish to overwrite the configuration. Please note that if we wish to include comments we must add them directly to our great_expectations. yml. Data Asset: A Data Asset is a logical collection of records within a Datasource. Often, Data Assets are tied to already-existing data that has a name (e. g. , “the UserEvents table”). Also, Data Assets can slice the data one step further (subsets) (e. g. , “new records for month within the UserEvents table. ”). Great Expectations protects the quality of Data Assets. More Data Asset examples are:  In a SQL database, a Data Asset may be the rows from a table grouped by the week.  In an S3 bucket or filesystem, a Data Asset may be the files matching a particular regex pattern. We can define multiple Data Assets built from the same underlying data source to support different workflows or use cases. To put it simply, the same data can be in multiple Data Assets. For instance, we may have different Expectations of the same raw data for different purposes. Not all records in a Data Asset need to be available at the same time or same place. A Data Asset could be built from:  Streaming data that is never stored Incremental deliveries Incremental updates Analytic queries Replacement deliveries or from a one-time snapshotThat implies that a Data Asset is a logical concept. So no matter where the data comes from originally, Great Expectations validates batches of data. Batch: A Batch is a discrete selection or subset of records from a Data Asset. Providing a Batch Request to a Datasource results in the creation of a Batch. A Batch adds metadata to precisely identify the specific data included in the Batch. With the help of these metadata, a Batch can be identified by a collection of parameters, such as the date of delivery, the value of a field, the time of validation, or access control permissions. Batch Request: A Batch Request is sent to a Datasource in order to create a Batch. A Batch Request contains all the necessary details to query the underlying data. A Batch Request will return all matching Batches if it finds more than one that satisfy the requirements of the user-provided batch identifiers. A Batch Request is always used when Great Expectations builds a Batch. When a Batch Request is passed to a Datasource, the Datasource will use its Data Connector to build a Batch Spec, which is an Execution Engine-specific description of the Batch. Datasource’s Execution Engine will use this Batch Spec to return a Batch of data. We will rarely need to access an existing Batch Request. Instead, we often find ourself defining a Batch Request in a configuration file, or passing in parameters to create a Batch Request which we will then pass to a Datasource. Once we receive a Batch back, it is unlikely we will ever need to reference to the Batch Request that generated it. In fact, if the Batch Request was part of a configuration, Great Expectations will simply initialize a new copy rather than load an existing one when the Batch Request is needed. How to create a Batch Request: Batch Requests are instances of either a RuntimeBatchRequest or a BatchRequest. A BatchRequest can be defined by passing a dictionary with the necessary parameters when a BatchRequest is initialized. from great_expectations. core. batch import BatchRequestbatch_request_parameters = { 'datasource_name': 'my_datasource', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'my-data-under-test. csv', 'limit': 1000 # Optional}batch_request = BatchRequest(**batch_request_parameters)Expectation: An Expectation is a test assertion that we can run against our data under test. Like unit test assertions in most of the programming languages, Expectations provide a flexible, declarative language for describing expected behavior. Unlike traditional unit tests, Great Expectations applies Expectations to data instead of code. Each Expectation is a declarative test assertion about the expected format, content, or behavior of our data under test. The test assertions are both human-readable and machine-verifiable assertions.  Expectation Gallery: Great Expectations comes with many built-in Expectations, and we can also develop our own custom Expectations. As an example, we could define an Expectation that states that a column has no null values. Great Expectations would then compare our data to that Expectation and report if a null value was found. Various ways of creating Expectations: There are a few workflows we can follow when creating Expectations. These workflows represent various ways of creating Expectations. There are four potential ways to create Expectations as shown below:  Interactive workflow (with inspecting data) (Recommended) Data Assistant workflow (with inspecting data) (Recommended) Manually define our Expectations (without inspecting data) (Default) Custom scriptsInteractive workflow: In this workflow, we will be working in a Python interpreter or Jupyter Notebook. In this case, we will navigate to our Data Context’s root directory in our terminal, where we will use the CLI to launch a Jupyter Notebook, which will assist us in the process. We will use a Validator and call expectations methods on it to define Expectations in an Expectation Suite. When we have finished, we will save that Expectation Suite in our Expectation Store. Use the CLI to begin the interactive process of creating Expectations. From the root folder of our Data Context, enter the following command: great_expectations suite newThis will bring up the following prompt: How would you like to create your Expectation Suite?  1. Manually, without interacting with a sample Batch of data (default)  2. Interactively, with a sample Batch of data  3. Automatically, using a Data Assistant:To start the Interactive Mode workflow, enter 2. Note that we can skip the above prompt by including the flag --interactive in our command-line input: great_expectations suite new --interactiveData Assistant workflow: In this workflow, we will use a Data Assistant to generate Expectations based on some input data. In this case, we will be working in a Python environment, so we will need to load or create our Data Context as an instantiated object. Next, we will create a Batch Request to specify the data we would like to Profile with our Data Assistant. Create a Batch RequestThis is how we create a BatchRequest: from great_expectations. core. batch import BatchRequestbatch_request_parameters = { 'datasource_name': 'my_datasource', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'my-data-under-test. csv', 'limit': 1000 # Optional}batch_request = BatchRequest(**batch_request_parameters) Caution: The Onboarding Data Assistant will run a high volume of queries against our Datasource. Data Assistant performance can vary significantly depending on the number of Batches, count of records per Batch, and network latency. It is recommended that we start with a smaller BatchRequest if we find that Data Assistant runtimes are too long. Prepare a new Expectation SuitePreparing a new Expectation Suite is done with the Data Context’s add_or_update_expectation_suite(. . . ) method as shown below: expectation_suite_name =  my_onboarding_assistant_suite expectation_suite = context. add_or_update_expectation_suite(  expectation_suite_name = expectation_suite_name)Run the Onboarding Data AssistantNext, run the Onboarding Data Assistant. Running a Data Assistant is as simple as calling the run(. . . ) method for the appropriate assistant. There are numerous parameters available for the run(. . . ) method of the Onboarding Data Assistant. For instance, the exclude_column_names parameter allows us to provide a list columns that should not be Profiled. In addition, we can also use other parameters, such as include_column_names, include_column_name_suffixes, and cardinality_limit_mode. The following code shows how to run the Onboarding Assistant. data_assistant_result = context. assistants. onboarding. run(  batch_request = batch_request,  exclude_column_names = [col3, col6, col9],  include_column_names = [col1, col2, col4, col5, col7, col8, col10])Save our Expectation SuiteOnce we have executed the Onboarding Data Assistant’s run(. . . ) method and generated Expectations for our data, we need to load them into our Expectation Suite and save them. We will do this by using the Data Assistant result: expectation_suite = data_assistant_result. get_expectation_suite(  expectation_suite_name = expectation_suite_name)Once the Expectation Suite has been retrieved from the Data Assistant result, we can save it as shown below: context. add_or_update_expectation_suite(expectation_suite = expectation_suite)Test our Expectation Suite with a SimpleCheckpointTo verify that our Expectation Suite is working, we can use a SimpleCheckpoint with the Expectation Suite and Batch Request that we have already defined: checkpoint_config = {   class_name :  SimpleCheckpoint ,   validations : [    {       batch_request : batch_request,       expectation_suite_name : expectation_suite_name,    }  ],}Once we have our SimpleCheckpoint’s configuration defined, we can instantiate a SimpleCheckpoint and run it. We can check the  success  key of the SimpleCheckpoint’s results to verify that our Expectation Suite worked. checkpoint = SimpleCheckpoint(  f my_{expectation_suite_name} ,  context,  **checkpoint_config,)checkpoint_result = checkpoint. run()assert checkpoint_result[ success ] is TruePlot and inspect the Data Assistant’s calculated Metrics and produced ExpectationsTo see Batch-level visualizations of Metrics computed by the Onboarding Data Assistant run: data_assistant_result. plot_metrics()To see all Metrics computed by the Onboarding Data Assistant run: data_assistant_result. metrics_by_domainTo plot the Expectations produced, and the associated Metrics calculated by the Onboarding Data Assistant run: data_assistant_result. plot_expectations_and_metrics() Note: If no Expectation was produced by the Data Assistant for a given Metric, neither the Expectation nor the Metric will be visualized by the plot_expectations_and_metrics() method. To see the Expectations produced and grouped by Domain run: data_assistant_result. show_expectations_by_domain_type(  expectation_suite_name = expectation_suite_name)To see the Expectations produced and grouped by Expectation type run: data_assistant_result. show_expectations_by_expectation_type(  expectation_suite_name = expectation_suite_name)Manually define our Expectations: This workflow is for advanced users who want to manually (without inspecting data) define the Expectations by writing the configurations. While source data is not necessary for this approach, a thorough understanding of the Expectations configurations is necessary. Create Expectation Configurations as shown below: expectation_configuration = ExpectationConfiguration(  expectation_type =  expect_column_values_to_be_in_set ,  kwargs = {    column :  transaction_type ,    value_set : [ purchase ,  refund ,  upgrade ]  },)Then, add the Expectation to the suite as shown below: suite. add_expectation(expectation_configuration = expectation_configuration)Custom scripts: Some advanced users have also taken advantage of this workflow, and have written custom methods that allow them to generate Expectations based on the metadata associated with their source data systems. Expectation Suite: Expectations are grouped into Expectation Suites, which can be stored and retrieved using an Expectation Store. The most critical aspect of Great Expectation is creating Expectation, or Expectation Suites. Note that a local configuration for an Expectation Store will be added automatically to great_expectations. yml when we initialize our Data Context for the first time. We can change this configuration to work with different Stores. Generally, we will not need to interact with an Expectation Store directly. Instead, our Data Context will use an Expectation Store to store and retrieve Expectation Suites behind the scenes. This means, we most likely use convenience methods in our Data Context to retrieve Expectation Suites. Create a new Expectations Suite: The below shows how to create an Expectations Suite using the CLI. Run the below command from Data Context: great_expectations suite newThe above command will bring up the following prompt: How would you like to create your Expectation Suite?  1. Manually, without interacting with a sample Batch of data (default)  2. Interactively, with a sample Batch of data  3. Automatically, using a Data AssistantStore: A Store is a location to store and retrieve information about metadata in Great Expectations. Great Expectations supports a variety of Stores for different purposes, but the most common Stores are:  Expectation Stores - Used to store and retrieve information about collections of test assertions about data.  Validations Stores - Used to store and retrieve information about objects generated when data is Validated against an Expectation Suite.  Checkpoint Stores - Metric Stores Evaluation Parameter Stores Data Docs StoresExpectation Store: Expectation Stores allow us to store and retrieve Expectation Suites. These Stores can be accessed and configured through the Data Context, but entries are added to them when we save an Expectation Suite. Validator: A Validator is the object responsible for running an Expectation Suite against data. In other words, we use a Validator to access and interact with our data. Checkpoints, in particular, use Validators when running an Expectation Suite against a Batch Request. However, we can also use our Data Context to get a Validator to use outside a Checkpoint - for instance, to create Expectations interactively in a Jupyter Notebook. Also, we can use the Validator to verify our Datasource. To verify a new Datasource, we can load data from it into a Validator using a Batch Request. Note that Validators don’t require additional configuration. Provide one with an Expectation Suite and a Batch Request, and it will work out of the box! Instantiate our Validator: The code shows how to instantiate a Validator: from great_expectations. core. batch import BatchRequestexpectation_suite_name =  insert_the_name_of_your_suite_here # Setting Batch Request configurationbatch_request_parameters = { 'datasource_name': 'my_datasource', 'data_connector_name': 'default_inferred_data_connector_name', 'data_asset_name': 'my-data-under-test. csv', 'limit': 1000 # Optional}validator = context. get_validator(  batch_request = BatchRequest(**batch_request_parameters),  expectation_suite_name = expectation_suite_name)After we get our Validator instantiated, we can call validator. head() to confirm that it contains the data that we expect. Checkpoint: In a production deployment of Great Expectations, a Checkpoint serves as the primary means for validating data. Checkpoints provide a convenient abstraction for bundling the Validation of a Batch (or Batches) of data against an Expectation Suite (or several), as well as the Actions (optional) that should be taken after the validation. Checkpoints have their own Store which is used to persist their configurations to YAML files. These configurations can be committed to version control. A Checkpoint uses a Validator to run one or more Expectation Suites against one or more Batches provided by one or more Batch Requests. Running a Checkpoint produces Validation Results and will result in optional Actions being performed if they are configured to do so. Create a Checkpoint: Using CLI to open a Jupyter Notebook for creating a new Checkpoint: The Great Expectations CLI has a convenience method that will open a Jupyter Notebook to easily configure and save our Checkpoint. Run the following CLI command from our Data Context: great_expectations checkpoint new my_checkpointWe can replace my_checkpoint in the above command with whatever name we would like to associate with the Checkpoint we will be creating. After running this command, a Jupyter Notebook will open, which will guide us through the procedure for creating a Checkpoint. We can modify the default setup of this Jupyter Notebook to fit our use case. Edit the existing Checkpoint configuration: The following shows the minimum required Checkpoint configuration generated by Jupyter Notebook, which uses the SimpleCheckpoint class that takes care of some defaults. The following example shows the YAML configuration as a Python string. config =    name: my_checkpoint # This is populated by the CLI. config_version: 1class_name: SimpleCheckpointvalidations: - batch_request:   datasource_name: my_datasource # Update this value.    data_connector_name: my_data_connector # Update this value.    data_asset_name: MyDataAsset # Update this value.    data_connector_query:    index: -1  expectation_suite_name: my_suite # Update this value.    We need to replace the names my_datasource, my_data_connector, MyDataAsset and my_suite with the respective Datasource, Data Connector, Data Asset, and Expectation Suite names we have configured in our great_expectations. yml. Validate and test our Checkpoint configuration: We can use the following Python statement to validate the contents of our config yaml string mentioned above: context. test_yaml_config(yaml_config=config)Here, context represents Data Context object. When executed, test_yaml_config(. . . ) will instantiate the passing component and run through a self-check procedure to verify that the component works as expected. In the case of a Checkpoint, this means:  Verifying that the Checkpoint class with the given configuration, if valid, can be instantiated.  Printing warnings in case the configuration is invalid or incomplete.  Raise error if our configuration was not set up correctly. Store our Checkpoint configuration: After we are satisfied with our Checkpoint configuration, we can store it in our Checkpoint Store. Run our Checkpoint and open the Data Docs: Before running a Checkpoint, make sure that all classes and Expectation Suites referred to in the configuration exist. We can use the below Python statement to run our Checkpoint. context. run_checkpoint(. . . )Great Expectations in detailInstalling GX OSS: The following steps show how to install GX OSS (open source software) locally on our desktop. Prerequisites:  A supported version of Python (versions 3. 7 to 3. 10) The ability to install Python packages with pipInstall GX using pip as shown below: python -m pip install great_expectationsGX and its associated dependencies will be installed by pip when we run the above command from the terminal. This may take a moment to complete. For best practices, set up a virtual workspace for our project. Let’s get into the fundamentals of writing test cases, validating them against the data, and generating test reports. The following shows various steps in the order they are given. Each step consists of a series of required and optional actions. Data Context |-- Initialize a Data Context with the CLI |-- Create a DataContext object |-- Load the existing Data Context |-- Save the Data Context for future useDatasource |-- Create a new Datasource through the CLI    |-- Filesystem datasource via Pandas or Spark    |-- Relational database |-- Test our Datasource configuration (Optional) |-- Save our Datasource configuration (Stored in great_expectations. yml)Data Asset (Subset of Datasource) (Optional) |-- Add a Data Asset to the DatasourceBatch Request |-- Create a Batch Request (Has Datasource details) |-- Create a Batch Request from Data Asset (In case of Data Asset)Expectation and Expectation Suite |-- Create via interactive workflow (With inspecting data) (Recommended) |-- Create via Data Assistant workflow (With inspecting data) (Recommended) 	  |-- Create a Batch Request (Has Datasource details) 	  |-- Prepare a new Expectation Suite 	  |-- Run the Onboarding Data Assistant (Uses Batch Request) 	  |-- Save our Expectation Suite 	  |-- Test our Expectation Suite with a SimpleCheckpoint 	  |-- Plot and inspect the Data Assistant's calculated Metrics and produced Expectations |-- Manually define our Expectations (Without inspecting data) (Default) |-- Custom scriptsValidator |-- Instantiate our Validator (Passing Batch Request and Expection Suite)Checkpoint |-- Create a new Checkpoint using CLI to open a Jupyter Notebook |-- Edit the existing Checkpoint configuration |-- Validate and test our Checkpoint configuration |-- Store our Checkpoint configuration |-- Run our Checkpoint and open the Data DocsFrequently asked questions (FAQs)How gx. get_context() works?: Load an on-disk Data Context from a great_expectations. yml configuration via the get_context() command. "
    }, {
    "id": 26,
    "url": "http://localhost:4000/data-quality/2023/data-quality-intro",
    "title": "Introduction to Data Quality",
    "body": "2023/03/09 - Overview"
    }, {
    "id": 27,
    "url": "http://localhost:4000/scala/2023/introduction-to-async-programming-in-scala",
    "title": "Introduction to Asynchronous Programming in Scala",
    "body": "2023/01/20 - OverviewBecause it might be difficult to understand at first, “asynchronous” is a topic that both beginners and seasoned developers frequently pay less attention to or totally avoid. Another reality is that we frequently work toward achieving the desired program results without paying as much attention to identifying the most effective means of achieving those results. Naturally, when we think of the intended result, the lines of instructions or statements are the first that come to mind. We begin with a statement-focused approach as opposed to a performance-focused one. Let’s first understand certain fundamentals before moving on to the concept of asynchronous programming:  A program vs. a process What is a thread in programming? What is multi-thread? How a thread differs from a process?Program vs process: Although I may often use the terms “program” and “process” interchangeably here, they are not the same in practice. A program is passive (not active) in nature because it does nothing until it is executed. It contains the set of instructions that are used to carry out particular tasks. These instructions are stored in a file or files in secondary memory, such as on a disk. A program starts, executes a series of instructions, and ends. When a program is executed, an active instance of the program is launched in the primary memory (RAM). This active instance is referred to as a process. So a process is a programme in execution. Unlike a program, a process needs resources like a CPU, memory, and IO to perform the task. What is a thread in programming?: A program or an application starts, executes a series of instructions, and ends. This is referred to as a program’s life cycle. With a single-core processor, the instructions are carried out in a traditional monolithic manner: sequentially, one after the other. As multi-core computers become more common these days, a programme can construct multiple execution paths very easily by using threads.                    Figure 1: An application’s life cycle.       A thread itself is not a program or application. This means, unlike a program, a thread cannot run on its own. A thread is the smallest and most lightweight sequence of instructions intended to be scheduled and executed by an operating system component called a scheduler independently of the parent process. Simply put, a thread is a sequential flow of tasks within a process. We can think of a process as a container where one or more threads run. It’s like french fries in a box, where the fries are threads and the box is the process :-)                    Figure 2: French fries in a box.       From a thread standpoint, there is a possibility of only one french fry in a box, though we don’t like it and definitely want more. It’s referred to as a single-threaded process. Having said that, every process has at least one thread. Having said that, a program under execution is referred to as a process, and a threat is a fundamental unit of execution. Since a thread deals solely with the execution of tasks, we can also say that a thread is a fundamental unit of CPU utilisation. Keep in mind that each CPU only executes one thread at a time.  Definition: A thread is a fundamental unit of CPU execution. It’s a single sequential flow of control within a process. Processes vs. threads: We are aware that a process and a thread are not the same now. Processes and threads vary from one another in the following ways:  A process is typically a big, independent entity, while threads exist as an integral part of a process.  A process carries a lot more state information than threads do, and multiple threads inside a process share the process state. In other words, threads share the process’s resources, including memory and open files.  Context switching most likely takes longer for processes while it takes less time for threads.  Compared to threads, processes use more resources.  The creation and termination of a process takes longer than that of a thread.                    Figure 3: A process with two threads of execution.       From the above thread definition, let’s understand what “flow of control” is.  Flow of control: In computer programming, “control flow” or “flow of control” refers to the order in which a program’s statements are executed or evaluated when a program is running. Every program has one or more threads of execution, which can be used to carry out various tasks concurrently or almost concurrently. As mentioned above, usually the operating system itself manages these threads of execution, scheduling them to run on the available cores and preemptively interrupting (stoping) them as necessary. Are we saying “stop the running threads”? Yes, you got it right. The need for interruption will be discussed further down, but first, let’s understand what preemption in threading means.  Preemption: Preemption in computing is the act of temporarily interrupting (stoping) an executing task with the intention of resuming it later from where it left off. Typically, this interruption is done by an external scheduler with no assistance or cooperation from the task. Here comes a question: Why should a running thread be preempted? The running thread is often preempted when a high-priority thread is added to the ready queue. A thread typically enters the READY state after its block condition is resolved. If two threads of the same priority are waiting for the CPU, the scheduler randomly chooses one of them to run. We will have no control over which goes first. So the objective of preempting a running thread is to free up resources so that other threads can execute. TODO: More more about context switching for concurrency. Before continuing, it’s crucial to understand how concurrency and parallelism differ from one another. Concurrency and parallelism: TODO Components of thread: A thread must allocate some of its own resources inside a running program. For instance, it must have its own execution stack and program counter. Having said that, a thread comprises the following components:  A thread ID Set of registers, including a program counter (PC) A stack spaceApart from these, a thread shares with other threads belonging to the same process:  its code its data other operating system resources, such as open files and signalsLet’s understand each of the main components in a little more detail. Registers: Registers are a type of high-speed, small amout of memory storage contained within the computer processor (CPU). It’s a component of the CPU and not RAM, or main memory.                    Figure 4: Registers are integral part of CPU.       The processor makes use of these registers to store small amounts of data that are needed during processing, such as:  Current instruction being decoded Address of the next instruction to be executed Results of calculationsA register must be large enough to hold an instruction; for instance, on a 64-bit computer, a register must be 64 bits long. A processor often contains several kinds of registers, which can be classified based on the values they can store or the instructions that can be executed on them. The number of registers used by different processors varies, although the majority of them have some—if not all—of the following:  Program counter Accumulator Address register Data register Temporary register Input register Output registerTypes of thread: TODO Single thread vs. multi thread: A single-threaded process (a process with only one thread of execution) follows a single sequence of control while executing, but a multi-threaded process has several sequences of control and is thus able to perform multiple independent tasks concurrently. The aim of multithreading is to split a single process into multiple threads as opposed to starting a whole new process. Multithreading is used to achieve real parallelism and improve the performance of the application.                    Figure 5: Single-threaded process vs. multi-threaded process.       Each thread in a multi-threaded process has its own program counter, stack, and set of registers, but they all share common code, data, and OS resources like open files.  Process-based multitasking: There are two distinct types of multitasking: process-based and thread-based. The distinction between the two must be understood. As mentioned earlier, a process is essentially an active program (under execution). A process-based multitasking is a capability that enables our computer to run two or more programs concurrently. For instance, using an IDE or text editor while running any compiler is possible with process-based multitasking. In process-based multitasking, an entire program itself is the smallest unit of code that can be managed by the OS scheduler. While thread-based multitasking takes care of the details, process-based multitasking deals with the big picture. Motivation:  Multi tasks: In modern programming, threads are immensely useful anytime a process needs to perform multiple tasks independently of one another.  Non-blocking nature: Threads offer a non-blocking approach, allowing the other tasks to continue without interruption when one of the tasks blocks. References https://medium. com/swift-india/concurrency-parallelism-threads-processes-async-and-sync-related-39fd951bc61d https://jenkov. com/tutorials/java-concurrency/single-threaded-concurrency. html#:~:text=Single%2Dthreaded%20Concurrency%20means%20making,progress%20on%20its%20own%20task https://jenkov. com/tutorials/java-concurrency/concurrency-vs-parallelism. html https://www. ibm. com/docs/en/aix/7. 2?topic=programming-understanding-threads-processes https://pages. mtu. edu/~shene/NSF-3/e-Book/FUNDAMENTALS/thread-management. html"
    }, {
    "id": 28,
    "url": "http://localhost:4000/apache-yarn/2023/introduction-to-apache-yarn",
    "title": "Introduction to Apache YARN",
    "body": "2023/01/01 - Introduction to Apache YARNThe term “YARN” stands for “Yet Another Resource Negotiator. ” As the name implies, Apache YARN is a resource manager designed to separate the functions of resource management and job scheduling/monitoring into separate daemons. In other words, YARN separates resource management and processing engine. This not only improves Hadoop, but also makes YARN a standalone component that can be used with other data processing engines like Apache Spark. Hadoop 2. 0 added Apache YARN to its ecosystem to provide a platform for processing data that doesn’t just use MapReduce but can also use other data processing engines. The YARN infrastructure is responsible for providing computational resources such as CPUs and memory that are required to run the various applications. The YARN infrastructure and HDFS are fundamentally separate entities. YARN provides resources for running an application, while HDFS provides storage.  Backward compatibility with Hadoop 1. x: MapReduce in Hadoop 2 maintains API compatibility with previous stable release (Hadoop 1. x). This means that all MapReduce jobs should still run unchanged on top of YARN with just a recompile. YARN architectureThere are three major components to the YARN architecture:  Resource Manager (RM) Node Manager (NM) ApplicationMaster (AM)Resource Manager (RM): The ResourceManager, or RM is the master daemon of YARN, which is usually one per cluster. In other words, it’s the master server. RM is responsible for managing the global assignments of resources (CPU and memory) among all the applications. The DataNode’s location and available resources are both known (referred as rack awareness) to the RM. ResourceManager has two main components:  Scheduler ApplicationsManagerScheduler: The Scheduler is responsible for allocating resources to the various running applications, using the standard limitations of capacity, queues, etc. Since it is exclusively dealing with task scheduling, it does not perform any tracking or monitoring of applications. Furthermore, it offers no guarantees about restarting failed tasks either due to application failure or hardware failures. The Scheduler performs its scheduling function based on the resource requirements of the applications. The Scheduler has a pluggable policy which is responsible for partitioning the cluster resources among the various queues, applications etc. The current schedulers such as the CapacityScheduler and the FairScheduler would be some examples of plug-ins. ApplicationsManager: The ApplicationsManager is responsible for accepting job submissions, negotiating the first container for executing the application-specific ApplicationMaster, and providing the service for restarting the ApplicationMaster container on failure. ApplicationMaster (AM): Every application has a specific ApplicationMaster associated with it, i. e. , one AM is assigned to each application. AM negotiates resources with the Resource Manager and works with the Node Manager. Specifically, the ApplicationMaster has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status, and monitoring progress. "
    }, {
    "id": 29,
    "url": "http://localhost:4000/web-server/2022/introduction-to-nginx",
    "title": "Introduction to NGINX",
    "body": "2022/12/27 - Introduction to NGINXNginx (pronounced “engine x”) is free and open source software that may serve as:  Web/HTTP server Reverse proxy Load balancer Mail proxy HTTP cacheThe initial goal was to develop a fast and reliable web server, in particular to address the C10K problem. The C10k problem is the inability of a server to handle a high number of concurrent connections (the 10K) before running out of available resources.  Web server vs. HTTP server: Web servers and HTTP servers are almost synonymous, with just a few minor distinctions. The term “web server” is more generic or broad. The term “HTTP server” often refers to a software implementation of the server part of the protocol (HTTP protocol). A web server typically uses more than one protocol, and HTTP is one of them. To communicate with clients such as web browsers, the HTTP server employs the HTTP protocol. Since web servers use the HTTP protocol to serve clients, one could say that the HTTP server is contained within the web server. In addition to its HTTP server capabilities, NGINX can also function as a proxy server for email (IMAP, POP3, and SMTP) and a reverse proxy and load balancer for HTTP, TCP, and UDP servers. NGINX as a web server: As a web server, NGINX’s job is to serve static or dynamic content to the clients. A primary motivation for developing NGINX was the need to create a web server that could handle massive amounts of traffic at lightning speeds, and this remains a primary focus. When comparing web server performance, NGINX consistently outperforms Apache and other popular servers. NGINX beyond web serving: Despite NGINX’s initial popularity as the fastest web server, its fundamental architecture has shown to be very suitable for many web tasks beyond serving content. NGINX is often used as a reverse proxy and load balancer to control incoming traffic and transfer it to slower upstream servers due to its high connection throughput. NGINX also is frequently placed between clients and a second web server, to serve as an SSL/TLS terminator or web accelerator. NGINX acts as an intermediate, handling activities like SSL/TLS negotiation and content compression and caching that may otherwise slow down our web server. Dynamic sites commonly deploy NGINX as a content cache and reverse proxy to reduce load on application servers. InstallationWe can confirm that NGINX is running by using the following command: ps -ef | grep nginx 501 92245   1  0 9:44AM ??     0:00. 00 nginx: master process nginx 501 92246 92245  0 9:44AM ??     0:00. 00 nginx: worker processIf NGINX is running, we will always see a master and one or more worker processes as shown above. Keep in mind that the master process is running as root, as NGINX requires elevated privileges in order to function properly. NGINX must be running as daemon in order for it to serve requests. We can check if NGINX is running as a daemon or in the foreground by using the ps command. NGINX files and directories/etc/nginx/nginx. conf: The /etc/nginx/nginx. conf file is the default configuration entry point used by the NGINX service. /etc/nginx/conf. d/: The /etc/nginx/conf. d/ directory contains the default HTTP server configuration file. NGINX terminologiesDirectives: Directives are key-value pairs found in the ngix. conf file. For example, worker_processes 1; is one of the directives in the config file. Technically, everything inside a NGINX configuration file is a directive. Directives are of two types:  Simple directives - Simple directives are the ones terminated by semicolons.  Block directives - However, unlike simple directives, which end with semicolons, block directives end with a pair of curly brackets enclosing additional instructions. Context: A block directive capable of containing other directives inside it is called a context. There are four core contexts in NGINX:  events { } - NGINX’s global configuration for handling requests is set in the events context. In a valid configuration file, there should be only one events context.  http { } - As the name suggests, the http context is used to provide the server’s configuration for processing HTTP and HTTPS requests. In a valid configuration file, there should be only one http context.  server { } - The server context is nested inside the http context and used for configuring specific virtual servers within a single host. There can be multiple server contexts in a valid configuration file nested inside the http context. Each server context is considered a virtual host.  main - The main context is the configuration file itself. Whatever we write that doesn’t fit into one of the three previously mentioned contexts is in the main context. Below is the context. Within the context, we can have directives for that specific context. server {  listen    8080;  server_name localhost;}Modules: As an open source project, NGINX benefits from contributions of new features and functionality from a large developer community. These new features developed by the community are available as modules that can be dynamically plugged into a running NGINX instance. NGINX keeps a repository of third-party modules that have been tested and certified to work well with NGINX. Upstream module: Upstream is a module used in NGINX to define the servers to be load balanced. Each server can listen on different port. Given that we have two servers, server1 with the IP 172. 42. 42. 10 and server2 with the IP 172. 42. 42. 20, the upstream name sample. server. com would look like this: upstream sample. server. com {  server 172. 42. 42. 10:8080 weight=2;  server 172. 42. 42. 20:8081;}In the above case, the name of the upstream is sample. server. com. A server’s ID address and other parameters can be specified in an additional directive called server, that is contained inside the upstream. Default upstream use weighted round-robin balancing method. According to the above example, every three requests will be distributed as follows: Two requests go to server1 (172. 42. 42. 10), and the other request goes to server2 (172. 42. 42. 20). Any time a server is unable to fulfill a request, the request is sent to the next available server. This process continues until all available servers have been exhausted. If no servers responded successfully, the client will receive the result of the communication with the last server. NGINX configurtionAs a web server, NGINX’s job is to serve static or dynamic content to the clients. The configuration files often control how that content will be served. NGINX’s default configuration file is nginx. conf, located in /usr/local/nginx/conf, /etc/nginx, or /usr/local/etc/nginx (if installed via Homebrew on Mac). Unless we are sure of ourselves, we shouldn’t edit the original nginx. conf file. If we make changes to the configuration file, NGINX will need to be instructed to reload the file again. We may do this in two different ways:  We can restart the NGINX service by executing the sudo systemctl restart nginx command.  We can dispatch a reload signal to NGINX by executing the sudo nginx -s reload command. The -s option is used for dispatching various signals (stop, quit, reload, reopen) to NGINX. The NGINX configuration file is made up of directives and contexts. These directives and contexts control NGINX modules. Any directives placed outside of any context are considered to be in the main context. Frequently asked questions (FAQ)How to validate the NGINX configuration?: Verifying the syntax of a file is the first step after creating or editing a configuration file. This helps prevent any unforeseen errors which can cause our website to gown down. There are two different commands that may be used to do this, and they both provide the same results: nginx -tOr use one of the following: service nginx configtestsystemctl config nginxResponse: nginx: the configuration file /usr/local/etc/nginx/nginx. conf syntax is oknginx: configuration file /usr/local/etc/nginx/nginx. conf test is successful"
    }, {
    "id": 30,
    "url": "http://localhost:4000/web-server/2022/stateful-vs-stateless-applications",
    "title": "Stateful vs. Stateless Applications",
    "body": "2022/12/27 - A breif ovewrive of network protocolA network protocol is a set of rules that dictates how different devices on the same network format, send, and receive data, no matter how different their underlying infrastructures are. Devices on both ends of a communication exchange must accept and adhere to protocol norms for data to be sent and received effectively. Network protocols can be broadly classified into two types:  Stateful StatelessThe term “state” refers to the condition or quality of being at a given point in time. Whether a protocol is stateful or stateless depends on how long a client stays in contact with it and how much of the information is stored. What is stateless?: A stateless architecture or application is a type of network protocol in which the state of previous transactions is not stored or referenced in later (or subsequent) transactions. Every transaction is treated as if it were the first time it had ever occurred. In a stateless protocol, it is not the receiver’s job to remember any session state from requests that came before. The sender sends all the necessary session state to the receiver in a way that lets each request be handled independently from the session data associated with the requests that came before it. Take the act of sending a text message as an example. In this case, the availability of the recipient is not verified before the SMS is sent. The receiving device does not acknowledge to the transmitting device that it has received the message. It’s possible but not guaranteed that the recipient will really get the message even if it was sent. There has to be no double-checking of status or retries. This is what it means to be stateless. HTTP (Hypertext Transfer Protocol) is an example of a stateless protocol because each request is processed independently of the requests that came before it. This means that the browser and the server will lose contact once a transaction has been completed. Advantages of stateless: TODO Disadvantages of stateless: TODO What is stateful?: TODO Advantages of stateful: TODO Disadvantages of stateful: TODO ConclusionsTODO "
    }, {
    "id": 31,
    "url": "http://localhost:4000/database/2022/inverted-index",
    "title": "Inverted Index",
    "body": "2022/12/26 - What is an inverted index?All modern information retrieval systems (IRS) rely heavily on inverted indexes, a kind of data structure that is essential to effective retrieval. It stores a mapping of words (or any search terms) to their locations in the database table or document. In other words, it maintains a key-value pair that identifies where in the database or document a given search phrase may be found. The purpose of an inverted index is to allow fast full-text searches, but at the expense of additional processing time whenever a new item is added to the database.       Term       Document/Table ID         large       1001, 1003       winter       1002       apple       1002, 1004       hot       1002, 1003, 1005   Table 1: Search Terms Mapped to Document/Table IDs. Now comes the obvious question: Which data structure is best? Can we use a fixed-size data structure such as an array for this? Well, fixed-size data structures, such as fixed-sized arrays, are impractical.  If we consider a dynamic index in which new documents are added or existing ones are modified, it will be challenging to modify the sizes of the fixed-data structures. For these reasons, we need a linked list or equivalent data structure with a dynamically changing size. In standard information retrieval terminology, this list is called a posting list (also referred to as a postings file, or inverted file). In memory, a posting list can be represented as a data structure such as a linked list or an array with variable length. One occurrence of a word-document pair (large =&gt; 1001, 1003) is referred to as a posting, and the sum of all the posting lists is then referred to as the postings. "
    }, {
    "id": 32,
    "url": "http://localhost:4000/apache-spark/2022/spark-interview-questions",
    "title": "Apache Spark Interview Questions",
    "body": "2022/12/21 - What are the different modes available in Apache Spark to handle orrupt records during parsing?: There are three different modes available:  PERMISSIVE (Default) - The literal meaning of “permissive” is “allowing” or “not preventing. ” In this case, all fields are set to null and corrupted records are placed in a string column called _corrupt_record.  DROPMALFORMED - Drops all rows containing corrupt records. In other words, it ignores or removes invalid data.  FAILFAST - Throws an exception when it encounters corrupted data.  Capturing bad data: To keep corrupt records, an user can set a string type field named columnNameOfCorruptRecord (option( columnNameOfCorruptRecord ,  _corrupt_record )) in an user-defined schema. If a schema does not have the field, it drops corrupt records during parsing. When inferring a schema, it implicitly adds a columnNameOfCorruptRecord field in an output schema. df = spark. read. option( mode ,  PERMISSIVE ). csv( /path/to/csv-file. csv , header=True, inferSchema=True)val df = spark. read. option( header , true). option( mode ,  PERMISSIVE ). csv( . /inputs/employees. csv )How to bad records and files using badRecordsPath option?: When we set badRecordsPath, all failed records or files that are detected during data loading will be logged to that location. Errors indicating deleted files, network connection exceptions, IO exceptions, and so on are also ignored and logged in the badRecordsPath. Unable to find input file: val df = spark. read . option( badRecordsPath ,  /tmp/badRecordsPath ) . format( parquet ). load( /input/parquetFile )df. show() In the above scenario, Spark generates an exception file in JSON format to record the error when df. show() unable to locate the input file.  Here, bad_files is the exception type.  An xyz file contains a JSON record, which has the path of the bad file and the exception/reason message. Input file contains bad record: // Creates a json file containing both parsable and corrupted recordsSeq(   { a : 1,  b : 2}   ,    {bad-record   ). toDF(). write. format( text ). save( /tmp/input/jsonFile )val df = spark. read . option( badRecordsPath ,  /tmp/badRecordsPath ) . schema( a int, b int ) . format( json ) . load( /tmp/input/jsonFile )df. show() In this example, the DataFrame contains only the first parsable record ({ a : 1,  b : 2}). The second bad record ({bad-record) is recorded in the exception file.  The exception file contains the bad record, the path of the file containing the record, and the exception/reason message. "
    }, {
    "id": 33,
    "url": "http://localhost:4000/knowledge-graph/2022/knowledge-graph-intro",
    "title": "Introduction to Knowledge Graph",
    "body": "2022/12/11 - OverviewA knowledge graph is a network of real-world entities, such as objects, events, situations, or ideas, and shows how they are related. Most of the time, this information is kept in a graph database and viewed as a graph structure. This is where the term “knowledge graph” comes from. There are three main components that make up a knowledge graph:  Nodes Edges LablesAny object, place, or person can be a node. An edge defines the relationship between the nodes. Having said that knowledge graphs focus on data that is connected. Why knowledge graph?A knowledge graph turns our data into knowledge that a machine can understand. In other words, converting our data into machine-understandable knowledge. Its only purpose is to find hidden insights; it doesn’t serve operational purposes. Knowledge graph vs. relational databasePurpose or goal: The goal of a knowledge graph is to uncover hidden insights; it doesn’t serve operational purposes. However, a relational database serves both operational and analytics purposes. Storage: In a knowledge graph, entities and relationships are stored as nodes and edges, respectively, while in a relational database, data is stored in tables as rows and columns. Join queries are used to establish the relationships between the tables. Schema: Knowledge graphs are schema-free and unstructured, while relational databases have strict schemas. The structure and format of the data are predefined. Performance: Knowledge graphs are lightning fast, even for big data sets. Relational databases are relatively slower than knowledge graphs. Maintenance: Knowledge graphs are much easier because they don’t have a schema. Relational databases are hard and often cumbersome because small changes can affect the whole structure. "
    }, {
    "id": 34,
    "url": "http://localhost:4000/apache-pinot/2022/getting-data-into-pinot",
    "title": "Getting data into Apache Pinot",
    "body": "2022/12/07 - Apache Pinot’s file systemLet’s first briefly discuss Pinot’s file system. The file system is an abstraction layer that allows users to store and read data in a data layer of their choice for real-time and offline segments. Pinot uses distributed file systems (DFS). Pinot supports the following distributed file systems:  Amazon S3 Google Cloud Storage Azure Data Lake Storage HDFSBy default, Pinot doesn’t have a storage layer, so if the system crashes, none of the data that was sent will be saved. To store the generated segments permanently in a local or distributed file system, we will have to change how the controller and server are configured to add deep store.  Deep store: The deep store is the permanent store for segment files. It is used to make backups and get them back (restore). A cluster’s new server nodes will get a copy of the segment files from the deep store. If the local segment files on a server get damaged in some way, a new copy will be pulled down from the deep store when the server is restarted. Note that the deep store stores a compressed version of the segment files, and it usually doesn’t have any indexes. These compressed files can be kept on a local or a distributed file system. Importing or ingesting data into Apache PinotThe following Pinot architectural diagram shows how batch and real-time ingestions use different data flows:                    Figure 1: Pinot Architecture.       Ingestion readiness: We can either ingest offline data or realtime data into Pinot. To get both offline and real-time data into Pinot, we need:  Pinot schema Pinot table configuration Ingestion job specPinot schema: Schema is used to define the names, data types, and other details for the columns in a Pinot table. The Pinot schema is composed of:  schemaName - Defines the name of the schema. Most of the time, this is the same as the table name.  dimensionFieldSpec - A dimensionFieldSpec is defined for each dimension column. Dimension columns are typically used in slice-and-dice operations such as GROUP BY and WHERE.  metricFieldSpec - A metricFieldSpec is defined for each metric column. Aggregation is done with the help of metric columns. In the dialect of a data warehouse, these can also be called fact or measure columns. Some operation for which metric columns are used:     Aggregation - SUM, MIN, MAX, COUNT, AVG, etc.    Filter clause such as WHERE    dateTimeFieldSpec - A dateTimeFieldSpec is defined for the time columns. Note that there can be multiple time columns in a table, but only one of them can be treated as primary. The primary time column is used by Pinot to maintain the time boundary between offline and real-time data in a hybrid table and for retention management. A primary time column is mandatory if the table’s push type is APPEND and optional if the push type is REFRESH. Common operations that can be done on time column: GROUP BY and WHERE. Pinot doesn’t have strict rules about which of these categories these columns belong to. Instead, you can think of the categories as indications that Pinot can use to do internal optimizations. When we do segment merges and rollups, the categories are also important. Pinot uses the time and dimension fields to figure out which records to merge or roll up. Data types: Data types determine the operations that can be performed on a column. Pinot supports the following data types:  INT LONG FLOAT DOUBLE BIG_DECIMAL BOOLEAN TIMESTAMP STRING JSON BYTES No explicit type exists for lists or arrays: Pinot also works with columns that have lists or arrays of items, but there isn’t a specific data type for these lists or arrays. We can instead say that a dimension column can take more than one value. Sample schema: Here’s an example of a Pinot schema for an employee table: {  schemaName :  employee ,  dimensionFieldSpecs : [  {    name :  id ,    dataType :  LONG   },  {    name :  name ,    dataType :  STRING ,  },  {    name :  age ,    dataType :  INT ,  } ],  metricFieldSpecs : [  {    name :  salary ,    dataType :  DOUBLE ,    defaultNullValue : 0  } ],  dateTimeFieldSpecs : [  {    name :  createdOn ,    dataType :  LONG ,    format :  EPOCH ,    granularity :  15:MINUTES   },  {    name :  modifiedOn ,    dataType :  LONG ,    format :  EPOCH ,    granularity :  15:MINUTES   } ]}Pinot table configuration: Now that we know what our schema is, we can use table configuration to set up the table. Table configuration is used to define the table properties, such as name, type, indexing, routing, retention, etc. It is written in JSON format and is stored in ZooKeeper, along with the table schema. This table definition has the following information at a very high level:  How Pinot should create segments for this table.  Required configurations for indexing.  Table type, which is set to OFFLINE in this case. Sample table configuration: A sample table configuration for an employee table is shown below. The tableType property shows us that it is an offline table. {  tableName :  employee ,  segmentsConfig  : {   timeColumnName :  createdOn ,   timeType :  MILLISECONDS ,   replication  :  2 ,   schemaName  :  employee  },  tableIndexConfig  : {   invertedIndexColumns  : [],   loadMode  :  MMAP  },  tenants  : {   broker :  DefaultTenant ,   server :  DefaultTenant  },  tableType :  OFFLINE ,  metadata : {}}Ingestion job spec: So far, we have set up the schema and table configuration. Just one more configuration needs to be made, which is the ingestion job specification. In order to ingest data into Pinot, Pinot requires us to create an ingestion job specification file. The job spec can be in either YAML or JSON format. This spec file tells Pinot about:  Where to find the raw data Where to create the segments Other ingestion-related configuration directivesThe following is the sample ingestion job spec file: executionFrameworkSpec: name: 'standalone' segmentGenerationJobRunnerClassName: 'org. apache. pinot. plugin. ingestion. batch. standalone. SegmentGenerationJobRunner' segmentTarPushJobRunnerClassName: 'org. apache. pinot. plugin. ingestion. batch. standalone. SegmentTarPushJobRunner' segmentUriPushJobRunnerClassName: 'org. apache. pinot. plugin. ingestion. batch. standalone. SegmentUriPushJobRunner'jobType: SegmentCreationAndTarPushinputDirURI: '$BASE_DIR/rawdata/'includeFileNamePattern: 'glob:**/*. csv'outputDirURI: '$BASE_DIR/segments/'overwriteOutput: truepinotFSSpecs: - scheme: file  className: org. apache. pinot. spi. filesystem. LocalPinotFSrecordReaderSpec: dataFormat: 'csv' className: 'org. apache. pinot. plugin. inputformat. csv. CSVRecordReader' configClassName: 'org. apache. pinot. plugin. inputformat. csv. CSVRecordReaderConfig'tableSpec: tableName: 'employee'pinotClusterSpecs: - controllerURI: 'http://localhost:9001'During ingestion, Pinot offers support for various popular input formats. By changing the format of the input, we can cut down on the time it takes to serialize and de-serialize and speed up the ingestion. The recordReaderSpec property in the ingestion job spec can be used to change the format of the input: recordReaderSpec: dataFormat: 'csv' className: 'org. apache. pinot. plugin. inputformat. csv. CSVRecordReader' configClassName: 'org. apache. pinot. plugin. inputformat. csv. CSVRecordReaderConfig' configs: 	key1 : 'value1'	key2 : 'value2'The config consists of the following keys:  dataFormat - Name of the data format to consume.  className - This class is used for parsing the data.  configClassName - This class is used to parse the values mentioned in configs.  configs - Key value pair for format specific configs. We can skip this config. For more details about the job spec properties, refer here. Data ingestion: As mentioned above, Pinot supports a number of popular input formats when it comes to ingesting. Here are the input formats that can be used:  CSV JSON Thrift AVRO Parquet ORC Protocol BuffersRefer the official documentation here to learn more about how to configure the properties of input formats. Ingesting offline data: Offline ingestion stores data in offline tables that are kept separate from realtime tables. Segments for offline tables are constructed outside of Pinot, usually in Hadoop via map-reduce jobs and ingested into Pinot via REST API provided by the Controller. Pinot has libraries that can create Pinot segments from input files in AVRO, JSON, or CSV formats in a Hadoop job and send them to the controllers via REST APIs. When an offline segment is ingested, the controller looks up the table’s configuration and assigns the segment to the servers that host the table. Depending on how many replicas are set up for that table, it may put more than one server in charge of each segment.  Note: Pinot supports different segment assignment strategies that are optimized for various use cases. Once segments are assigned, Pinot servers get notified via Helix to host or serve the segment. The segments are downloaded from the remote segment store to the local storage, where they are untarred/unzipped and mapped to memory. Helix lets brokers know that these segments are available once the server has loaded (memory-mapped) them. The brokers start to add the new segments for queries.  Offline data segments are immutable: Data in offline segments are immutable (rows cannot be added, deleted, or modified). However, segments may be replaced with data that has been changed. Pinot supports uploading offline segments to real-time tables. This is helpful when a user wants to start a real-time table with some initial data. Ingesting realtime data: When realtime data is ingested, it is stored in realtime tables. Pinot servers ingest rows from data streams like Kafka and use them to build segments for realtime tables. As soon as a row is ingested from a stream, it is made available for query processing. This is because events are instantly indexed in memory upon ingestion.  Note: The ingestion of events into the realtime table is not transactional, so replicas of the open segment are not immediately consistent or same. A pinot table can be configured to use one of two ways to get data from streams:  lowLevel - This is the preferred mode of consumption. Pinot creates separate consumers at the partition-level for each partition.  highLevel - Pinot creates one stream-level consumer that consumes from all partitions. Each message consumed could be from any of the partitions of the stream. In either mode, the rows that are ingested are stored in volatile memory until one of the following happens:  A certain number of rows are consumed.  Consumption has been going on for a certain amount of time. When one of the above limits is reached, the servers do the following:  Pause consumption.  Persist the rows consumed so far into non-volatile storage.  Keep consuming new rows into volatile memory. The persisted rows form what we call a completed segment. This is different from a consuming segment, which resides in volatile memory. "
    }, {
    "id": 35,
    "url": "http://localhost:4000/apache-pinot/2022/running-apache-pinot-locally",
    "title": "Running Apache Pinot Locally",
    "body": "2022/12/07 - Apache Pinot can be run in any of the following environments:  locally on our own computer in Docker in KubernetesHere, we’ll discuss about how to deploy and run Apache Pinot locally on our computer. Download Apache PinotLet’s start by getting the Apache Pinot distribution. We can either download a packaged release or build a distribution from the source code.  Prerequisites:Install JDK11 or higher (JDK16 is not yet supported). For JDK 8 support use Pinot 0. 7. 1 or compile from the source code. Here, we’ll download a packaged release. Download the latest binary release from Apache Pinot, or use the following command. At the time this blog was written, 0. 11. 0 was the most recent version of Pinot. PINOT_VERSION=0. 11. 0 #set to the Pinot version you want to usewget https://downloads. apache. org/pinot/apache-pinot-$PINOT_VERSION/apache-pinot-$PINOT_VERSION-bin. tar. gzOnce we have the tar file downloaded, untar it as shown below: # untar ittar -zxvf apache-pinot-$PINOT_VERSION-bin. tar. gz# navigate to directory containing Pinot binaries and launch scriptscd apache-pinot-$PINOT_VERSION-binLaunching Apache PinotWe can launch Apache Pinot either by using Quick Start or by launching all of its components individually, one at a time.  Pinot launch script: The pinot-admin. sh launch script can be found in the bin directory of Pinot. It can be used to start different Pinot components. If we run it without any arguments, it will show us all the commands we can use. Launching Apache Pinot using Quick Start: Pinot comes with QuickStart commands that let us run all of its components in a single process and import pre-built data sets. If you are new to Pinot, these QuickStarts are a good place to start. The following quick-start launches Apache Pinot with a set of sample data, schemas, and table configurations already loaded into it.   . /bin/pinot-admin. sh QuickStartThe above quick-start command does the following:  Starts Apache ZooKeeper Starts Pinot Controller Starts Pinot Broker Starts Pinot Server Creates the baseballStats table (comes with pre-loaded dataset) Launches a standalone data ingestion (batch processing) job that builds one segment for a given CSV data file for the baseballStats table and pushes the segment to the Pinot Controller Issues sample queries to Pinot macOS Users: AirPlay receiver allows nearby Apple devices to send video and audio content to our Mac with AirPlay. By default, the Airplay receiver server runs on port 7000, which is also the port used by the Pinot server in the Quick Start. So we may get an address-bind exception when running quick-start commands. If we turn off the AirPlay receiver server and try again, this error message should go away. Here, we will focus more on launching all the required components individually, one at a time. To know more about Quick Start commands, refer here. Launching all the components individually: If we want to deal with bigger sets of data (more than a few MB), we can start each component separately. Prerequisites: Before we start, make sure we’ve done everything on the following list:       #   Step   Link         1   Download sample data and configs   https://github. com/npawar/pinot-tutorial       2   Download latest Apache Pinot release binary   https://pinot. apache. org       3   Install Java 9 or higher   https://openjdk. java. net       4   Install Apache Maven* 3. 5. 0 or higher   https://maven. apache. org       5   Download ZooInspector   https://github. com/zzhang5/zooinspector   * Apache Maven is required to build and package ZooInspector tool.  ZooInspectorZooInspector is a UI we can use for inspecting our znode structure. Every node in a ZooKeeper tree is refered to as a znode. Each time a znode’s data changes, the version number increases. Znodes are the main enitity that a programmer access. Make sure to click the refresh button on the ZooInspector tool to see any changes. Build and run ZooInspector:git clone https://github. com/zzhang5/zooinspector. gitcd zooinspector/mvn clean packagechmod +x target/zooinspector-pkg/bin/zooinspector. shtarget/zooinspector-pkg/bin/zooinspector. sh We’ll put together a Pinot cluster with the following components:  1 ZooKeeper 2 Pinot Controllers 2 Pinot Brokers 2 Pinot ServersWe will do the following activities in the order they appear:  Starting ZooKeeper Starting Pinot Controller Starting Pinot Broker Starting Pinot ServerStarting ZooKeeper: We will start the ZooKeeper using the pinot-admin script (pinot-admin. sh), which can be found in the Apache Pinot installed directory. We uses the default ZooKeeper port, 2181. . /bin/pinot-admin. sh StartZookeeper -zkPort 2181We can use ZooInspector tool to browse the ZooKeeper instance.                    Figure 1: ZooInspector Tool: Shows ZooKeeper instance.       Starting Pinot Controller: Pinot Controller hosts Apache Helix, and together they are responsible for managing all the other components of the cluster. Controller’s default port is 9000. Controller 1 on port 9001: . /bin/pinot-admin. sh StartController \  -zkAddress localhost:2181 \  -clusterName PinotCluster \  -controllerPort 9001Controller 2 on port 9002: . /bin/pinot-admin. sh StartController \  -zkAddress localhost:2181 \  -clusterName PinotCluster \  -controllerPort 9002In the above commands, two Pinot controllers are started on ports 9001 and 9002. We can give any name to a cluster using the -clusterName option. The controller will communicate to ZooKeeper when it starts up to register itself. Also, it will open port 9001 so that we can use its user interface (localhost:9001). Let’s look at the ZooInspector tool to see what changes show up after starting the Pinot controller. We have a new cluster called PinotCluster which has cluster-level config properties.                    Figure 2: ZooInspector Tool: New Cluster, PinotCluster is showing up.       We have a participants directory that lists all of the cluster participants. So far, we only have the controllers (two controllers) that we just started.                    Figure 3: ZooInspector Tool: Shows two controllers under participant directory.       In the controller directory, we can see a leader node, which tells us which of the two controllers is the lead controller. The lead controller has additional responsibilities, such as running some periodic maintenance and cleanup tasks in the background.                    Figure 4: ZooInspector Tool: Shows two controllers with a leader controller node.       Let’s see what else our controller can do. Type localhost:9001 into the web browser’s address bar. This opens the dashboard for the Pinot cluster, which is shown below:                    Figure 5: Apache Pinot - Cluster Dashboard.       This dashboard has the following options:  Cluster Manager Query Console - lets us run queries on the tables in our cluster.  ZooKeeper Browser Swagger REST API - has admin endpoints to operate and manage the cluster. Here we can perform read/write/delete operations on other entities of a cluster. Below is the Swagger REST API page:                    Figure 6: Swagger REST API Page.       Starting Pinot Broker: Brokers handle Pinot queries. They accept queries from clients and forward them to the right servers (data servers). They gather results from the servers and combine them into a single response to send back to the client. Use the following command to start a Broker: . /bin/pinot-admin. sh StartBroker \  -zkAddress localhost:2181 \  -clusterName PinotCluster \  -brokerPort 7001Let’s also start another Broker using a different port, 7002: . /bin/pinot-admin. sh StartBroker \  -zkAddress localhost:2181 \  -clusterName PinotCluster \  -brokerPort 7002Let’s look at the ZooInspector tool again. Now, it shows two instances of the broker:                    Figure 7: ZooInspector with Brokers.       Starting Pinot Servers: Use the following commands to start two Pinot servers: . /bin/pinot-admin. sh StartServer \  -zkAddress localhost:2181 \  -clusterName PinotCluster \  -serverPort 8001 -serverAdminPort 8011. /bin/pinot-admin. sh StartServer \  -zkAddress localhost:2181 \  -clusterName PinotCluster \  -serverPort 8002 -serverAdminPort 8012ZooInspector shows all services now:                    Figure 8: ZooInspector with all services.       Kudos! The cluster has been set up now. Use Zooinspector to explore the cluster. Explore the Admin endpoints using Rest API on the controller http://localhost:9001.                    Figure 9: Pinot Dashboard with all services.       From the dashboard, we can see that there are two controllers, two brokers, two servers, and one tenant. The Pinot cluster is now up and running.  Tenant is a logical grouping of nodes (servers and brokers) with the same Helix tag. In our cluster, we have a default tenant called “default tenant. ” When nodes are created in the cluster, they automatically get added to the default tenant. "
    }, {
    "id": 36,
    "url": "http://localhost:4000/elasticsearch/2022/introduction-to-kibana",
    "title": "Introduction to Kibana",
    "body": "2022/11/14 - OverviewKibana is a user interface for data analysis and visualization of our Elasticsearch data. Frequently asked questions (FAQ)"
    }, {
    "id": 37,
    "url": "http://localhost:4000/elasticsearch/2022/introduction-to-elasticsearch",
    "title": "Introduction to Elasticsearch",
    "body": "2022/11/14 - OverviewElasticsearch is the core of the Elastic Stack. It’s a distributed search and analytics engine. Logstash and Beats make it easy to collect, combine, enrich, and store our data in Elasticsearch. LogstashLogstash is an open source data collection engine with real-time pipelining capabilities. Logstash can unify data from disparate sources, normalize it in real-time, and send it to destinations of our choice. With a wide range of input, filter, and output plugins, any type of event can be enhanced and transformed. Many native codecs also make the process of ingesting events easier. What are Beats?: Beats are lightweight open-source data shippers that we install them as agents on our servers to send specific types of operational data to Elasticsearch. Elastic provides Beats for capturing:  Audit data (Auditbeat) Log files and journals (Filebeat) Cloud data (Functionbeat) Availability (Heartbeat) Metrics (Metricbeat) Network traffic (Packetbeat) Windows event logs (Winlogbeat)Beats can send data directly (or via Logstash) to Elasticsearch. Once the data is in Elasticsearch, it can be further processed and enhanced visualizing it in Kibana. If we want to solve a certain use case, we can create a community Beat.                    Figure 1: Beats flow. Image Courtesy: elastic. co.       Frequently asked questions (FAQ)What is the ELK stack?: ELK is an acronym for three open source projects: Elasticsearch, Logstash, and Kibana.  Elasticsearch is a search and analytics engine.  Logstash is a server-side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to Elasticsearch.  Kibana lets users visualize data in Elasticsearch with charts and graphs. What is Elastic APM?: APM stands for application performance monitoring. APM is one of the most common methods developers use today to measure the availability, response times, and behavior of applications and services. Elastic APM, on the other hand, is an application performance monitoring system that is built on top of the ELK Stack (Elasticsearch, Logstash, Kibana, and Beats).  Like other APM solutions, Elastic APM allows us to track key performance related information such as requests, responses, database transactions, errors, etc. Elastic APM ships with support for Java, Go, Node. js, Python, Ruby, . NET, and JavaScript. The Elastic APM solution is made up of four building blocks:  Elasticsearch for data storage and indexing Kibana for analyzing and visualizing the data APM server APM agentAPM agents are in charge of gathering performance data and sending it to the APM server. The APM server is in charge of getting the data, turning it into documents, and sending it to Elasticsearch for storage.                    Figure 1: APM clients and server.       When to use Logstash or beats?: Beats are lightweight open-source data shippers that we install them as agents on our servers to send specific types of operational data to Elasticsearch. Logstash has a larger footprint, but provides a broad array of input, filter, and output plugins for collecting, enriching, and transforming data from a variety of sources. What is an inverted index, and why do we need it?: "
    }, {
    "id": 38,
    "url": "http://localhost:4000/elasticsearch/2022/elastic-apm",
    "title": "Introduction to Elastic APM",
    "body": "2022/11/14 - OverviewAPM stands for application performance monitoring. APM is one of the most common methods developers use today to measure the availability, response times, and behavior of applications and services. Elastic APM, on the other hand, is a free and open application performance monitoring system that is built on top of the ELK Stack (Elasticsearch, Logstash, Kibana, and Beats).  Like other APM solutions, Elastic APM allows us to track key performance related information such as requests, responses, database transactions, errors, etc. Elastic APM ships with support for Java, Go, Node. js, Python, Ruby, . NET, and JavaScript. The Elastic APM solution is made up of four building blocks:  Elasticsearch for data storage and indexing Kibana for analyzing and visualizing the data APM server APM agentAPM agents are in charge of gathering performance data and sending it to the APM server. The APM server is in charge of getting the data, transforms the data into Elasticsearch documents, and sending it to Elasticsearch for storage.                    Figure 1: APM clients and server.       Frequently asked questions (FAQ)"
    }, {
    "id": 39,
    "url": "http://localhost:4000/data-lake/2022/introduction-to-lakefs",
    "title": "Introduction to lakeFS",
    "body": "2022/10/07 - What is lakeFS?lakeFS is distributed as a single binary that contains several logical services. lakeFS offers a git-like version control interface for data lakes so that it gives us the ability to manage our data lake in a manner that is similar to how we manage our code. lakeFS adapts the best practices from the field of software engineering to the field of data engineering. Collaboration: There is a need for an improved system that can version huge amounts of data. For team members to work collaboratively and concurrently on a data set that is rapidly evolving, they need a version (snapshot) of that data that is reserved exclusively for their use. Lineage: Obtaining data lineage has never been an easy operation, and it is now made much more complicated by the fact that each data set has numerous versions over time. If we did not provide the lineage details, it would be pointless. Data storage: lakeFS stores data in an underlying object store (GCS, ABS, S3, or any S3-compatible stores like MinIO or Ceph), with some of its metadata stored in PostgreSQL. lakeFS provides the ability to branch, merge, rollback, etc. Frequently asked questions (FAQ)Difference between lakeFS and Delta Lake: "
    }, {
    "id": 40,
    "url": "http://localhost:4000/apache-spark/2022/spark-on-presto",
    "title": "Presto on Apache Spark",
    "body": "2022/10/07 - Overview Note: The Presto referred to here is the PrestoDB, not the PrestoSQL or Trino. Presto’s strengths and weaknessesPresto has both strengths and weaknesses. Strengths:  It’s ANSI SQL.  Widely adopted.  Interactive.  Federated query design.  Extensively used in scheduled (batch) workloads. Weaknesses:  Scale limitations.  High memory query reliability.  Long running query reliability. Frequently asked questions (FAQ)What is federated query engine?: A federated query is a way to send a query statement across data stored in various external data sources, such as relational, non-relational, object, or custom data sources. The federated query engine runs in a completely decoupled architecture, with computing on one side and storage on the other side. What makes Query Federation such a game-changing breakthrough is its ability to simplify the process of accessing data from a variety of sources via the use of a single query. This is due to the fact that in the past, combining data from a variety of sources was a time-consuming and tedious procedure. In order to combine several data sources into a single, standardized format, we will need to use ETL operations. Federated query engines are great for the infrequent analytics use cases where we can’t have the data in a single place and second-level performance isn’t important. Why optimized data warehouses are faster than federated query engine?TODO "
    }, {
    "id": 41,
    "url": "http://localhost:4000/coding-problem-solving/2022/remove-duplicates-int-array",
    "title": "Remove duplicates from an integer array",
    "body": "2022/09/21 - Problem: Remove duplicates from an integer arrayDescription: Given an array of integers nums, create a function that returns an array containing the values of nums without duplicates; the order doesn’t matter. Example 1:  Input: nums = [4, 2, 5, 3, 3, 1, 2, 4, 1, 5, 5, 5, 3, 1] Output: [4, 2, 5, 3, 1]Example 2:  Input: nums = [1, 1, 1, 1, 1, 1, 1, 1] Output: [1]Solution 1 (Brute force solution): In this brute force solution we create an empty array, output. For each element of nums, we check if we didn’t put in the output yet. If it’s the case, we push it, and we continue. # Brute force approachdef remove_duplicates(nums):  output = []  for element in nums:    if element not in output:      output. append(element)  return outputif __name__ ==  __main__ :  print(remove_duplicates([4, 2, 5, 3, 3, 1, 2, 4, 1, 5, 5, 5, 3, 1])) # returns [4, 2, 5, 3, 1]  print(remove_duplicates([1, 1, 1, 1, 1, 1, 1, 1])) # returns [1]Complexity:  Time complexity: O(n2) - The loop is traversing elements of nums, so it does n iterations, and at each iteration, we are checking if the element is not in output. Note that searching for an element in an unsorted array has an O(n) cost.  Space complexity: O(n) - Because we are storing the output in a separate additional array that will contain n elements in the worst case, when there are no duplicates in nums. Let’s find a better solution than this one. Solution 2 (Using sorting approach): It’s a sorting approach. def remove_duplicates_sorted(nums):  if len(nums) == 0:    return []    nums. sort()  output = [nums[0]]  for i in range(1, len(nums)):    if nums[i] != nums[i-1]:      output. append(nums[i])  return outputComplexity:  Time complexity: O(n long n) - Because we sorted the array Space complexity: O(n)Let’s find a better solution than this one. Solution 3 (Using hash table and without the need for sorting): This solution uses hash table. The hash table is a powerful tool when solving coding problems because it has an O(1) lookup on average, so we can get the value of a certain key in O(1). Also, it has an O(1) insertion on average, so we can insert an element in O(1). Also, this solution does not require the input data to be sorted. def remove_duplicates(nums):  visited = {} # Dictionary as hash table  for element in nums: # This iterates n times though!    visited[element] = True # Overwrites the already present elements  return list(visited. keys())Complexity:  Time complexity: O(n) - Because we are traversing completely during worst case.  Space complexity: O(n) - Because of the hash map. "
    }, {
    "id": 42,
    "url": "http://localhost:4000/coding-problem-solving/2022/first-repeating-character-in-a-string",
    "title": "Find first repeating character in a given string",
    "body": "2022/09/19 - Problem: Find first repeating character in a given stringDescription: Given a string str, create a function that returns the first repeating character. If such character doesn’t exist, return the null character '\0'. Example 1:  Input: str = “inside code” Output: ‘i’Example 2:  Input: str = “programming” Output: ‘r’Example 3:  Input: str = “abcd” Output: ‘\0’Solution 1 (Brute force solution): # Brute force approachdef first_repeating_character(str):  for i in range(len(str)):    for j in range(i):      if str[i] == str[j]:        return str[i]  return '\0'    if __name__ ==  __main__ :  print(first_repeating_character( inside code )) # returns 'i'  print(first_repeating_character( programming )) # returns 'r'  print(first_repeating_character( abcd )) # returns '\0'Complexity Time complexity: O(n2) - For each character we are traversing str again using both outer and inner loops.  Space complexity: O(1) - We are using two integer variables (i and j). The extra space here is 2, which is a constant. Let’s find a better solution than this one. Solution 2 (Using hash table): This solution uses hash table. The hash table is a powerful tool when solving coding problems because it has an O(1) lookup on average, so we can get the value of a certain key in O(1). Also, it has an O(1) insertion on average, so we can insert an element in O(1). def first_repeating_character(str):  visited = {} # Dictionary as hash table    for chr in str:    if visited. get(chr): # O(1) for lookup      return chr    else:      visited[chr] = True # O(1) for insertion  return '\0'The below one uses index: # Using indexdef first_repeating_character(str):  visited = {} # Dictionary as hash table    for i in range(len(str)):    if visited. get(str[i]): # O(1) for lookup      return str[i]    else:      visited[str[i]] = True # O(1) for insertion  return '\0'Complexity:  Time complexity: O(n) - We are fully traversing str once i. e. , it does n iterations. However, the hash table lookup and insertion have an O(1) cost on average.  Space complexity: O(n) - Because of the hash table we used. In the worst case, every character needs to be inserted into the hash table. This case happens when each character is unique and we don’t find a repeating character. Since we have n characters, the extra space would be n. "
    }, {
    "id": 43,
    "url": "http://localhost:4000/coding-problem-solving/2022/find-pair-that-sums-up-to-k",
    "title": "Find pair that sums up to `k`",
    "body": "2022/09/19 - Problem: Find pair that sums up to “k”Description: Given an array of integers nums and an integer k, create a boolean function that checks if there are two elements in nums such that we get k when we add them together. Example 1:  Input: nums = [4, 1, 5, -3, 6], k = 11 Output: true Explanation: 5 + 6 is equivalent to 11Example 2:  Input: nums = [4, 1, 5, -3, 6], k = -2 Output: true Explanation: 1 + (-3) is equivalent to -2Solution 1 (Brute force solution): This solution follows brute force approach. # Brute-force approachdef find_pair(nums, k):  for i in range(len(nums)):    for j in range(i+1, len(nums)):      if nums[i] + nums[j] == k:        return True  return Falseif __name__ ==  __main__ :  nums = [4, 1, 5, -3, 6]  print(find_pair(nums, 11)) # True  print(find_pair(nums, -4)) # False  print(find_pair(nums, -2)) # TrueComplexity:  Time complexity: O(n2) Space complexity: O(1)Let’s find a better solution than this one. Solution 2: This approach begins by sorting the numbers and then reduces the amount of traversal needed. Since the numbers are sorted in ascending order, then:  nums[i] &gt;= nums[i-1] nums[i] &lt;= nums[i+1]This approach uses the left index and the right index. If we increasing the left index, the sum value (k) will either increase or remain the same. In a similar manner, decreasing the right index will either bring about a reduction in the sum value (k) or cause it to stay unchanged. def find_pair(nums, k):  nums. sort()  left_idx = 0  right_idx = len(nums) - 1  while left_idx &lt; right_idx:    if nums[left_idx] + nums[right_idx] == k:      return True    elif nums[left_idx] + nums[right_idx] &lt; k:      left_idx += 1    else:      right_idx -= 1  return FalseComplexity:  Time complexity: O(n log n) Space complexity: Depends on the sorting algorithm we use. For example, if it’s O(log n), then the space complexity of this algorithm is O(long n). Let’s find a better solution than this one. Solution 3 (Using hash table. It’s the most optimal solution): This solution uses hash table. The hash table is a powerful tool when solving coding problems because it has an O(1) lookup on average, so we can get the value of a certain key in O(1). Also, it has an O(1) insertion on average, so we can insert an element in O(1). def find_pair(nums, k):  visited = {} # Dictionary as hash table  for element in nums:    if visited. get(k - element): # O(1) for lookup      return True    else:      visited[element] = True # O(1) for insertion  return FalseComplexity:  Time complexity: O(n) Space complexity: O(n) - We are using additional space for a hash table that can contain n elements in the worst case. The lookup and insertion are constant on average in this case. Hence, the O(n). "
    }, {
    "id": 44,
    "url": "http://localhost:4000/coding-problem-solving/2022/coding-problem-solving_old",
    "title": "Coding Problem Solving",
    "body": "2022/09/19 - OverviewHaving a talent for finding solutions to problems may accelerate our performance and put us ahead of our contemporaries or colleagues. In general, the solutions to certain problems may be accomplished in the blink of an eye, while others might take much more time, even days. As a result, it is essential to approach the programming problems in the appropriate manner in order to save time and effort. Let’s not attempt to solve the problem by using a brute-force approach. Getting started with problems: Find pair that sums up to “k”: Problem: Given an array of integers nums and an integer k, create a boolean function that checks if there are two elements in nums such that we get k when we add them together. Example 1 Input: nums = [4, 1, 5, -3, 6], k = 11 Output: true Explanation: 5 + 6 is equivalent to 11Example 2 Input: nums = [4, 1, 5, -3, 6], k = -2 Output: true Explanation: 1 + (-3) is equivalent to -2Solution 1 (Brute force solution): This solution follows brute force approach. # Brute-force approachdef find_pair(nums, k):  for i in range(len(nums)):    for j in range(i+1, len(nums)):      if nums[i] + nums[j] == k:        return True  return Falseif __name__ ==  __main__ :  nums = [4, 1, 5, -3, 6]  print(find_pair(nums, 11)) # True  print(find_pair(nums, -4)) # False  print(find_pair(nums, -2)) # TrueComplexity Time complexity: O(n2) Space complexity: O(1)Let’s find a better solution than this one. Solution 2: This approach begins by sorting the numbers and then reduces the amount of traversal needed. Since the numbers are sorted in ascending order, then:  nums[i] &gt;= nums[i-1] nums[i] &lt;= nums[i+1]This approach uses the left index and the right index. If we increasing the left index, the sum value (k) will either increase or remain the same. In a similar manner, decreasing the right index will either bring about a reduction in the sum value (k) or cause it to stay unchanged. def find_pair(nums, k):  nums. sort()  left_idx = 0  right_idx = len(nums) - 1  while left_idx &lt; right_idx:    if nums[left_idx] + nums[right_idx] == k:      return True    elif nums[left_idx] + nums[right_idx] &lt; k:      left_idx += 1    else:      right_idx -= 1  return FalseComplexity Time complexity: O(n log n) Space complexity: Depends on the sorting algorithm we use. For example, if it’s O(log n), then the space complexity of this algorithm is O(long n). Let’s find a better solution than this one. Solution 3 (Using hash table): This solution uses hash table. The hash table is a powerful tool when solving coding problems because it has an O(1) lookup on average, so we can get the value of a certain key in O(1). Also, it has an O(1) insertion on average, so we can insert an element in O(1). def find_pair(nums, k):  visited = {} # Dictionary as hash table  for element in nums:    if visited. get(k - element): # O(1) for lookup      return True    else:      visited[element] = True # O(1) for insertion  return FalseComplexity Time complexity: O(n) Space complexity: O(n) - We are using additional space for a hash table that can contain n elements in the worst case. The lookup and insertion are constant on average in this case. Hence, the O(n). Find first repeating character in a given string: Problem: Given a string str, create a function that returns the first repeating character. If such character doesn’t exist, return the null character '\0'. Example 1 Input: str = “inside code” Output: ‘i’Example 2 Input: str = “programming” Output: ‘r’Example 3 Input: str = “abcd” Output: ‘\0’Solution 1 (Brute force solution): It’s a brute force solution. # Brute force approachdef first_repeating_character(str):  for i in range(len(str)):    for j in range(i):      if str[i] == str[j]:        return str[i]  return '\0'    if __name__ ==  __main__ :  print(first_repeating_character( inside code )) # returns 'i'  print(first_repeating_character( programming )) # returns 'r'  print(first_repeating_character( abcd )) # returns '\0'Complexity Time complexity: O(n2) - For each character we are traversing str again using both outer and inner loops.  Space complexity: O(1) - We are using two integer variables (i and j). The extra space here is 2, which is a constant. Let’s find a better solution than this one. Solution 2 (Using hash table): This solution uses hash table. The hash table is a powerful tool when solving coding problems because it has an O(1) lookup on average, so we can get the value of a certain key in O(1). Also, it has an O(1) insertion on average, so we can insert an element in O(1). def first_repeating_character(str):  visited = {} # Dictionary as hash table    for chr in str:    if visited. get(chr): # O(1) for lookup      return chr    else:      visited[chr] = True # O(1) for insertion  return '\0'The below one uses index: # Using indexdef first_repeating_character(str):  visited = {} # Dictionary as hash table    for i in range(len(str)):    if visited. get(str[i]): # O(1) for lookup      return str[i]    else:      visited[str[i]] = True # O(1) for insertion  return '\0'Complexity Time complexity: O(n) - We are fully traversing str once i. e. , it does n iterations. However, the hash table lookup and insertion have an O(1) cost on average.  Space complexity: O(n) - Because of the hash table we used. In the worst case, every character needs to be inserted into the hash table. This case happens when each character is unique and we don’t find a repeating character. Since we have n characters, the extra space would be n. Remove duplicates from an interger array: Problem: Given an array of integers nums, create a function that returns an array containing the values of nums without duplicates; the order doesn’t matter. Example 1 Input: nums = [4, 2, 5, 3, 3, 1, 2, 4, 1, 5, 5, 5, 3, 1] Output: [4, 2, 5, 3, 1]Example 2 Input: nums = [1, 1, 1, 1, 1, 1, 1, 1] Output: [1]Solution 1 (Brute force solution): In this brute force solution we create an empty array, output. For each element of nums, we check if we didn’t put in the output yet. If it’s the case, we push it, and we continue. # Brute force approachdef remove_duplicates(nums):  output = []  for element in nums:    if element not in output:      output. append(element)  return outputif __name__ ==  __main__ :  print(remove_duplicates([4, 2, 5, 3, 3, 1, 2, 4, 1, 5, 5, 5, 3, 1])) # returns [4, 2, 5, 3, 1]  print(remove_duplicates([1, 1, 1, 1, 1, 1, 1, 1])) # returns [1]Complexity Time complexity: O(n2) - The loop is traversing elements of nums, so it does n iterations, and at each iteration, we are checking if the element is not in output. Note that searching for an element in an unsorted array has an O(n) cost.  Space complexity: O(n) - Because we are storing the output in a separate additional array that will contain n elements in the worst case, when there are no duplicates in nums. Let’s find a better solution than this one. Solution 2 (Using sorting approach): It’s a sorting approach. def remove_duplicates_sorted(nums):  if len(nums) == 0:    return []    nums. sort()  output = [nums[0]]  for i in range(1, len(nums)):    if nums[i] != nums[i-1]:      output. append(nums[i])  return outputComplexity Time complexity: O(n long n) - Because we sorted the array Space complexity: O(n)Let’s find a better solution than this one. Solution 3 (Using hash table and without the need for sorting): This solution uses hash table. The hash table is a powerful tool when solving coding problems because it has an O(1) lookup on average, so we can get the value of a certain key in O(1). Also, it has an O(1) insertion on average, so we can insert an element in O(1). Also, this solution does not require the input data to be sorted. def remove_duplicates(nums):  visited = {} # Dictionary as hash table  for element in nums: # This iterates n times though!    visited[element] = True # Overwrites the already present elements  return list(visited. keys())Complexity Time complexity: O(n) - Because we are traversing completely during worst case.  Space complexity: O(n) - Because of the hash map. Find the duplicate: Problem: Given an array of integers nums that contains n+1 elements between 1 and n inclusive, create a function that returns the duplicate element (the element that appears more than once). Assumptions: There is only one duplicate number.  The duplicate number can be repeated more than once.  Pigeonhole principle: The pigeonhole principle states that if n items are put into m containers, with n &gt; m, then at least one container must contain more than one item. In other words, at least 2 items share the same container. In this problem case, at least two elements have the same value i. e. , duplicate values. Example 1 Input: nums = [4, 2, 1, 3, 1] Output: 1Example 2 Input: nums = [1, 4, 2, 2, 5, 2] Output: 2Solution 1 (Brute force solution): It’s a brute force solution. # Brute force approachdef find_duplicate(nums):  for i in range(len(nums)):    for j in range(i+1, len(nums)):      if nums[i] == nums[j]:        return nums[i]if __name__ ==  __main__ :  print(find_duplicate([4, 2, 1, 3, 1])) # Returns 1  print(find_duplicate([1, 4, 2, 2, 5, 2])) # Returns 2Complexity Time complexity: O(n2) Space complexity: O(1)Let’s find a better solution than this one. Solution 2 (Sorting approach): This solution uses sorting approach. def find_duplicate(nums):  nums. sort()    for i in range(1, len(nums)):    if nums[i] == nums[i-1]:      return nums[i]Complexity Time complexity: O(n log n) Space complexity: Depends on the sorting algorithm we use. For example, if it’s O(log n), then the space complexity of this algorithm is O(log n). Let’s find a better solution than this one. Solution 3 (Using hash table): This solution uses hash table. def find_duplicate(nums):  visited = {} # Dictionary as hash table  for element in nums:    if visited. get(element):      return element    else:      visited[element] = True  return FalseComplexity Time complexity: O(n) - Because we traversing the array once i. e. , n times.  Space complexity: O(n) - Due to hash table. Let’s find a better solution than this one. Solution 4 (Using floyd’s cycle detection):  Floyd’s cycle detection or tortoise and hare algorithm:Floyd’s cycle detection method is a pointer algorithm that employs just two pointers, going through the sequence at various speeds. It is also known as the tortoise and the hare algorithm. The objective of this algorithm is to establish whether or not the linked list contains a cycle. First, we keep two pointers to the head node. At each iteration, we will move one of the points forward by two steps, while the other pointer will advance by one step. So we have two pointers, the tortoise and the hare. In practice, the tortoise is able to pull ahead by one distance unit, but the hare eventually gets within two distance units of it.  Note: The below code throws array out of bound exception if any of the element value goes beyond the length of the array. def find_duplicate(nums):  tortoise = nums[0]  hare = nums[nums[0]]  while tortoise != hare:    tortoise = nums[tortoise] # Is the index of the next node    hare = nums[nums[hare]] # Moves by 2 nodes. It's the index of other node that comes after the next one  tortoise = 0 # Goes back to where it starts  while tortoise != hare:    tortoise = nums[tortoise]    hare = nums[hare]  return tortoiseComplexity Time complexity: O(n) Space complexity: O(1)Tree depth first search: Problem: Given a binary tree of integers root, create 3 functions to print the tree nodes in preorder, inorder, and postorder traversal. Preorder: Print the root value, then print the left subtree, then print the right subtree. Inorder: Print the left subtree, then print the root value, then print the right subtree. Postorder: Print the left subtree, then print the right subtree, then print the root value. It is important to keep in mind that the tree is not a linear data structure like an array or a linked list. Array has a starting point (first element), and an ending point (last element). To traverse an array, we can just start at the first element and move straightforwardly until the last element. This, however, is not the case with trees, even if we do have a starting point, which is the root. Therefore, we need to figure out a means to go through all of the nodes in a tree. We have two main ways to do it:  Do it via BFS (Breadth First Search) - We traverse the tree level-by-level, starting from the root towards the bottom.  DO it via DFS (Depth First Search) -ConclusionIn my opinion, solutions that are straightforward and simple are preferable to those that use the newest buzzword technology. Let’s not get too excited about new products before discovering if they are flexible, useful, and have any adverse effects. "
    }, {
    "id": 45,
    "url": "http://localhost:4000/coding-problem-solving/2022/coding-problem-solving",
    "title": "Coding Problem Solving",
    "body": "2022/09/19 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! OverviewHaving a talent for finding solutions to problems may accelerate our performance and put us ahead of our contemporaries or colleagues. In general, the solutions to certain problems may be accomplished in the blink of an eye, while others might take much more time, even days. As a result, it is essential to approach the programming problems in the appropriate manner in order to save time and effort. Let’s not attempt to solve the problem by using a brute-force approach. Let’s try different ways to solve each problem, from brute force to finding the most efficient solution. Problems and solutionsYou can find all the code examples here. Problem 1: Find pair that sums up to “k”: Given an array of integers nums and an integer k, create a boolean function that checks if there are two elements in nums such that we get k when we add them together. Solutions Problem 2: Find first repeating character in a given string: Given a string str, create a function that returns the first repeating character. If such character doesn’t exist, return the null character '\0'. Solutions Problem 3: Remove duplicates from an integer array: Given an array of integers nums, create a function that returns an array containing the values of nums without duplicates; the order doesn’t matter. Solutions ConclusionIn my opinion, solutions that are straightforward and simple are preferable to those that use the newest buzzword technology. Let’s not get too excited about new products before discovering if they are flexible, useful, and have any adverse effects. "
    }, {
    "id": 46,
    "url": "http://localhost:4000/database/2022/how-indexing-works",
    "title": "How Does Indexing Work?",
    "body": "2022/09/19 - What is indexing in database?Indexing in a database is a special data structure that enables quick access to specific information without having to read the whole data of a given table in the database. Having quick access implies taking less time to find anything on a table. Having said that, it allows us to reduce the number of rows or records that need to be analyzed. Let’s say we have a table called Employee, and two of its fields are named Employee_Id and Employee_Name. On this table, we wish to run the select query shown in the following example: SELECT * FROM employee WHERE employee_name =  John ;When this query is processed, each and every row is examined to see whether or not the employee’s name contains the name “John. TODO: An index is an ordered representation of the indexed data.  Note: It is important to keep in mind that indexing makes reading faster but writing slower! With every insert, update, and delete we perform, we also have to insert, update, and delete into the index. Therefore, as a general guideline, we should always strive to restrict the number of indices to a minimum. Ordered vs. unordered rows: If the table were ordered alphabetically, the process of locating the item that we are looking for would either be accelerated or slowed down, depending on which row it is present within the table. In addition, if we are successful in finding the item we were looking for, we might skip the remaining rows in the table. In the event that the rows in the table are not organized in any particular fashion, the whole table will need to be scanned, a process that is more frequently referred to as a full table scan. You guessed correctly that doing a full table scan may take a considerable amount of time since it involves going over each row, and it is not very efficient. Ordering is so important. Searching on ordered data is significantly more efficient than searching on unordered data. How indexing works?Indexing is achieved by creating index-table or index. We will be able to create a proper index only when we have a clear understanding of our query as well as the criteria (table columns) that will be used to find the results. This index will create a data structure based on a certain column (search column), and it will not store any other column in the data structure. Our data structure for the Employee table above will only contain the the employee_name. The interesting question is what data is actually stored on the index. The values of the columns that we actually indexed are the only things that are stored in the index.  It’s important to note that the index is not a copy of our table. Types of indexFrequently asked questions (FAQ)"
    }, {
    "id": 47,
    "url": "http://localhost:4000/postgresql/2022/postgres-basics",
    "title": "PostgreSQL Basics",
    "body": "2022/09/18 - OverviewPostgreSQL is an open source object-relational database system. It runs on all major operating systems, and it’s ACID-compliant. PostgreSQL is highly extensible, which means that we can define our own data types, create custom functions, and even write code in different programming languages without having to recompile our database! Even if many of the features required by the SQL standard are supported, some of them may have slightly different syntax or function. As of the version 14 release in September 2021, PostgreSQL meets at least 170 of the 179 required features for SQL:2016 Core compliance. Note that no relational database meets full conformance with this standard. JSON features set it apart from some of the most widely used NoSQL databases, despite the fact that it is a relational database, like Oracle, MySQL, or SQL Server databases. To ensure durability, PostgreSQL writes all its transactions to a Write-Ahead Logging (WAL) segment on disk, to ensure that a committed transaction is safe if a crash occurs.  Write-Ahead Logging (WAL) is a method that is commonly used for transaction logging. The idea is that any modifications made to data files should only be written to disk after they have been logged to permanent storage. When we follow this approach, we eliminate the requirement to flush data pages to disk on every transaction commit. This is because we are certain that in the event of a crash, we will be able to restore the database by using the logs. Refer below for more information. Clusters in PostgreSQLBefore we can do anything, we need to setup a storage space on the disk for our database. This is what we refer to as a database cluster. A database cluster is a collection of databases that is managed by a single instance of a running database server. A database cluster, after it has been initialized, will have a database by the name of postgres. This database serves as the default database and is intended to be used by utilities, users, and third party applications. Note that the existence of the postgres database is not strictly necessary for the database server itself; however, many other external utility applications do presume that it is there. A database cluster, when speaking in terms of file systems, is just a single directory under which all of the data will be kept. We call this the data directory or data area. It’s completely up to us where (location) we choose to store our data. Before the data directory can be used, it has to be initialized with the help of the program initdb, which is included with PostgreSQL by default. To initialize a database cluster manually, run initdb and specify the desired file system location of the database cluster with the -D option, for example: $ initdb -D /usr/local/pgsql/dataFrequently asked questions (FAQ)What is Write-Ahead Logging (WAL)?: Write-Ahead Logging (WAL) is a method that is commonly used for transaction logging. The idea is that any modifications made to data files should only be written to disk after they have been logged to permanent storage. When we follow this approach, we eliminate the requirement to flush data pages to disk on every transaction commit. This is because we are certain that in the event of a crash, we will be able to restore the database by using the logs. With the help of WAL, we can be able to do both roll-forward and roll-backward recoveries:  Roll-forward recovery, also known as REDO, is the process by which any changes that have not been applied to the data pages will first be redone from the log records.  Roll-backward recovery, also known as UNDO, is the process by which any changes made by uncommitted transactions will be deleted from the data pages. In addition, WAL is used for replication. Benefits of WAL:  WAL significantly reduces the number of disk writes since only the log file needs to be flushed to disk at the time of transaction commit.  Data consistency. With WAL, database is able to guarantee consistency in the case of a crash. How to identify the location of the data directory in the simplest way possible?: Use ps command to identify PostgreSQL cluster: $ ps -eaf | grep /postOutput: 501 8874   1  0 3:40PM ??     0:00. 20 /Applications/Postgres. app/Contents/Versions/14/bin/postgres -D /Users/john/Library/Application Support/Postgres/var-14 -p 5432The output reveals that the data directory may be found at the location indicated by the -D notation. "
    }, {
    "id": 48,
    "url": "http://localhost:4000/scala/2022/access-modifiers",
    "title": "Access Modifiers in Scala",
    "body": "2022/09/06 - What is access modifiers in OOP?Before we proceed, let’s know what access modifiers are in object-oriented programming. Access modifiers, also known as access specifiers, determine the accessibility and scope of classes, methods, and other members. The encapsulation principle of object-oriented programming is managed through the use of access modifiers. In general, access modifiers are stated using the following keywords:  Public Private Protected PackageNote that if we try to refer to an inaccessible member, we’ll usually get a compile-time error! Access modifiers in ScalaScala’s access modifiers closely resemble those of Java, although they provide more granular and powerful visibility control than Java. Public: There is no explicit modifier for public members in Scala. Any member that is not labeled as private or protected is public, making all members public by default. Public members are accessible from any anywhere. Private: Private Scala members are treated similarly to private Java members. A private member is only accessible to the current class or instance and also other instances of the same class in which it is specified. This implies that private members of a parent class are not available to sub-classes (aka derived classes). class MyClass { private var myFlag: Boolean = false}class MySubClass extends MyClass { def myMethod(): Unit = {  // Note that private members are not available to sub-classes.   // Hence, the below statement won't compile as it is trying to  // access the private member, myFlag of the base class.   println(myFlag) // won't compile! }}Object-private: Scala’s object-private goes beyond private scope to make fields and methods object-private, extending the level of privacy. It’s the most restrictive access. Mark a method as object-private by placing the access modifier private[this] before the method declaration: private[this] def isMaxVal = trueThis makes the method accessible only from within the same object that contains the definition; other instances of the same class cannot access the method. Note that the other instances of the same class cannot access them: class Cat { var breed: String =  Persian  private[this] var sex: String =  Male  def showDetails(other: Cat): Unit = {  println(other. breed)  // Below statement won't compile as we are accessing  // via other instance of the same class.   //println(other. sex) }}Protected: Accessing protected members in Scala is a bit more restrictive than in Java. In Java, protected members can be accessed by other classes in the same package, but this is not true in Scala. In Scala, protected members can be accessible from:  Within the class.  Within its subclasses.  Within the companion objects. class MyBaseClass { protected var myFlag: Boolean = false def showFlag(): Unit = {  println( myFlag:   + myFlag) // Accessible from within the class }}class MySubClass extends MyBaseClass { def showDetails(): Unit = {  // Unlike private, we can access protected member from subclass.   println(myFlag) }}The following code won’t compile even though both the classes are in the same package: package planet {  class Bird {    protected def fly {}  }  class Forest {    val bird = new Bird    bird. fly  // error: this line won't compile  }}The above code won’t compile because the Forest class can’t access the fly method of the Bird class, even though they’re in the same package. Package or private[package]: To make a method available to all members of the current package, mark the method as being private to the current package by specifying it using the private[packageName] syntax: package org. earlycode. scalatutorial. basics { class Fruit {  private[basics] def doChop {}  private def doEat {} } class PackageScope {  val fruitObj = new Fruit  // Access by other classes in the same package   // i. e. basics package.   fruitObj. doChop  // Below statement won't compile as doEat method is available  // only to the Fruit class  //fruitObj. doEat }}Quick reference:                    Figure 1: Quick Reference of Access Specifiers.       Default access modifiers: Scala’s default access modifier is public. As metioned earlier, there is no explicit modifier for public members; any member not labelled private or protected is public. Public members can be accessed from anywhere. Access modifiers - differences and similarities with JavaDifferences:  Public — Unlike Java, Scala has no explicit modifier for public members.  Protected members in Java have wider access than in Scala; Java’s protected members can be accessible not only from within the class or within its subclasses, but also from other classes in the same package.  Default access modifier — Java defaults to package internal visibility, while Scala, on the other hand, defaults to public, i. e. can be accessed from anywhere.  Java adopts an all-or-nothing access strategy, i. e. , either it’s visible to all classes in the current package or it’s not visible to any, whereas Scala gives fine-grained control over visibility.  Object-private scope — Scala private[this] takes privacy a step further than private scope and makes the fields and methods object-private which means they can only be accessed from the object that contains them. Similarities:  Private members in Scala are treated similarly to Java. "
    }, {
    "id": 49,
    "url": "http://localhost:4000/scala/2022/collections",
    "title": "Scala Collections",
    "body": "2022/08/31 - Introduction to collectionsA collection is an object in programming that groups multiple elements into a single unit. It’s comparable to a container that can hold various items. A collection has an underlying data structure that is used to store, manipulate, and retrieve aggregate data in an efficient manner. When collections are used, readability and maintenance of the code are enhanced. Collections in ScalaScala has a very rich collections library, located under the scala. collection package. There are two types of collections in Scala:  Mutable collections Immutable collectionsAll collection classes are found in the following packages:  scala. collection scala. collection. immutable scala. collection. mutable Scala collections vs Java collections: Scala collections are notably distinct from Java collections. For example, the Scala List class differs significantly from the Java List class, including the immutable nature of the Scala List. Mutable collections: A mutable collection can be modified or extended in place. This means that we can modify, add, or remove elements from a collection with side effects. All mutable collection classes are present in the scala. collection. mutable package. Immutable collections: In contrast, immutable collections never change. There are still operations that simulate additions, deletions, and updates, but each of these operations returns a new collection and leaves the original collection unchanged. All immutable collections are present under scala. collection. immutable. The collections in scala. collection are supertypes of scala. collection. mutable and scala. collection. immutable. The base operations are added to the types in the scala. collection package, while the immutable and mutable operations are added to the types in the other two packages. A high-level view of the Scala collections: TODO Collections table: Frequently asked questions (FAQ)What is “InPlace” transformation operation?: Instead of always returning a new collection after a map or filter operation, mutable collections have a couple of new operations (filterInPlace and mapInPlace) that let us change the elements right where they are (in-place). These new operations change the source collection. val buf1 = ArrayBuffer(0, 1, 2, 3, 5, 6, 8, 9)buf1. filterInPlace(_ % 2 == 0) buf1 // ArrayBuffer(0, 2, 6, 8)val buf2 = ArrayBuffer(0, 1, 2, 3, 5, 6, 8, 9)buf2. mapInPlace(_ * 2)buf2 // ArrayBuffer(0, 2, 4, 6, 10, 12, 16, 18)What are transformer methods?: Transformer methods are collection methods that transform an input collection into a new output collection based on an algorithm we provide. Note that these transformations create copies of the collection while leaving the original untouched. This implies that if the original array is still in use, its contents will not be affected by the transform. The transformer methods are as follows:  map filter reverseWhat is collection buffer?: Buffers are used to create sequences of elements incrementally by appending, prepending, or inserting new elements. A buffer can grow and shrink. Having said that, buffers are mutable. Scala has buffers such as:  ArrayBuffer ListBufferWhat is collection builder?: A builder is a simplified form of a Buffer that is limited to generating its assigned collection type and only performing append operations. This is most useful for constructing immutable collections where we cannot add or remove elements once the collection has been constructed. To construct a builder for a certain collection type, use the type’s newBuilder method and pass in the element type as shown below. Invoke the builder’s result method to convert it back into the final type (in this case Array). $ val af = Array. newBuilder[Int]af: collection. mutable. ArrayBuilder[Int] = ArrayBuilder. ofInt$ af += 4$ af += 7$ af. resultres1: Array[Int] = Array(4, 7)Builder vs buffer (TODO): Difference 1: Unlike with buffers, a builder knows its immutable counterpart: Lets first construct an array collection using builder: $ val af = Array. newBuilder[Int]af: collection. mutable. ArrayBuilder[Int] = ArrayBuilder. ofIntNow, let’s the know the type of the collection using getClass() method. The type shows below is a mutable one. $ af. getClassres1: Class[T] = class scala. collection. mutable. ArrayBuilder$ofIntLet’s append a value and see the type again: $ af += 6res2: collection. mutable. ArrayBuilder[Int] = ArrayBuilder. ofInt$ af. getClassres3: Class[T] = class scala. collection. mutable. ArrayBuilder$ofInt Now, let’s freeze the collection using result method and check the type again. Type is implicitly converted to its immutable counterpart. $ af. result$ af. getClassres68: Class[T] = class scala. collection. mutable. ArrayBuilder$ofIntWhat is persistent data structure?: A persistent data structure, also known as a non-ephemeral data structure, is a data structure that always preserves its prior state when it is updated. These data structures are essentially immutable, since their operations do not alter the structure in-place, but rather always result in a new structure that has been modified. Having said that, all immutable data structures or collections are all persistent. Consider the following examples. I have used ammonite REPL to demostrate persistent data structure. Therefore, it displays the ‘@’ prefix. We created a list of numbers and used a filter method to choose just the even elements. As seen in the example below, the updated list produces a new list named res142. It does not update the original list - we checked the same by printing the original list. @ val numSeq: List[Int] = List(10, 7, 21, 8, 3, 17)numSeq: List[Int] = List(10, 7, 21, 8, 3, 17)@ numSeq. filter(_ % 2 == 0)res142: List[Int] = List(10, 8)@ print(numSeq)List(10, 7, 21, 8, 3, 17)What are the benefits of using Vector in Scala over List or other data structures?: Vector is an immutable data structure introduced in version 2. 8 of the Scala to address the inefficiencies of random access in other existing data structures. Vector is a persistent data structure using structural sharing. Random access is much better with vector, especially when the collection is big. Implemention: Vector extends AbstractSeq and IndexedSeq, thus it provides constant-time (O(1)):  Element access Lenght computationVector is currently the default implementation of immutable IndexedSeq. Vector is implemented as a base-32 integer trie. There are 0 to 32 (25) nodes at the root level, and there can be another 32 nodes connecting to each node at the root level. Thus, the second level can accommodate a maximum of 32*32 (210) elements. Similarly, the third level will have 32*32*32 (215) elements, so and so forth. Thus, a Vector of five levels can hold up to 230 elements. Accessing items from such a large collection requires traversing just five layers of nodes, making the process effectively constant time. Performance: For head access and iteration algorithms, a linear sequence collection will provide a little advantage over an indexed sequence. For all other purposes, an indexed sequence will perform much quicker, particularly in large collections. Prepend and append: As stated above, Vector is a persistent data structure using structural sharing. This method makes the append/prepend process almost as quick as any mutable data structure. When a new element is added by appending or prepending, the structure is modified via structure sharing and a new Vector is returned. Random access and random updates: When it comes to random access and random updates, Vector excels in comparison to all other immutable collections. Due to its implementation of a trie data structure, accessing an element at any place in the tree involves traversing just a few levels and branches. Head and tail access: Head and tail element access is a further common procedure. As expected, accessing the beginning and end elements of an indexed tree data structure is a quick process. Note that tail command returns an iterable that contains all the elements without the first element. Iteration: Some algorithms need iterating over all collection components. Although traversing all of a Vector’s elements is somewhat more complex than traversing a linked list. What is trie data structure?: Trie (to be pronounced as “try”) is an ordered tree data structure used for storing and searching a specific key from a set. A trie is also called a digital tree or prefix tree, which is a type of k-ary search tree. These keys are most often strings.  k-ary tree: An m-ary tree (also known as n-ary, k-ary, or k-way tree) is a rooted tree in graph theory in which each node has no more than m children. For example, a binary tree has a max of 2 children (m = 2), and a ternary tree is another case with m = 3, which limits its children to three. What is identity function?: An identity function is a basic function that takes one argument and the only thing it does is return the same argument value, unchanged i. e. , f(x) = x. In other words, it’s a function in which the output is the same as the input. The identity function is also known as an identity map or identity relation. Just like zero is a neutral value for additions, an identity function is a nuetral value for higher-order-functions. "
    }, {
    "id": 50,
    "url": "http://localhost:4000/kubernetes/2022/kubernetes-basics",
    "title": "Kubernetes Basics",
    "body": "2022/08/17 - What is Kubernetes?Kubernetes is a Google-developed open source container orchestration platform for automating the deployment, scaling, and management of containerized applications across a distributed cluster of nodes. Containerized applications are those that run inside containers.  Kubernetes supports various container runtimes: Kubernetes supports multiple container runtimes, one of them being Docker, which is one of the most well-known container tools on the market. The name Kubernetes originates from Greek, meaning helmsman or pilot. Kubernetes is often shortened to K8s, which comes from the fact that there are eight letters between the “K” and the “s. ” Kubernetes has become the de facto standard for container orchestration. Containers are a good way to bundle and run our applications.  In a production environment, it’s important to keep an eye on and manage the containers that run the applications to make sure there’s no downtime.  In the event that one container goes down, another needs to be started. If a system were responsible for managing this behavior, wouldn’t it make things simpler? That’s how Kubernetes comes to the rescue! Kubernetes provides us a framework that allows us to run distributed systems in a resilient manner. It takes care of scaling and failover for our application. When developers want to deploy an application on multiple computers, they use Kubernetes rather than deploying the application on each computer individually. Kubernetes provides an abstraction layer on top of the underlying infrastructure (computers, networks, etc. ) for both users and applications. Kubernetes hides the infrastructure underneath from the applications, making development and configuration simpler. Kubernetes’ primary goal is to hide the complexity of managing a collection of containers.                    Figure 1: Kubernetes provides an abstraction on top of underlying infrastructure.       Having said that, Kubernetes enabled development teams to spend less time on technology and more time on capability. Kubernetes provides us with:  Service discovery: Kubernetes has the ability to expose a container by using either the DNS name or their own IP address.  Load balancing: Kubernetes is able to load balance and distribute the network traffic to ensure that the deployment remains stable even when there is a huge volume of traffic to a container.  Storage orchestration: Kubernetes allows us to automatically mount a storage system of our choice.  Automated rollouts and rollbacks: Using Kubernetes, we are able to specify the desired state of our deployed containers, and it is able to transform the actual state to the desired state at a controlled rate. For instance, we might automate Kubernetes to create new containers for our deployment, delete any existing containers, and transfer all of those containers’ resources to the newly generated container.  Automatic bin packing: We provide a cluster of nodes available to Kubernetes so that it may utilize those nodes to run containerized tasks. We inform Kubernetes of the required amount of memory and processor for each container. Kubernetes enables us to make the most efficient use of our resources by fitting containers onto our nodes.  Self-healing: Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t expose them to clients until they are ready to serve.  Secret and configuration management: We are able to store and handle sensitive information with the help of Kubernetes. This includes passwords, OAuth tokens, and SSH keys. We can deploy and update secrets and application configuration without rebuilding our container images, and without exposing secrets in our stack configuration. Kubernetes componentsKubernetes cluster is made up of:  A control plane (aka master).  A set of worker machines known as worker nodes or simply workers.  A distributed storage system that is responsible for maintaining the cluster’s consistent state. When we deploy Kubernetes, it creates a cluster. A Kubernetes cluster is made up of a set of machines known as nodes. The nodes can be virtual machines (VMs) or physical machines. The nodes include a master node and a set of worker nodes. The worker nodes are responsible for running containerized applications. At least one worker is present in every cluster. Our applications that are contained inside the containers. Each worker node holds one or more containers. The worker node(s) hosts the Pods. A pod represents one or more running containers. The control plane is responsible for managing the cluster’s worker nodes as well as the Pods. The control plane generally runs across multiple nodes in production environments, which provides fault tolerance and high availability. What is a control plane and why do we need it?: The control plane is the central nervous system of a Kubernetes cluster. It ensures that every component in the cluster is kept in the desired state.  Desired state vs. actual state: Desired state is one of the core concepts of Kubernetes. It’s the state that we want the system to be in. The desired state is defined in a Kubernetes resource. The actual state, on the other hand, is the state that the system is actually in. Control plane receives data about internal cluster events, external systems, and third-party applications. It analyses the data, and based on that, it takes decisions and puts them into action. The control plane manages and maintains the worker nodes that hold the containerized applications. Components of the control plane: The control plane is made up of several components. All these components run on a node called the primary node or master node.  An API server as kube-apiserver A scheduler as kube-scheduler A controller manager as kube-controller-manager A persistent data store as etcd A cloud controller manager as cloud-controller-managerkube-apiserver: The core of Kubernetes’ control plane is the API server called Kubernetes API. Kubernetes API acts as the interface via which the control plane communicates with the worker nodes and other external systems. An API can be anything that’s used for programmatic communication. The Kubernetes API in particular is RESTful. The command line interface, web user interface, users, and services communicate with the cluster through Kubernetes API. The Kubernetes API lets us query and manipulate the state of API objects in Kubernetes such as Pods, Namespaces, ConfigMaps, and Events. The main implementation of the Kubernetes API server is the kube-apiserver. Since it is meant to extend horizontally, we are able to deploy many instances of the kube-apiserver in order to evenly distribute the load. etcd (persistent data store): Persistent data storage is provided by etcd, which is a distributed, reliable key-value store. It is a standalone open source tool, and Kubernetes communicates to it using the kube-apiserver. It stores the configuration information that is needed by the worker nodes as well as other data that is required to manage the cluster. kube-scheduler: kube-scheduler is responsible for allocating new pods to the worker nodes. When the pods are assigned to a new worker node, the kubelet (node agent) running on the node retrieves the pod definition from the Kubernetes API. Then, the kubelet creates the required resources and containers in accordance with the pod specification. In other words, the scheduler is a component that runs inside the control plane and is responsible for distributing the resources and workload across the worker nodes. kube-controller-manager: The Kubernetes controller manager is a daemon that acts as a continuous control loop in a Kubernetes cluster. A control loop is a kind of loop that does not terminate, and it monitors the current state of the cluster via calls made to the API Server, and changes the current state to match the desired state described in the cluster’s declarative configuration. The controller manager is made up of four different control loops that are referred to as controller processes. In order to achieve this goal, a controller loop is required to comprise two components:  Snapshot of the current state of the system.  Access to the desired state of the system. These controller processes keep an eye on the status of the different services deployed through the API and take corrective action if the current state doesn’t match the desired state. The controllers that ship with Kubernetes are:  Replication controller Endpoints controller Namespace controller Serviceaccounts controllerEven though kube-controller-manager is composed of four separate processes, it runs as a single process in order to keep things as simple as possible. cloud-controller-manager: The cloud-controller-manager is a separate component that connects the cluster to the API of the underlying cloud infrastructure. It runs only the controllers specific to the cloud provider, such as AWS, GCP, and so on. This way, the components interacting with our cloud provider are kept separate from the components that only interact with our cluster. The cloud-controller-manager consists of three controller processes, which are combined into a single process to reduce complexity:  Node controller Router controller Service controllerKubernetes concepts: Cluster: A cluster is a collection of hosts or nodes that work together to offer computing, memory, storage, and networking resources. Kubernetes uses these resources to run the various workloads (applications). Nodes: A single computer or host is referred to as a node.  It may be a physical or virtual machine. Its primary function is to run pods. Each node in a Kubernetes cluster is responsible for running a number of Kubernetes components, including the kubelet, the container runtime, and the kube-proxy. Nodes are managed by a Kubernetes master. The nodes are Kubernetes’s “worker bees,” and they are responsible for doing all of the heavy lifting. Workloads: An application that is being executed on Kubernetes is referred to as a workload. On Kubernetes, we will execute our workload inside of a set of pods, regardless of whether our workload consists of a single component or multiple that operate together. In Kubernetes, a Pod represents a set of running containers on our cluster. Note that we don’t need to manage each Pod directly. Pods are are managed by controllers in the form of a control loop. A controller is responsible for monitoring the current state of a Kubernetes resource and and makes the requests necessary to change its state to the desired state. Kubernetes comes with a number of built-in workload resources, including the following:  DeploymentPod: In Kubernetes, the unit of work is referred to as a pod. Each pod contains one or more containers. Containers that are contained inside pods are always scheduled together and always run on the same machine. All of the containers that make up a pod share the same IP address and port space. They connect with one another through localhost or standard inter-process communication. In addition, all of the containers that are contained inside a pod have the ability to access shared local storage that is located on the node that is running the pod. By default, containers do not have access to either their local storage or any other storage. Volumes of storage must be mounted into each container inside the pod explicitly. Pods are a great way to manage groups of containers that are closely related, depend on each other, and need to work together on the same host to get their job done.  It is essential to keep in mind that pods are thought to be ephemeral (short-live), disposable entities that may be discarded and replaced whenever it is convenient. Any pod storage is destroyed with its pod. Every pod is given a unique identifier, also known as a UID, so that we can distinguish between them if necessary. Labels: In Kubernetes, a label is a kind of metadata that can be attached to objects like pods and services in the form of a key-value pair. The purpose of a label is to help users identify the characteristics of objects in a way that is meaningful and relevant to them. In other words, labels can help describe meaningful and relevant information about an object. However, labels do not directly change or effect any functionality in the core system. Having said that, users will often need Kubernetes labels in order to identify Kubernetes objects and carry out helpful activities on those items. For example, consider a set of pods running on the Kubernetes cluster. Let’s say it’s necessary for us to delete all of the pods that are associated with the development environment. There is no simple method to determine which pods suit that description if labels are not assigned to them beforehand. As a result of this, locating each pod and deleting it can become challenging and time consuming.                    Figure 2: Labels in Kubernetes - Two separate environments, labeled as production and development.       Note that the purpose of label is to identifying objects and not for attaching arbitrary metadata to objects. Labels are intentionally designed with particular limitations in mind:  Each label that is attached to an object has to have its own unique key.  The label key has two parts: prefix8 and *name.  It is not required to use the prefix.  In the event that prefix does exist, it must be a valid DNS subdomain and is denoted by the forward slash (/), which is separated from the name.  The prefix must be 253 characters long at most.  Names must start and end with an alphanumeric character (a-z, A-Z, 0-9) and contain only alphanumeric characters, dots, dashes, and underscores.  Values follow the same restrictions as names. One of the most common ways to add labels to our resources is to add them directly to our config files. We can specify label values at metadata. lables like below: apiVersion: v1kind: Podmetadata: name: metadata-demo labels:  environment: demo  app: nginxspec: containers:  - name: nginx   image: nginx:1. 14. 2   ports:    - containerPort: 80The kubectl command-line interface tool is the alternative method for working with labels. This comes in helpful when making minor adjustments to your resources. However, it is vital to keep in mind that the modifications we make will not be automatically reflected back to the configuration files we use. To add a label to an existing resource, we can use the following command: # It creates a label “group” with a value of “temp”kubectl label pod/metadata-demo group=tempTo remove the label, use this command: kubectl label pod/metadata-demo group-Label selectors: Label selectors provide us the ability to filter objects based on their labels. For example, we can filter out all of the objects that have the label env: production. Annotations: Annotations are another type of metadata we can use in Kubernetes. Like labels, annotations are key-value pairs. Annotation lets us associate arbitrary metadata with Kubernetes objects. Labels, on the other hand, can be used in the process of identifying and selecting items, but annotations cannot. Annotations are meant to be used to store any information about an object that doesn’t identify it. Similar to labels, annotations can be added in a number of different methods, the most common of which are via config files or the kubectl command line. For example, here’s the configuration file for a Pod that has the annotation imageregistry: https://hub. docker. com/: apiVersion: v1kind: Podmetadata: name: annotations-demo annotations:  imageregistry:  https://hub. docker. com/ spec: containers: - name: nginx  image: nginx:1. 14. 2  ports:  - containerPort: 80Kubernetes command-line toolsThe command-line tools for Kubernetes are as follows:  kubectl kind minikube kubeadmkubectl: The Kubernetes command-line tool is known as kubectl. It allows us to run commands against Kubernetes clusters. We can use kubectl to:  Deploy applications Inspect and manage cluster resources View logsSyntax: kubectl [command] [type] [name] [flags]To verify that your cluster is working, use the following command: kubectl cluster-infoKubernetes control plane is running at https://127. 0. 0. 1:50918CoreDNS is running at https://127. 0. 0. 1:50918/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. The above output of kubectl cluster-info command indicates that the control plane is active and responding to requests. Now use the kubectl command to list all nodes in our cluster: kubectl get nodesThe output will resemble what is shown below: NAME    STATUS  ROLES      AGE  VERSIONminikube  Ready  control-plane  48d  v1. 24. 3The above output shows a cluster with single node.  Note: Everything in Kubernetes is represented as an object, which can be obtained and manipulated via the RESTful API. The kubectl get command retrieves a list of objects of the specified type from the API. In the above case, it’s of type “node”. To see more detailed information about an object, we use the following command: kubectl describe node &lt;name of the node&gt;Because it is such a large output, I have chosen to exclude the actual output that the describe command generates. To check the status of the pods for all the namespaces kubectl get pods --all-namespacesThe output look like the ones shown below: NAMESPACE       NAME                     READY  STATUS  RESTARTS    AGEdefault        nicepod                   1/1   Running  3 (45m ago)  45dkube-system      coredns-6d4b75cb6d-nsx8w           1/1   Running  4 (45m ago)  45dkube-system      etcd-minikube                1/1   Running  5 (45m ago)  45dkube-system      kube-apiserver-minikube           1/1   Running  5 (45m ago)  45dkube-system      kube-controller-manager-minikube       1/1   Running  6 (45m ago)  45dkube-system      kube-proxy-8fhb9               1/1   Running  4 (45m ago)  45dkube-system      kube-scheduler-minikube           1/1   Running  6 (45m ago)  45dkube-system      storage-provisioner             1/1   Running  16 (45m ago)  45dkubernetes-dashboard  dashboard-metrics-scraper-78dbd9dbf5-9xnmp  1/1   Running  3 (45m ago)  45dkubernetes-dashboard  kubernetes-dashboard-5fd5574d9f-bpqrz    1/1   Running  8 (45m ago)  45dTo check the physical and internal IP details of all the pods, use the following command: kubectl get pods -n &lt;namespace&gt; -o widekind: kind lets us run Kubernetes on our local computer. This tool requires that we have Docker installed and configured. In other words, it runs local Kubernetes clusters using Docker container “nodes”. kind was primarily designed for testing Kubernetes, but it can also be used for local development or CI. It has both support for:  Multi-cluster Multi-nodeIt puts the cluster into Docker containers. This leads to a significantly faster startup speed compared to spawning a VM. However, creating a cluster is very similar to minikube’s approach. Installation: Refer here to install kind. Create a cluster: To create a cluster: kind create clusterThis will bootstrap a Kubernetes cluster using a pre-built node image. The node image is a Docker image for running nested containers, systemd, and Kubernetes components. Prebuilt images are hosted at kindest/node. To specify another image use the --image flag as shown below: kind create cluster --image &lt;&gt;Note that kind has the ability to load our local images directly into the cluster. This saves us from having to set up a registry and push our image every time we want to try out our changes. Using the following command, we can able to load our local images: kind load docker-image my-app:latestBy default, the cluster will be given the name kind. Use the --name flag to assign the cluster a different context name. kind create cluster --name kind-2Interacting with the cluster: After creating a cluster, you can use kubectl to interact with it by using the configuration file generated by kind. By default, the cluster access configuration is stored in ${HOME}/. kube/config if $KUBECONFIG environment variable is not set. To list the clusters created using kind: kind get clustersIn order to interact with a specific cluster, we only need to specify the cluster name as a context in kubectl: kubectl cluster-info --context kind-&lt;name&gt;Deleting a cluster: kind delete cluster --name &lt;name&gt;Loading an image into the cluster: kind load docker-image my-custom-image-0 my-custom-image-1 --name kind-2Note that --name flag is optional if the cluster created wit default name i. e. , kind. minikube: Like kind, minikube is a tool that enables us to run Kubernetes on a local computer. In order for us to get a feel for Kubernetes, minikube runs a Kubernetes cluster with a single node on each of our individual computers (including Windows, macOS, and Linux PCs). It does this by spawning a VM that is essentially a single-node K8s cluster.  Spawning: Spawning in computing refers to a function that loads and executes a new child process. Note that minikube does not support multi-cluster; however, it supports multi-node cluster. Minimum requirements to install minikube:  2 CPUs or more 2GB of free memory 20GB of free disk space Container or virtual machine manager, such as:     Docker   Hyperkit   Hyper-V   KVM   Parallels   Podman   VirtualBox   VMware Fusion/Workstation   minikube with Docker: If we choose Docker as the container or VM manager for minikube, minikube runs as a Docker container as shown below:Figure 1: minikube custer runs as a Docker container. To install the latest minikube stable release on on x86-64 macOS using Homebrew: brew install minikubeStart our cluster: To start a single-node cluster: minikube startminikube dashboard: minikube has integrated support for the web-based dashboard. To access the web-based dashboard: minikube dashboardThis will enable the dashboard add-on, and open the proxy in the default web browser. If we we do not want to launch a web browser, we can instruct the dashboard command to instead merely output a URL as shown below: minikube dashboard --urlWe can use dashboard to:  Deploy containerized applications to a Kubernetes cluster.  Troubleshoot your containerized application.  Manage the cluster resources.  Get an overview of applications running on your cluster.  Creating or modifying individual Kubernetes resources such as Deployments, Jobs, DaemonSets, etc. kubeadm: We can use the kubeadm tool to create and manage Kubernetes clusters. It performs the tasks required to get a minimum viable, secure cluster up and running in a manner that is friendly to users. Deploying a Kubernetes clusterThere are many different ways that Kubernetes cluster can be deployed:  Installing and configuring a full-fledged Kubernetes cluster with multiple nodes.  Using the built-in Kubernetes cluster in Docker Desktop.  Running a local cluster using minikube CLIInstalling and configuring a full-fledged Kubernetes cluster with multiple nodes: It is not an easy job to set up a full-fledged Kubernetes cluster with multiple nodes, particularly if we are not familiar with Linux or network management. A proper Kubernetes installation involves the use of many physical or virtual machines and requires proper network setup to allow all containers in the cluster to communicate with one another. A full-ledged Kubernetes cluster can be installed:  On our our local machines.  On virtual machines provided by cloud providers like Google Compute Engine, Amazon EC2, Microsoft Azure, and so on.  On the other hand, almost all cloud service providers now offer managed Kubernetes services, which saves us the trouble of installing and managing it.  A brief list of the services offered by the most popular cloud providers follows: Google offers GKE, Amazon has EKS, Microsoft has AKS, and so on. Keep in mind that installing and managing Kubernetes ourselves is a much more complicated task than just using it, especially before we know a lot about its architecture and how it works. Using the built-in Kubernetes cluster in Docker Desktop: Docker Desktop for macOS and Windows comes preinstalled with a single-node Kubernetes cluster that can be enabled by going into the Settings dialog box as shown in Figure 2 below.                    Figure 2: Kubernetes preinstalled with Docker Desktop.       Running a local cluster using minikube CLI: minikube, which is a command-line tool that is maintained by the Kubernetes community, is yet another method that can be used to create a Kubernetes cluster. It enables us to run Kubernetes on a local computer. In order for us to get a feel for Kubernetes, minikube runs a Kubernetes cluster with a single node on each of our individual computers (including Windows, macOS, and Linux PCs). Installing minikube CLI: minikube CLI supports macOS, Linux, and Windows. It just consists of a single executable binary file, which can be found in the minikube repository on GitHub. More information on how to install it for various operating systems can be found here.   Packing applications in containers: Applications come in many different shapes and sizes. They begin by taking input, manipulating the data, and then providing the results. Application programs normally have a language runtime, libraries including external libraries, configurations, and source code as their primary components. There are many ways to deploy these applications as a whole:  Configuration management systems: We can use configuration management systems like Puppet or Ansible, which use code to install, run, configure, and update applications.  Omnibus package: It’s an effort to include everything that the program requires in a single file as much as possible. For example, a Java omnibus package would include the Java runtime as well as all the JAR files for the application. From an operational point of view, we need to be able to handle not just all of these various kinds of packages or applications, but also a fleet of servers on which to host them. These servers need to be provisioned, networked, deployed, configured, kept up-to-date with security patches, monitored, and managed. This all takes a significant amount of time, technical expertise, and effort just to provide a platform to run applications on. Kubernetes configuration: Frequently asked questions (FAQ)Can we have more than one container in a worker node?: Yes. Each worker node holds one or more containers. Where the pods are hosted?: The worker node(s) hosts the Pods. What is the workload in Kubernetes?: A workload is an application running on Kubernetes. On Kubernetes, our workload is always run within a collection of pods, regardless of whether it consists of a single component or several that work together. What is kubelet?: Every worker node has a node agent called Kubelet that it runs. It oversees communicating with the master components and manages the running pods. Here are the key things that the Kubelet does:  Receiving pod specs Downloading pod secrets from the API server Mounting volumes Running the pod’s containers (via the configured runtime) Reporting the status of the node and each pod Running the container startup, liveness, and readiness probesWhat is the term “resource” used in Kubernetes?: Anything we create in a Kubernetes cluster is considered a resource: deployments, pods, services and so on. What Are Kubernetes watches?: Getting the state of the resources using the Kubernetes API at a certain point in time isn’t always enough when working with Kubernetes clusters, because these clusters are highly dynamic in nature. In most cases, we also want to keep an eye on these resources and track events as they happen. Regular pooling is one strategy that may be used. However, polling puts us at risk of missing events that take place in the interval between polling cycles. Additionally, when the number of resources that need to be monitored rises, this strategy will not scale up very effectively. TODO - How it’s addressed by watches? What is Kubernetes API?: TODO What are proxies in Kubernetes?: KubeProxy is a network proxy that is implemented as a component of the Kubernetes Service. It runs on every node in the cluster. It maintains a record of the network rules that govern communication between pods located within and outside of the cluster. In other words, these network rules allow network communication to our Pods from network sessions inside or outside of our cluster. To put it simply, kube-proxy on each node takes care of redirecting traffic to the correct pod. Modes: There are two kube-proxy modes:  IPTABLES (default) IPVS (IP Virtual Server) - It’s an advanced configuration used in a cluster with thousands of servers and offers high network performance. What is Container Runtime Interface (CRI) in Kubernetes?: In the early days of Kubernetes, the Docker Engine was the only container runtime that was supported. After some time, more container runtimes were made available, such as rkt and hypernetes, and each container runtime has it own strengths. It became evident that users of Kubernetes want a choice of runtimes that would work best for them. The Container Runtime Interface (CRI) was released (1. 5 release) to allow that flexibility. It’s a plugin interface which enables kubelet to use a wide variety of container runtimes without the need to recompile. For more information about CRI, refer here. What it mean by Kubernetes deprecating Docker?: Docker as an underlying container runtime is being deprecated in favor of runtimes that use the Container Runtime Interface (CRI) created for Kubernetes. Note that Docker-produced images will continue to work in our cluster with all runtimes normally. Docker is still a useful tool for building containers, and the images that are produced as a result of executing docker build can still run in our Kubernetes cluster. The container runtime is responsible for pulling and running our container images. Docker is a popular choice for that runtime, but Docker was not designed to be embedded inside Kubernetes, and that causes a problem. Docker is not just a single piece; rather, it refers to a whole technology stack. It includes a component known as “containerd,” which is a high-level container runtime by itself. Also, Docker has a lot of UX enhancements that make it really easy for humans to interact with when we are doing development work. However, these UI enhancements aren’t required for Kubernetes, , because it isn’t a human. Kubernetes cluster has to use another tool called Dockershim to get at what it really needs, which is containerd. Kubernetes support for Docker via dockershim is now removed. What is dockershim in Kubernetes and why was it removed from Kubernetes?: The introduction of the Container Runtime Interface (CRI) was a great step forward in providing us with the flexibility to use any container runtime of our choice, but it did introduce a problem: Docker Engine was being used as a container runtime well before CRI was introduced, and Docker Engine is not compatible with CRI.  What is the term “shim” in computing? A shim is a small library that transparently intercepts and modifies calls to an API, usually for compatibility purposes. To solve this issue, a small software shim called dockershim was introduced as part of the kubelet component specifically to fill in the gaps that exist between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted. However, this little software shim was never intended to be a long-term solution. Over the years, dockershim’s existence has made the kubelet much more complicated than it needs to be. Unfortunately, this caused some concern among the community since the deprecation notice wasn’t presented as clearly as it should have been. Kubernetes has published a blog post that includes a frequently asked questions section in an effort to allay the concerns of the community and clear up any misunderstandings on what Docker is and how containers function inside Kubernetes. Note that Docker is not going away, either as a tool or as a company. Which container runtimes are supported by Kubernetes?: In its early stages, Kubernetes only provided support for the Docker container runtime engine. However, it is not the case any more. Kubernetes now provides support for a variety of runtimes:  Docker via dockershim rkt CRI-O Frakti rktlet CRI-containerdHow to SSH into minikube virtual machine?: Use the following command to SSH into minikube VM: minikube sshuname -alogoutHow to deploy pods in minikube cluster?: Let’s create a pod and expose it on port 80: kubectl create deployment hello --image=docker. io/nginx:1. 23kubectl expose deployment hello --type=NodePort --port=8080The create deployment command will create a deployment, which will then create a pod with one container running the given image. We can use the following command to get the status of the deployment: kubectl get deployTo see the pod that the deployment created, run the following command: kubectl get poThe output looks like the pictures below: NAME           READY  STATUS  RESTARTS    AGEecho-5b565549d6-6rrv2  1/1   Running  1 (5h21m ago)  8hhello-795877ccfc-245tt  1/1   Running  1 (5h21m ago)  8hnicepod         1/1   Running  4 (5h21m ago)  46dNote that the reason the Pod has two sets of random characters in the name is that the deployment has created a ReplicaSet to perform the actual pod management, and the ReplicaSet created the Pod. When the service is exposed as type NodePort, it is available to the host on some port. But we did not run the pod on the 8080 port. In the cluster, ports are mapped. We need the cluster IP and the exposed port in order to get to the service. Use the following command to find the cluster IP: minikube ipTo check if the pod is created, use the following command: kubectl get podsHow to start a cluster with multi nodes using minikube?: We can start a cluster with multi nodes in the driver of our choice. In the below, we are starting a cluster with 2 nodes: minikube start --nodes 2 -p multinode-demoWe can also check the status of your nodes: minikube status -p multinode-demoOutput: multinode-demotype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configuredmultinode-demo-m02type: Workerhost: Runningkubelet: RunningWhat is CoreDNS?: CoreDNS is a flexible DNS server written in the Go programming language that can serve as the Kubernetes cluster DNS. Since it is written in Go, the top-end performance is also not good. But it’s not a problem because we don’t use CoreDNS in places where we need really fast performance. It uses a plugin-based architecture, which is easily extended. To support different cloud-native stacks, for example. It also supports DNS over TLS (DoT) and DNS over gRPC. It has native support for service discovery for Kubernetes. It is integrated with etcd and cloud vendors (e. g. , AWS’s Route 53). It has support for Prometheus metrics. It is also capable of forwarding to a recursive DNS server, and this is really important because it doesn’t actually have the capability to act as a full-service DNS recursive itself. CoreDNS vs. kube-dns:  An easily extensible plugin-architecture with a rich set of plugins It is simple, with fewer moving parts (single executable and process), and all written in Go.  Customizable DNS entries in and out of the cluster domain. For example, we can add static DNS data within the cluster domain.  Experimental server-side search path to reduce query volume. Is multi-cluster supported by minikube?: No. Note that minikube does not support multi-cluster; however, it supports multi-node cluster. Is multi-cluster supported by kind?: Yes. Note that kind does support both multi-cluster and multi-node cluster. What are the pods running under “kube-system” namespace in minikube?: The following pods are running under “kube-system” namespace:  coredns etcd-minikube kube-apiserver-minikube kube-controller-manager-minikube kube-proxy kube-scheduler-minikube storage-provisionerWhat are Kubernetes objects?: Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of our cluster. Specifically, they can describe:  What containerized applications are running (and on which nodes) The resources available to those applications The policies around how those applications behave, such as restart policies, upgrades, and fault-toleranceOnce we’ve create the object, Kubernetes will keep working to make sure that object exists. By creating an object, we tell the Kubernetes system how we want our cluster’s workload to look. This is the desired state of our cluster. "
    }, {
    "id": 51,
    "url": "http://localhost:4000/apache-spark/2022/spark-fundamentals",
    "title": "Apache Spark Fundamentals",
    "body": "2022/08/07 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! The necessity for data distribution over several computers arises from either the data being too massive to store on a single machine or the computation being too long to accomplish on a single machine. Spark architectureSpark is built on a master-slave architecture, which we refer to as the Spark cluster. Spark cluster is made up of one master node and one or more worker nodes. Each worker node has at least one executor.                    Apache Spark Architecture.        Note: From Spark 2. 0 onwards we can access SparkContext object through SparkSession. Spark driver: The Spark driver is a Java process that runs the main() function of the user program. In other words, the user program’s main() function executes in the driver process. It runs on the master node of the Spark cluster. It is the core of a Spark Application and stores all relevant information during the application’s lifetime. When we’re working with a Spark shell, the driver is part of the shell. The driver is responsible for:  Maintaining information about the Spark application.  Responds to user’s program or input.  Analyzing, distributing, and scheduling work (as tasks) across executors. Executor: An executor resides in the worker node, and each worker node consists of one or more executors. Executors are responsible for running one or more tasks. Executors are launched at the start of a Spark application in coordination with the cluster manager. The driver launches and removes executors dynamically as needed. Each executor is responsible for:  Executing tasks assigned to it by the driver and reporting the status of the computation to the driver.  Caching (in memory) or persisting (on disk) the data in the worker node. Cluster manager: The cluster manager controls physical machines and allocates resources such as CPU, memory, and so on to Spark applications. Cluster manager types: Apache Spark currently supports the following cluster managers:  Standalone - a basic cluster manager included with Spark that makes it simple to build up a cluster.  Hadoop YARN - the resource manager in Hadoop 2 and 3.  Kubernetes - an open-source system for automating deployment, scaling, and management of containerized applications. Local mode: Spark features a local mode in addition to its cluster mode. In local mode, the driver and executors run as threads on our own machine rather than as part of a cluster. Spark’s APIsSpark has two core sets of APIs:  Low-level APIs High-level APIsPartitionsIn order to enable each executor to run in parallel, Spark splits the data into chunks called partitions. A partition is a collection of rows that are stored on a single physical machine in a Spark cluster. The number of executors and partitions influences Spark’s parallel processing capability.  Many executors but only one partition = No parallism.  Many partitions but only one executor = No parallism.  Many executors with more than one partition = Parallism. It is important to note that with DataFrames, we do not (for the most part) manually manipulate partitions. We merely specify high-level data transformations in the physical partitions. Spark determines how the tasks will be executed in parallel on the cluster. Resilient Distributed Datasets (RDDs)Spark is built around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of data that can be processed in parallel. There are two ways to create RDDs:  Parallelizing an existing collection in our driver program Referencing a dataset in an external storage system such as a shared filesystem, HDFS, Amazon S3, etc. What is fault tolerance in Spark RDD?: Fault refers to failure or defect or flaw. Fault tolerance is the ability of a system to continue working normally in the event of the failure of one or more of its components. RDDs have the capability of automatically recovering from failures. Traditionally, distributed computing systems have provided fault tolerance through data replication or checkpointing. However, Spark uses a different approach called “lineage. ” In data-intensive applications, lineage-based recovery is much more efficient than replication. It saves both time (since writing data over the network takes significantly longer than writing it to RAM) and memory space. The operations carried out in an RDD are a set of Scala functions that are run on that partition of the RDD. This set of operations is combined to form a DAG. RDD tracks the graph of transformations (in DAG) that was used to build it and reruns these operations on base data to reconstruct any lost partitions. TransformationOn RDDs, Spark uses two types of operations: transformation and action. Transformation is an operation that produces new RDD from the existing RDDs. Since RDDs are immutable in nature, each transformation operation always results in a new RDD instead of changing an existing one. Having said that, it takes RDD as input and produces one or more RDD as output. Because RDD is immutable, Spark generates an RDD lineage that will be used to perform transformations on base data to recover any lost RDD. Also, it’s important to note that transformations are lazy. This means Spark will not act on transformations until we call an action. Let’s do a simple transformation to identify all the even integers in our current DataFrame. &gt;&gt;&gt; nums = spark. range(5)&gt;&gt;&gt; mums. collect()[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]&gt;&gt;&gt; even_nums = nums. where( id % 2 = 0 )&gt;&gt;&gt; even_nums. collect()[Row(id=0), Row(id=2), Row(id=4)]Types of transformations: There are two types of transformations:  Narrow dependencies aka narrow transformation Wide dependencies aka wide transformationNarrow transformation: Transformations with narrow dependencies have each input partition contributing to just one output partition. Wide transformation: Multiple input partitions contribute to many output partitions in a wide dependency style transformation. Wider transformations are the result of the groupByKey and reduceByKey functions, which compute data that spans many partitions. This type of transformation is also known as “shuffle transformations” since it shuffles the data. Wider transformations are more costly than narrow transformations due to shuffling. "
    }, {
    "id": 52,
    "url": "http://localhost:4000/apache-spark/2022/spark-shuffling",
    "title": "Shuffling in Apache Spark",
    "body": "2022/08/06 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! What is shuffling in Spark?"
    }, {
    "id": 53,
    "url": "http://localhost:4000/data-management/2022/data-engineering-and-data-science",
    "title": "Data Engineering vs. Data Science",
    "body": "2022/08/02 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! Data engineering and data science are not sub-disciplines of each other. They complement one another but do not rely on each other. Data engineering is the step before data science. They give data scientists the data they need to turn it into something useful.                    Data Engineering vs. Data Science.       "
    }, {
    "id": 54,
    "url": "http://localhost:4000/data-engineering/2022/reverse-etl",
    "title": "Reverse ETL",
    "body": "2022/07/31 - Before we dive deep into the subject, let’s clarify operational analytics briefly. Operational analytics is a subset of data analytics that aims to improve business operations. Business operations are the everyday activities businesses participate in to improve their value and generate a profit. The primary distinction between operational analytics and other forms of analytics is that operational analytics is analytics on the fly, which means that data emerging from different segments of an organization is analyzed in real-time to feed back into the organization’s immediate decision-making. Some people refer to operational analytics as continuous analytics, which is another way of highlighting the continuous digital feedback loop that can exist between different components of an organization. Traditional approachETL, which stands for extract, transformation, and load, is a traditional data integration approach that takes data from several data sources, transforms it, and stores it in a centralized data repository. This centralized data repository might be a data warehouse or a data lake, and it would serve as the only reliable source of truth. In this ETL-based method, data goes through staging and integration phases before arriving at either a data warehouse or a data lake as the final destination. What is reverse ETL?We switched from ETL to ELT (Extract, Load, and Transform) as data volumes increased and we needed a faster way to load data into a data warehouse or data lake. "
    }, {
    "id": 55,
    "url": "http://localhost:4000/data-security-and-compliance/2022/data-governance",
    "title": "Data Governance",
    "body": "2022/07/26 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! What do we mean when we claim the data is secure? It means:  Data is only accessible to authorized users in authorized ways.  Data is auditable, which means that all accesses, including modifications, are logged.  Data complies with all regulations. What is data governance?Data governance is a collection of processes, roles, policies, responsibilities and standards that ensure data is secure, accurate, and available as an asset across the organization. In other words, it is the process of defining security guidelines and policies and making sure they are followed by having authority and control over how data assets are managed. It defines who can take what actions based on what data, under what conditions, and using what methods. The practice of data governance also includes adhering to external standards set by industry associations, government agencies, and other stakeholders. Regulations such as the GDPR and many others impose legal accountability and severe penalties on firms (in the case of GDPR, the penalty may be up to 4% of global revenue) that fail to adhere to governance principles around data privacy, retention, and portability. No matter how large the organization is or the volume of data, the principles of data governance remain the same. However, data governance practitioners make decisions about tools and ways to implement them based on practical factors that are affected by the environment in which they work.  It is important to note that data governance is not just about data security; it is more than that. Data governance guarantees that data is trustworthy while also maintaining its quality and integrity. Enhancing trust in data: The goal of data governance is to establish trust in data. Trustworthy data is required to enable decision making and risk assessment. To ensure data trust, a data governance strategy must address three key aspects:  Discoverability Security AccountabilityData quality: We can accomplish data quality by using data governance. Data quality is focused on making sure that the data complies with our data quality dimensions such as accuracy, completeness, validity, timeliness, consistency, and uniqueness. To put it simply, data quality guarantees that we have high-quality data. We can figure out the data quality by looking at its source, i. e. , where it came from (e. g. , was it entered by humans, who often make mistakes?). A sense of ownership is one method for improving data quality—making sure the business unit responsible for generating the data also owns the quality of that data. The organization can set up a data acceptance process that states that data cannot be used until the people who own it demonstrate that it meets the organization’s quality criteria. Why is data governance important?: Data governance is not only about managing the rules but also making data useful. Effective data governance implementation ensures that high-quality data must be efficiently available to the right people throughout the organisation. Data governance vs. data management: Data governance:  Data governance describes the general structure that need to be in place.  It has policies, procedures, and accountability.  It’s more about what should happen and how things should happen. Data management:  The goal of data management is to put all of the policies into practice.  It’s a hands-on daily effort to make sure that the policies we put in place are being followed. Data encryption: Data encryption adds another level of protection. Only the systems or users who have the keys can make sense of the data. There are several data encryption implementations available. However, the envelope encryption technique offers the best security while also performing well. Identity and access management (IAM): Access control is based on who the user is (called “authentication”) and whether or not the user is allowed to access certain data (called “authorization”). Authorization is based on a set of permissions and roles that are tied to a user’s or service’s identity. Authorization decides whether or not the user is allowed to access or perform any activity on the data in question. Authentication is typically handled by providing a password associated with the individual seeking access. The obvious weakness of this approach is that anybody who has obtained access to the password may access whatever that person has access to. So we must add extra layers of protection to the authentication process by making it more difficult for attackers to acquire access-we can enhance it even further by adding two-factor authentication. If possible, we may include biometric data in the authentication request. Roles: The first thing we need to consider when it comes to data governance is who is engaged and what their roles are. There are usually several roles, but the most important one is data owner or data sponsor. These are the individuals who have ultimate decision-making authority over the data and are solely responsible for ensuring that the data is accurate and up to date. These individuals have a deeper understanding of the groundwork that’s being done with the data. There could be many data owners or data sponsors. For instance, a sales data owner, an inventory data owner, etc. "
    }, {
    "id": 56,
    "url": "http://localhost:4000/data-lake-and-lakehouse/2022/introduction-to-data-lake",
    "title": "Introduction to Data Lake",
    "body": "2022/07/25 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! Data lake trade-offsThere are trade-offs involved in the shift from traditional data storage and processing platforms, such as databases and data warehouses, to data lakes. After the migration to the data lake, we have sacrificed the following capabilities in favor of others:  We have given up durability and consistency features like ACID transactions in return for the ability to process them on a highly scalable platform.  We have traded performance characteristics such as indexing and caching in exchange for the capacity to handle data in multiple formats.  We have given up features like versioning and auditing in exchange for the ability to decouple storage and computing. "
    }, {
    "id": 57,
    "url": "http://localhost:4000/data-files-and-formats/2022/hdfs-parquet-file",
    "title": "Let’s Know About the Parquet File",
    "body": "2022/07/25 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! Parquet MotivationParquet was created to make the benefits of compressed, efficient columnar data representation available to all Hadoop ecosystems. It’s an open source file format. As stated, it’s a columnar storage format for data that is not necessarily tubular (rows and columns), such as data with complex nested structures. In columnar storage, data in a single column is stored contiguously as shown below:                Columnar storage format.    Numerous benefits are associated with the columnar format. We will explore them in detail below. Efficient compression: Parquet is built to support very efficient compression and encoding schemes. The columnar representation efficiently compresses the data more than the row-based type. Since the values in a column are often similar and of the same type, compression codecs are able to compress it more efficiently than a large row of data with a variety of data types and ranges. When compressing similar data sequences, many compression codecs achieve a better compression ratio. Parquet allows compression schemes to be specified on a per-column level, and each column may use a different scheme for compression to get a better compression ratio. It is designed to be future-proof so that new encodings may be added as they become available. Column pruning: Columnar storage can be a good choice when queries do not read all columns of the data. This is known as column pruning and may result in a significant speed boost. For example, if we have a table with 100 columns but only need 10 of them, we must load all 100 columns in a row-based format since they are constructed row by row. However, just 10 columns will need to be loaded in Parquet. Storing nested data in columns: Now comes the question of how to store nested data in columns while keeping its structure! "
    }, {
    "id": 58,
    "url": "http://localhost:4000/apache-spark/2022/spark-bucketing-and-partitions",
    "title": "Partitions and Bucketing in Spark",
    "body": "2022/07/25 - Partitioning and bucketing are used to improve the reading of data by reducing the cost of shuffles, the need for serialization, and the amount of network traffic. Partitioning in SparkApache Spark’s speed in processing huge amounts of data is one of its primary selling points. Spark’s speed comes from its ability to allow developers to run multiple tasks in parallel and independently across hundreds of machines in a cluster or across multiple cores on a desktop. It’s all possible because Apache Spark RDDs serve as the main interface. These RDDs are partitioned and run in parallel behind the scenes. So, what exactly is the partition in Spark? Spark organizes data into smaller pieces called “partitions”, each of which is kept on a separate node in the cluster. Each partition is an atomic chunk of data. We can think of partition as a subset of our data. Simply said, it’s a subset of the superset. Parallelism: Partitions are the fundamental building blocks of parallelism in Apache Spark. Spark’s parallelism enables developers to parallelly execute several tasks across a large number of nodes in a cluster. Co-location: Partitioning can also deliver a significant speed benefit by reducing the amount of data to be shuffled across the network. If RDDs are too large to fit on a single node, they must be partitioned (distributed) across many nodes. Apache Spark automatically partitions RDDs and distributes them across different nodes.                    Figure 1: Data Partitioned by Spark.       Spark is a distributed computing system, so it can operate on data partitons in parallel. A transformation or any sort of computation on a data partitons is called a task and each task generally takes place on one core. As there is one task for every partition, the total number of tasks is the same as the total number of partitions. Optimal partitioning in Spark strikes a balance between read performance and write performance. Please take the following considerations into account:  Too many partitions: Too many partitions can slow down the time it takes to read and force Spark to create more tasks to process the data, which could cause the driver to get an “out of memory” error.  Too many small partitions could waste a lot of time because the cluster would spend more time coordinating tasks and sending data between workers than actually doing the job.  Overly large partitions can even cause executor “out of memory” errors.  Using a small number of large partitions may leave some worker cores idle. If one of the workers is falling behind the other, we may have to wait a long time for the last task to be completed. If a worker goes down, we’ll have to reprocess a huge amount of data.  Few or lack of partitions: On the other hand, a lack of or few partitions may indicate that Spark’s parallelism is not being fully utilized, resulting in long computation and write times. Furthermore, having few partitions may result in skewed data and inefficient resource use.  Data skewness: We refer to data as “skewed” when it is not evenly distributed among workers. In Apache Spark, transformations like join, groupBy, and orderBy change data partitioning, which results in data skewness. Common effects of skewed data include the following:     Slow running stages or tasks: Certain operations will take very long to complete because of the huge volume of data that a particular worker must process.    Spilling data to disk: In the event that more data is not able to fit in the memory of a worker, it must be written to disk.    Out-of-memory error: If worker runs out of disk space, an error is thrown.    Spark generally does a good job splitting data into partitons to ensure parallelism and efficient computing. However, when dealing with very large data sets, it is sometimes necessary to manually adjust partitions to ensure optimal performance. As a rule of thumb, 128 megabytes is a good size for a data partition when dealing with data sets above 1 gigabyte. That is, # Partitions = Dataset Size (mb) / 128 mb. To get us started with partitioning, here are some fundamentals:  Every node (worker) in a Spark cluster contains one or more partitions of any size.  By default, Spark tries to set the number of partitions automatically based on the total number of cores on all the executor nodes. This is the most effective method for determining the total number of spark partitions in an RDD. However, we can manually set it by passing it as a second parameter to parallelize (for example, sc. parallelize(data, 10)).   The number of partitions in Spark is configurable.  Only one partition is processed by one executor at a time, so the size and number of partitions handed to the executor are directly proportional to the time it takes to complete them. Types of partitioning: We bring in the data, transform it, and then either write it somewhere else or display it on our console or screen. This allows us to further divide Spark partitions into three categories.    Input partition - Control the size of the partition using spark. sql. files. maxPartitionBytes Output partition - Control the size of the partition using coalesce (to reduce partition) and repartition (to reduce or increase partition) Shuffle partition - Control the partition count using spark. sql. shuffle. partitionsInput partition: When a job is submitted for processing, each data partition is sent to the specific executors. Each executor processes one partition at a time. Hence, the time it takes each executor to process data is directly proportional to the size and number of partitions. The more partitions there are, the more work will be distributed (shuffled) among the executors. There will also be fewer partitions. This means that processing will be done faster and in larger chunks. What influences the input partition?: Since the input partition is a critical parameter for performance, how should we control the size of the partition? Spark has a property called spark. sql. files. maxPartitionBytes that can be used to control the number of bytes that are packed into a single partition when reading files, and if we want to check the number of partitions, we can do it by using the method rdd. getNumPartitions(). The another alternative method to get the number of partitions is rdd. partitions. size().  Note: The default value for spark. sql. files. maxPartitionBytes property is 134217728 (128 MB). In the event that the size of either the input file block or a single partition is more than 128 MB, Spark will split the read into multiple partitions. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC. By using the aforementioned property and method, we can know the default number of partitions being created and then modify it to our liking. We want to control the size of the partitions because we don’t want too much data shuffled around. Because each partition is sent to an executor, the number of partitions determines how much data is shuffled around. Output partition: Output partition is essential whenever we are reading data for further processing. If there are several partitions, data will be spread over many files, making it more time-consuming to search for specific criteria in the first query. Whenever our job is finished and we are writing back the data, at that point, the output partition determines the number of files that will get returned to our disk. The larger the number of partitions, the larger the number of files. The total number of files generated is directly proportional to the number of output partitions. Also, our memory utilization will be higher while processing the metadata table, as it contains several partitions. Having said that, the output partition influences how we write the data back into the disk after the job is completed. What influences the output partition?: There are two methods that can influence the way our data is written:  repartition - The repartitioning operation reduces or increases the number of partitions. This is a costly operation that involves shuffling all the data over the network and redistributing it so that it is spread out evenly. Data is serialized, transferred, and then de-serialized throughout this process.  coalesce - The coalesce operation uses existing partitions and reduces (doesn’t increase) the number of partitions to minimize the amount of data that’s shuffled. Coalesce results in partitions that contain different amounts of data. In most cases, the coalesce runs faster than the repartition operation. By using coalesce and repartition, we can limit the output to a certain number of files or tasks. Shuffle partition: In cases of wide transformations, where data is required from other partitions, Spark performs a data shuffle. We can’t avoid making such wide transformations, but we can reduce the impact on performance by configuring parameters. Wide transformations use shuffle partitions to shuffle data. We can control the shuffle partition using spark. sql. shuffle. partitions.  Note: The number of partitions is fixed at 200 regardless of the data’s size or the number of executors. How to set shuffle partition?: When the data is small, the number of partitions should be reduced; otherwise, too many partitions containing less data will be created, resulting in too many tasks with less data to process. When working with a big dataset, it may be beneficial to increase the shuffle partition from the default value of 200. Spark partitioning strategies: Apache Spark supports two types of partitioning strategies:  Hash partitioning (Default) Range partitioningLet’s understand the rationale for the need for a variety of partitioning strategies. A suitable data partitioning strategy will enable us to reduce the skew in the data. Keep in mind that Spark limits the size of a partition to 2 GB, although there may be cases when a single key includes several relevant records that add up to more than 2 GB in total. If we have partitioned our data on that specific key, then we are going to have problems shuffling that data; we will get errors. So, picking the right partitioning strategy is very important. It also helps us get the best performance out of different operations like join and groupby. Choosing the right partitioning strategy will help us to co-locate the data. The term “collate the data” refers to the fact that data that is processed together is also stored together. By doing so, we may avoid redistributing data around the cluster every time we do a groupby or join operation. Partitioning decisions are influenced by a wide variety of factors, including:  Available resources (CPU, memory, and network) Transformation used to derive RDD -How do we get the right partitions?: Recommended number of partitions: Apache Spark can only run a single concurrent task for every partition of an RDD, up to the number of available cores in our cluster (and probably 2 to 3 times that). Hence, it is common practice to choose a good number of partitions equal to or more than the number of executors to maximize parallelism. For example, if we have eight worker nodes and each node has four CPU cores, we may set the number of partitions to be anywhere from 64 (2 x 8 x 4) to 96 (3 x 8 x 4). By calling sc. defaultParallelism we can get the default level of parallelism defined on SparkContext. The maximum size of a partition is limited by how much memory an executor has. Recommended partition size: The average partition size ranges from 100 MB to 1000 MB. For instance, if we have 30 GB of data to be processed, there should be anywhere between 30 (30 gb / 1000 mb) and 300 (30 gb / 100 mb) partitions. Other factors to be considered: It is important to understand and carefully choose the right operators for actions like reduceByKey or aggregateByKey so that our driver is not put under pressure and the tasks are properly executed on executors.   When data is skewed, it is recommended to use an appropriate key that can spread the load evenly. Sometimes, it may not be clear which re-partitioning key should be used to make sure data is evenly distributed. In these situations, we can use methods like salting, which involves adding a new fake or random key and using it along with the current key for better distribution of data. This is how it works: saltKey = actualJoinKey + randomFakeKey. Hive partition vs. Spark partition: Hive partition is not the same as Spark partition. The two are completely different. They are both subsets of the superset, but a Spark partition is a piece of data that has been broken down so that it can be processed in parallel in memory. Hive partition is in disk storage and persistence. Bucketing in SparkBucketing is an optimisation feature that Apache Spark (also in Apache Hive) has supported since version 2. 0. It’s a way to improve performance by dividing data into smaller, manageable portions called “buckets” to identify data partitioning as it’s being written down. This feature is intended to improve the efficiency of reading same data. This efficiency improvement is mostly about getting rid of shuffles (also called exchanges) in join and aggregation queries.  Apache Hive popularised buckets, and Apache Spark added their support. Shuffle is a very expensive operation as it moves the data between executors or even across worker nodes in a cluster. Hence, it should be avoided wherever feasible. When there is a problem with the performance of Spark jobs, we should examine the transformations that involve shuffling. With bucketing, we can pre-shuffle and store the data in a pre-shuffled form. It controls the physical arrangement of the data, thus we shuffle the data beforehand to prevent having to do it later on. Sorting the bucket: In addition to bucketing, we can also do sorting, which sorts each bucket according to the given fields. However, the opposite is not possible; we cannot sort without bucketing. Configure bucketing: Note that bucketing is enabled by default. Spark controls whether or not it should be enabled using the configuration property spark. sql. sources. bucketing. enabled. How to create buckets?: In Spark API, the bucketBy method can be used for this purpose. bucketBy(n, field1, field2, . . . )The first argument specifies the number of buckets to generate. The number of buckets is equal to the number of files generated. # In Pythondf. write\  . bucketBy(16,  key )\  . sortBy( value )\  . saveAsTable( table_name )Bucketing applicable only to persistent tables: Only persistent tables can be used for bucketing and sorting. We can only use the saveAsTable method; we can’t use the save method. "
    }, {
    "id": 59,
    "url": "http://localhost:4000/scala/2022/case-class",
    "title": "Case Class in Scala",
    "body": "2022/07/24 - What is a case class?Case classes are a special kind of class created using the keyword case. Case classes are excellent for data transfer objects and for representing immutable data. It is a type of class that is mostly utilised for data storage. case class Person(name: String, age: Int)The following beneficial features or conveniences are automatically added to our class when the Scala compiler discovers a case class:  It adds a factory method, apply() for creating new instances, so we don’t need to use the keyword new to instantiate a class.  Unless all arguments in the parameter list of a case class are declared as var, all arguments implicitly get a val prefix, and thus the val keyword is optional. To put it another way, by default, case classes automatically transform arguments to value fields (val fields), therefore the val keyword is not required to prefix them. If we require a variable field, we can still use the var keyword, but this is not what the case class was intended for.  The compiler automatically implements the following methods for the class:     apply   unapply   copy   equals   hashCode   toString    Every case class has a method named copy that allows us to easily create a same or a modified copy of the class’s instance.  A companion object is created automatically with the appropriate apply and unapply methods.  The methods generated by the Scala compiler for case classes aren’t special in any way, other than that they are automatically generated for us. By adding the methods and companion objects ourselves, we may avoid using case classes. Because it would take a lot of time and effort to write each of these methods appropriately for every data-storage class, case classes have the advantage of being more convenient. Like a regular class, a case class can extend other classes, including trait and case classes. Case classes are Scala’s way of allowing pattern matching on objects without requiring a large amount of boilerplate code. Let’s explore the advantages of a case class in action: scala&gt; case class Person(name: String, age: Int)defined case class Personscala&gt; val person = Person( John , 36)val person: Person = Person(John,36)scala&gt; val otherPerson = person. copy(name =  Robert )val otherPerson: Person = Person(Robert,36)scala&gt; val someOtherPerson = person. copy()val someOtherPerson: Person = Person(John,36)scala&gt; println(person. equals(someOtherPerson))truescala&gt; person == otherPersonval res0: Boolean = falsescala&gt; person match {   |   case Person(x, 36) =&gt; s $x is a younger person    |   case Person(x, 50) =&gt; s $x is a older person    | }val res1: String = John is a younger person Line 1: Able to instantiate without a new operator because of the companion object’s factory method, Person. apply().  Line 5: The auto-generated toString method prints the fields in our instance.  Line 7: The second instance (otherPerson) shares the same value for the second field, so we only need to specify a new value for the first field in the copy method.  Line 10: Copies of case classes result in strict equivalence. Hence, the equals statement in line 12 results in true.  Case class extends other class: If our case class had extended another class with its own fields but we hadn’t added the fields as case class parameters, the generated methods wouldn’t have been able to make use of them. Before using case classes, it’s necessary to be aware of this important caution. Copy vs. cloneThe case class’s copy method lets us make a copy of an object. Remember that a copy method differs from a clone method in that a copy allows us to modify fields at any time while it is being copied. case class Worker(name: String, department: String)object CaseClassCopyMethod extends App { val john = Worker( John ,  Sales ) val mike = john. copy(name= Mike )}Abstract case classWhen we declare an abstract case class, Scala won’t generate the apply method in the companion object, which makes sense as we can’t create an instance of an abstract class.  abstract case class PositiveInt(value: Int)Case objectWe can also create case objects. Just like a regular object, a case object inherits all the features of a regular object. Note that a case object is serialisable by default, whereas a regular object is not.  case object Fruit {  val costPerKg = 10}We must extend the Serializable trait in order to make a case object serialisable. "
    }, {
    "id": 60,
    "url": "http://localhost:4000/apache-spark/2022/spark-caching-data-in-memory",
    "title": "Need for Caching in Apache Spark",
    "body": "2022/07/24 - Caching is a common approach for reusing certain computation in Apache Spark. It’s one of the optimisation techniques in Spark. Among big data practitioners, caching receives a lot of consideration and discussion. The underlying data store is accessed each time an operation is performed in a Spark DataFrame, requiring the entire dataset to be sent across the network for each execution. When the same data is retrieved frequently, caching is immensely useful. The objective of caching is to minimize disk I/O and retrieve data as quickly as possible by storing it in RAM rather than on the disk. Caching helps in storing interim and partial results so they’ll be utilised in subsequent computation stages. These intermediate results are kept either in memory (by default) or on disk and are stored as RDDs. Obviously, data that has been cached in memory is faster to access, but cache space is always limited.  Run out of memory: We will quickly exhaust our memory if we cache every RDD. We must thus carefully consider our options before choosing to cache an RDD. The art of cachingCaching frequently involves a lot of trial and error since it needs taking into account the number of available nodes, the importance of each RDD, and the amount of memory available for caching. As stated above, we will quickly exhaust our memory (resulting in an out-of-memory situation) if we cache every RDD. We must thus carefully consider our options before choosing to cache an RDD. Caching a DataFrameIn Spark, there are two functions that can be used to cache both RDD, DataFrame, and Dataset:  dataFrame. cache() dataFrame. persist(storageLevel)Cache and persist are distinct in that cache will store the RDD in memory while persist can cache at various storage levels. Persistence storage levels: There are various persistence storage levels, including:  MEMORY_ONLY - Persist data on disk only in serialised format.  MEMORY_ONLY_SER MEMORY_ONLY_SER_2 DISK_ONLY - Persist data in memory only in deserialised format.  DISK_ONLY_2 DISK_ONLY_3 MEMORY_AND_DISK - Persist data in memory, and if enough memory is not available, evicted blocks will be serialised to disk.  MEMORY_AND_DISK_2 MEMORY_AND_DISK_SER MEMORY_AND_DISK_SER_2 OFF_HEAP - Data is persisted outside the heap called off-heap.  Need for off-heap storage type: Bad memory management with data-intensive applications might cause the garbage collector to pause for long durations. Off-heap storage is not managed by the JVM’s garbage collector. Therefore, it must be explicitly managed by the application. Caching a tableWe should use sqlContext. cacheTable( table_name ) in order to cache Spark SQL, or alternatively use CACHE TABLE table_name SQL query. Uncaching a DataFrame or tableThe following are the ways to uncache:  dataFrame. unpersist() spark. catalog. uncacheTable( tableName ) UNCACHE TABLE table_name spark. catalog. clearCache() - It clears both DataFrames/tables"
    }, {
    "id": 61,
    "url": "http://localhost:4000/data-engineering/2022/introduction-to-data-engineering",
    "title": "Introduction to Data Engineering",
    "body": "2022/07/24 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! What is data engineering?Data engineering is the process of designing and building systems for acquiring large amounts of raw operational data from various sources and formats, analysing it, transforming it, and storing it at scale. The management of the data infrastructure is a particular focus of data engineering. Data engineering is usually regarded as the primary support for present data analytics needs. Data is handled by various technologies and stored in a variety of forms and structures, making data analysis difficult. Hence, we need the right specialists (data engineers) and technology to unify various data sets so that they remain available and usable for further analysis by data analysts and scientists. The first step in data engineering is obtaining vast quantities of data from a variety of sources and bringing them to a data lake or any other big data platform. We call this as data ingestion. Once the data has been landed in the data lake, data curation is carried out. Data curation is the process of organising data to fulfil the requirements and interests of a given group or organisation. Here, the data is transformed into value. The diagram below shows all of the different data engineering tasks that happen in the direction shown.                    Data engineering activities at a high level - By Author.       Let’s explore each data engineering activity individually. What is a data pipeline?:                Designed by Freepik.    A data pipeline is commonly constructed as part of data engineering to transfer data from a source to a destination. Data is transformed and optimised along the pipeline’s journey, eventually reaching a state where it can be analysed and used to generate business insights. Many of the manual processes required in processing and optimising continuous data loads are now automated by modern data pipelines. Having stated that, data engineering’s primary task is the creation of data pipelines. They provide a seamless, automatic flow of data from one stage to another and eliminate the most of manual steps from the process. To effectively meet analytics expectations, data engineers construct a series of operations into a data pipeline that consists of many phases. Components of a data pipeline: A data pipeline consists of several components. Each component does a specific function and contributes to the overall success of the source-to-destination data journey. The different components include:  Ingestion Transformation PresentationIngestion: The first step in a typical pipeline is obtaining data from various sources like databases, files, APIs, logs, and so on, and storing it in a data lake or equivalent big data platform. The landing zone, which is essentially a dumping ground, is where data from multiple sources is first stored. Depending on the data source and business needs, data engineers may need to choose between batch or streaming (real-time) ingestion, or both. The complexity of data ingestion is related to the nature of the data source, whether static or dynamic.  Static data: Static data that normally does not change over time once it has been recorded, or if it does, it changes infrequently. It is a fixed dataset, thus it does not need refreshing. A list of nations, cities, counties, etc. , would be an example of static data. Ingestion from a static data source is straightforward, and we can get it using a scheduler-driven batch-ingestion procedure.  Dynamic data: The literal meaning of the term “dynamic” is change. When used to describe data, as in “dynamic data,” the term refers to data that is subject to change as necessary. There are several reasons why data should be dynamic. Dynamic data is mostly transactional data that is often updated, meaning it changes as new information becomes available. On an e-commerce website, for instance, product information is kept in a database and updated in real-time. The true challenge of data ingestion emerges when such dynamic data is ingested. Data engineers must use effective change data capture (CDC) approaches to incrementally ingest data. Ingestion types: There are two types of ingestion commonly used:  Batch ingestion Event streaming (real-time)Batch ingestionTODO Event streamingThe event-streaming type of ingestion is occurring in real time and is gaining popularity and becoming more common. Note that there is no true real-time ingestion capability. When collecting data from several sources and delivering it to a destination system, there is a certain amount of inherent delay in every data pipeline. So a more appropriate term would be near-real-time. The pipeline processes events sequentially as they come. Event streaming ingestion is a natural fit for several modern data source types. Transformation: Before the ingested data is put to real use, it must undergo many actions, such as cleansing, curation, standardization, and finally aggregation. This collaborative process is known as data transformation. In other words, the data transformation process prepares data to the point where it can be readily and easily consumed for analytical processes. Depending on the business requirements, data transformation might be a fairly complicated process. The levels of difficulty depends on a variety of factors including the following:  Volume (size), variety (types), and breadth (how many) of data sources Complexity of business requirements Frequency of data ingestionPresentation: Presentation is the stage where transformation outcomes are stored. This stage could be one of the following:  Intermediate - This is where the intermediate results of the transformations are kept, and it is referred to as the silver-layer in data lakes and data warehouses.  Final - This is where the final outputs of the transformations are kept, and it is referred to as the gold-layer in data lakes and data warehouses.  The three layers of the data lake: Typically, the data lake has the following three layers: Bronze, silver and gold. Bronze represents unaltered raw data input from data sources, silver represents filtered and cleansed data, and gold represents business-level aggregates. "
    }, {
    "id": 62,
    "url": "http://localhost:4000/data-security-and-compliance/2022/envelope-encryption",
    "title": "Envelope Encryption - Putting Your Encryption Key in an Envelope Is the Safer Option",
    "body": "2022/07/22 - Before we get into envelop encryption, let’s go over some of the fundamentals of encryption/decryption. Key material: A key material is a random sequence of bits that is used in a cryptographic algorithm to convert plaintext to ciphertext (encrypted text) and vice versa. To prevent the ciphertext from being decoded back to plaintext, this key material must be kept secret. Note that public key cryptography, also known as asymmetric cryptography, employs both public and private key materials, with the public key material intended to be shared. Key: A key, which is just a short name for an encryption key, has a key-id or alias, key material, and other information about it, such as who created it and so on. Plaintext + Key = CiphertextCiphertext - Key = PlaintextNote: + and - signs indicate encryption and decryption actions, respectively. The image below shows what a key is composed of in general:                    Figure 1: A key with key id, key material and other metadata.       key | |--- Key ID: 12t9c |--- Key Material: 010101110101100 |--- Other Metadata: [Created By: User1, Created On: 08-01-2022, Modified By: User1, Modified On: 08-01-2022,. . . ]What is envelope encryption?When we encrypt our data using an encryption key, it is protected, but we also need to protect that encryption key. Envelope encryption is the method of first encrypting plaintext data using a key known as a data key and then encrypting that data key with a different key known as a root key or master key. When compared to a real-life use case, the envelope encryption approach is the action of locking the home with a key and keeping the key somewhere safe. It’s a two-step procedure:  Encrypt data with a data key (aka encryption key).  Further encrypt the data key with another key called the root key or master key. Having said that, the envelop encryption scheme generates two keys. The only key that is encrypted in this envelop encryption scheme is the data key; the root key is not. The root key has to be kept in plaintext so that it can be used to decrypt the data key.  This two-step procedure can be made longer by encrypting with another encryption key and then encrypting the resulting encrypted key with yet another encryption key, and so on, as shown below. Root key –&gt; Encryption-keyN –&gt; … Encryption-key1 –&gt; Data key –&gt; DataBut in the end, though, the top-level key must stay in plaintext so that we can decrypt the rest of the keys and our data. This top-level plaintext key is known as the root key.                    Figure 2: Envelope encryption high-level flow.       How can root keys generated in plaintext be protected?: Thankfully, the key vault saved the day! The key vault safeguards our root keys by securely storing and managing them using specialised cryptographic hardware with the highest level of security. In the case of AWS, the root keys are stored securely in the AWS KMS. Where to keep the data keys?: We have just learnt that root keys are stored securely inside a key vault, but where are data keys stored securely? Can’t the data keys be safely kept in the same vault as the root keys? We certainly can, but why is it necessary? Remember that the data keys are inherently protected by encryption. So we should not be concerned about where the data keys are kept. We can put them anywhere, but it’s best to put them alongside the encrypted data.                    Figure 3: Envelope encryption flow.       Benefits of envelop encryption: This strategy isn’t meant to make things more secure; instead, it’s meant to improve performance. Public-key algorithms are often sluggish and use asymmetric algorithms. In contrast, symmetric algorithms are very fast. So, the data, considerably very large in size, is quickly encrypted with a symmetric algorithm using a random key. The random key is subsequently encrypted using a public-key scheme. This approach combines the benefits of public-key scheme with the efficiency of symmetric encryption. Envelope encryption reduces the network load since only the request and delivery of the considerably smaller data key go over the network. The data key is used locally in our application, so we don’t have to send the whole block of data to encrypt or decrypt it, which would cause network latency. Key rotation: Key rotation is the process of retiring an encryption key and replacing it with a new cryptographic key. Changing the keys on a regular basis helps us meet industry standards and best practices for cryptography. A good security practice is to rotate keys on a regular and automated basis. Key rotation is required by several industrial requirements. It is important to note that key rotation changes only the key material, which is used in encryption or decryption operations. Regardless of how many times the key material changes, the key id remains unchanged. So every time we rotate the key, a new key material is created.                    Figure 4: Key rotation.       In general, key-vault tools safely keep all old versions of the key material forever, so we can decrypt any data that was encrypted with that key and do not delete any rotated old key materials until we delete the keys. When we use a rotated key to encrypt data, the key-vault uses the current key material. When we use the rotated key to decrypt ciphertext, key-vault uses the key material version that was used to encrypt it. Automatic key rotation: It’s recommended to rotate keys automatically on a regular schedule. The frequency of rotation is defined by a rotation schedule. The rotation schedule may be determined by:  Age of the key Number or volume of data encrypted with a certain key versionWe may enable automatic key rotation for an existing key depending on the key-vault tool we use. When we set up automatic key rotation for the keys, key-vault tools usually make new key materials for those keys on a regular basis, like once a year. How often should the keys be rotated?: Key rotations should ideally occur every few months, but once every three months is recommended. If we want to perform this more often, we:  Have a large amount of data.  Have a high-value data.  Have a shared-environment. Benefits of key rotation:  Key rotation reduces the amount of data encrypted with the same key version, making assaults less likely.  Regular key rotation ensures that our system is capable of handling manual key rotation, whether due to a security breach or for any other reason. "
    }, {
    "id": 63,
    "url": "http://localhost:4000/scala/2022/def-vs-lazy-val",
    "title": "Defining Variables Using the `def` Keyword in Scala",
    "body": "2022/07/21 - The title reads as follows: “Defining Variables Using the def Keyword. ” Before we continue, let’s see if this title is appropriate. It’s not! Let’s first be clear that a def is not a variable declaration. It’s a declaration of a function or method instead. Consider the following code: scala&gt; def i = 3def i: Intscala&gt; i. getClassval res0: Class[Int] = intscala&gt; println(i)3scala&gt; val j = 3val j: Int = 3scala&gt; j. getClassval res1: Class[Int] = intscala&gt; println(j)3scala&gt; i + jval res2: Int = 6As we see above, variable definition works using def. Unlike a val or var declaration, a def is not a variable declaration. It’s a function declaration. What we are doing is creating a function or method that returns a constant number 3. Having said that, both i and j are functions that do not take any arguments. Unless the lazy keyword is prefixed, variables declared with the val or var are evaluated immediately, whereas def is evaluated on call, which means it is lazy evaluated. Having said, def evaluation happens when called explicitly.  Note that Scala does not permit the creation of lazy variables, i. e. , var. Only lazy values (val) are allowed. Difference between lazy val and def               Designed by Freepik.    Difference between lazy val and def:  When accessed for the first time, lazy evaluates and caches the result.  The def declaration is evaluated every time we call method name. Let’s see the difference between lazy val and def in action: scala&gt; lazy val a = { println( a value evaluated ); 1}lazy val a: Intscala&gt; aa value evaluatedval res0: Int = 1scala&gt; aval res1: Int = 1scala&gt; def b = { println( b function evaluated ); 2}def b: Intscala&gt; ab function evaluatedval res3: Int = 2scala&gt; ab function evaluatedval res4: Int = 2As we see above, a is evaluated only once, and any subsequent invocations of a return the cached result. This indicates that lazy val only needs to be evaluated once and then the result is stored forever. On the other hand, a def is evaluated every time it is invoked. In this case, we see println output every time we invoke the function. I hope this explains how def differs from val in terms of evaluation frequency. Happy learning! "
    }, {
    "id": 64,
    "url": "http://localhost:4000/data-streaming/time-concepts-in-data-streaming",
    "title": "Time Concepts in Data Streaming",
    "body": "2022/07/21 -  Writing in progress: If you have any suggestions for improving the content or notice any inaccuracies, please email me at hello@senthilnayagan. com. Thanks! Timestamps, particularly event-times, are significant aspects of any data streaming application. The entity that produces events sets the timestamp as part of the event itself. "
    }, {
    "id": 65,
    "url": "http://localhost:4000/rust/2022/ownership-and-borrowing",
    "title": "Rust’s Ownership and Borrowing Enforce Memory Safety",
    "body": "2022/07/19 - The Rust’s ownership and borrowing might be confusing if we don’t grasp what’s really going on. This is particularly true when applying a previously learned programming style to a new paradigm; we call this a paradigm shift. Ownership is a novel idea, yet tricky to understand at first, but it gets easier the more we work on it. Before we go further about Rust’s ownership and borrowing, let’s first understand what memory safety and memory leak are and how programming languages deal with them. What is memory safety?Memory safety refers to the state of a software application where memory pointers or references always refer to valid memory. Because memory corruption is a possibility, there are very few guarantees about a program’s behaviour if it is not memory safe. Simply put, if a program isn’t really memory safe, there are few assurances about its functionality. When dealing with a memory-unsafe program, a malicious party is able to use the flaw to read secrets or execute arbitrary code on someone else’s machine.                    Designed by Freepik.       Let’s use a pseudocode to see what valid memory is. // pseudocode #1 - shows valid reference{ // scope starts here int x = 5  int y = &amp;x} // scope ends hereIn the above pseudocode, we’ve created a variable x assigned with a value of 10. We use the &amp; operator or keyword to create a reference. Thus, the &amp;x syntax lets us create a reference that refers to the value of x. To put it simply, we’ve created a variable x that owns 5 and a variable y that is a reference to x. Since both variables x and y are in the same block or scope, variable y has a valid reference that refers to the value of x. As a result, variable y has a value of 5. Take a look at the below pseudocode. As we can see, the scope of x is limited to the block in which it’s created. We get into dangling references when we try to access x outside of its scope. Dangling reference…? What exactly is it? // pseudocode #2 - shows invalid reference aka dangling reference{ // scope starts here int x = 5} // scope ends hereint y = &amp;x // can't access x from here; creates dangling referenceDangling reference: A dangling reference is a pointer that points to a memory location that has been given to someone else or released (freed). If a program (aka process) refers to memory that has been released or wiped out, it might crash or cause non-deterministic results. Having said that, memory unsafety is a property of some programming languages that allows programmers to deal with invalid data. As a result, memory unsafety introduced a variety of problems that might cause the following major security vulnerabilities:  Out-of-bounds Reads Out-of-bounds Writes Use-After-FreeVulnerabilities caused by memory unsafety are at the root of many other serious security threats. Unfortunately, uncovering these vulnerabilities can be extremely challenging for developers. What is a memory leak?It’s important to understand what a memory leak is and what its consequences are.                    Designed by Freepik.       A memory leak is an unintentional form of memory consumption whereby the developer fails to free an allocated block of heap memory when it is no longer needed. It’s simply the opposite of memory safety. More on the different memory types later, but for now, just know that a stack stores fixed-length variables known at compile time, whereas the size of variables that may change later at runtime must be placed on the heap. When compared to heap memory allocation, stack memory allocation is considered to be safer since memory is automatically released when it is no longer relevant or necessary, either by the programmer or by the program-runtime itself. However, when programmers generate memory on the heap and fail to remove it in the absence of a garbage collector (in the case of C and C++), a memory leak develops. Also, if we lose all references to a chunk of memory without deallocating that memory, then we have a memory leak. Our program will continue to own that memory, but it has no way of ever using it again.  A little memory leak is not a problem, but if a program allocates a larger amount of memory and never deallocates it, the program’s memory footprint will continue to rise, resulting in Denial-of-Service. When a program exits, the operating system immediately recovers all of the memory it owns. As a result, a memory leak only affects a program while it’s running; it has no effect once the program has terminated. Let’s go over the key consequences of memory leaks. Memory leaks reduce the performance of the computer by reducing the amount of available memory (heap memory). It eventually causes the whole or a portion of the system to stop working correctly or to slow down severely. Crashes are commonly linked with memory leaks. Our approach to figuring out how to prevent memory leaks will vary depending on the programming language we’re using. Memory leaks might begin as a small and nearly “unnoticeable problem”, but they can escalate very quickly and overwhelm the systems they impact. Wherever feasible, we should be on the lookout for them and take action to rectify them rather than leave them to grow. Memory unsafety vs. memory leaksMemory leaks and memory unsafety are the two types of issues that have received the greatest attention in terms of prevention and remediation. It’s important to note that fixing one does not automatically fix the other.                    Figure 1: Memory unsafety vs. memory leaks.       Various types of memories and how they operateBefore we go any further, it’s important to understand the different types of memory that our code will use at runtime. There are two types of memory, as follows, and these memories are structured differently.  Processor register Static Stack HeapBoth processor register and static memory types are beyond the scope of this post. Stack memory and how it works: The stack stores data in the order in which it is received and removes it in the reverse order. Items can be accessed from the stack in the last in, first out (LIFO) order. Adding data onto the stack is called “pushing,” and removing data off the stack is called “popping. ” All data stored on the stack must have a known, fixed size. Data with an unknown size at compile time or a size that might change later on must be stored on the heap instead. As developers, we do not have to worry about stack memory allocation and deallocation; the allocation and deallocation of stack memory is “automatically done” by the compiler. It implies that when data on the stack is no longer relevant (out of scope), it is automatically deleted without the need for our intervention. This kind of memory allocation is also known as temporary memory allocation, because as soon as the function finishes its execution, all the data that belongs to that function is flushed out of the stack “automatically. ”  All primitive types in Rust live on the stack. Types like numbers, characters, slices, booleans, fixed-size arrays, tuples containing primitives, and function pointers can all sit on the stack. Heap memory and how it works: Unlike a stack, when we put data on the heap, we request a certain amount of space. The memory allocator locates a large enough unoccupied place in the heap, marks it as in use, and returns a reference to that location’s address. This is referred to as allocating. Allocating on the heap is slower than pushing to the stack because the allocator never has to hunt for an empty location to put new data. Furthermore, because we must follow a pointer to get to data on the heap, it is slower than accessing data on the stack. Unlike the stack, which is allocated and deallocated at compile time, heap memory is allocated and deallocated during the execution of a program’s instructions. In some programming languages, to allocate heap memory, we use the keyword new. This new keyword (aka operator) denotes a request for memory allocation on the heap. If sufficient memory is available on the heap, the new operator initialises the memory and returns the unique address of that newly allocated memory. It’s worth mentioning that heap memory is “explicitly” deallocated by the programmer or the runtime. How do various other programming languages guarantee memory safety?When it comes to memory management, particularly heap memory, we’d prefer our programming languages to have the following characteristics:  We’d prefer to release memory as soon as possible when it’s no longer needed, with no runtime overhead.  We should never maintain a reference to a data that has been freed (aka a dangling reference). Otherwise, crashes and security issues might occur. Memory safety is ensured in different ways by programming languages by means of:  Explicit memory deallocation (adopted by C, C++) Automatic or implicit memory deallocation (adopted by Java, Python, and C#) Region-based memory management Linear or unique type systemsBoth region-based memory management and linear type systems are beyond the scope of this post. Manual or explicit memory deallocation: Programmers must “manually” release or erase allocated memory when using explicit memory management. A “deallocation” operator (for instance, delete in C) exists in languages with explicit memory deallocation.  Garbage collection is too costly in systems languages like C and C++, therefore explicit memory allocation continues to exist. Leaving the responsibility of freeing memory to the programmer has the benefit of giving the programmer total control over the life cycle of the variable. However, if deallocation operators are used incorrectly, a software fault may occur during execution. In fact, this manual allocation and releasing process is prone to errors. Some common coding errors include:  Dangling reference Memory leakDespite this, we preferred manual memory management over garbage collection since it gives us more control and provides better performance. Note that the goal of any system programming language is to get as “close to the metal” as possible. In other words, they favour better performance over convenience features in the tradeoff.  It’s entirely our (developers) responsibility to ensure that no pointer to the value we freed is ever used. In the recent past, there have been several proven patterns for avoiding these errors, but it all boils down to maintaining rigorous code discipline, which requires applying the right memory management method consistently. Key takeaways are:  Have greater control over memory management.  Less safety as a result of dangling references and memory leaks.  Results in a longer development time. Automatic or implicit memory deallocation: Automatic memory management has become an essential feature of all modern programming languages, including Java. In the case of automatic memory deallocation, the garbage collectors serve as an automatic memory managers. These garbage collectors periodically go through the heap and recycle chunks of memory that are not being used. They manage the allocation and release of memory on our behalf. So we don’t have to write code to perform memory management tasks. That’s great since garbage collectors free us from the responsibility of memory management. Another advantage is that it reduces the development time. Garbage collection, on the other hand, has a number of drawbacks. During garbage collection, the program should pause and spend time determining what it needs to clean up before proceeding. Furthermore, automatic memory management has higher memory needs. This is due to the fact that a garbage collector performs memory deallocation for us, which consumes both memory and CPU cycles. As a result, automated memory management might degrade application performance, particularly in large applications with limited resources. Key takeaways are:  Eliminates the need for developers to release memory manually.  Provides efficient memory safety with no dangling references or memory leaks.  Simpler and straightforward code.  Faster development cycle.  Have less control over memory management.  Causes latency as it consumes both memory and CPU cycles. How does Rust guarantee memory safety?Some languages provide garbage collection, which looks for memory that is no longer in use while the program runs; others require the programmer to explicitly allocate and release memory. Both of these models have benefits and drawbacks. Garbage collection, though perhaps the most widely used, has some drawbacks; it makes life easy for developers at the expense of resources and performance. Having said that, one gives efficient memory management control, while the other provides higher safety by eliminating dangling references and memory leaks. Rust combines the benefits of both worlds.                    Figure 2: Rust has better control over memory management and provide higher safety with no memory issues.       Rust takes a different approach to things than the other two, based on an ownership model with a set of rules that the compiler verifies to ensure memory safety. The program will not compile if any of these rules are violated. In fact, ownership replaces runtime garbage collection with compile-time checks for memory safety.                    Explicit memory management vs. Implicit memory management vs. Rust’s ownership model.       It takes some time to get used to ownership because it is a new concept for many programmers, like myself. OwnershipAt this point, we have a basic understanding of how data is stored in memory. Let’s look at ownership in Rust more closely. Rust’s biggest distinguishing feature is ownership, which ensures memory safety at compile-time. To begin, let’s define “ownership” in its most literal sense. Ownership is the state of “owning” and “controlling” legal possession of “something”. With that said, we must identify who the owner is and what the owner owns and controls. In Rust, each value has a variable called its owner. To put it simply, a variable is an owner, and the value of a variable is what the owner owns and controls.                    Figure 3: Variable binding shows the owner and its value/resource.       With an ownership model, memory is automatically released (freed) once the variable that owns it goes out of scope. When values go out of scope or their lifetimes end for some other reason, their destructors are called. A destructor, particularly an automated destructor, is a function that removes traces of a value from the program by deleting references and frees up memory. Borrow checker: Rust implements ownership through the borrow checker, a static analyzer. The borrow checker is a component in the Rust compiler that keeps track of where data is used throughout the program, and by following ownership rules, it’s able to determine where data needs to be released. Furthermore, the borrow checker ensures that deallocated memory can never be accessed at runtime. It even eliminates the possibility of data races caused by concurrent mutation (modification). Ownership rules: As previously stated, the ownership model is built on a set of rules known as the ownership rules, and these rules are relatively straightforward. The Rust compiler (rustc) enforces these rules:  In Rust, each value has a variable called its owner.  There can only be one owner at a time.  When the owner goes out of scope, the value will be dropped. The following memory errors are protected by these compile-time checking ownership rules:  Dangling references: This is where a reference points to a memory address that no longer contains the data to which the pointer was referring; this pointer points to null or random data.  Use after frees: This is where memory is accessed once it has been freed, which can crash. This memory location can also be used by hackers to execute code.  Double frees: This is where allocated memory is freed, and then freed again. This might cause the program to crash, potentially exposing sensitive information. This also allows a hacker to run whatever code they choose.  Segmentation faults: This is where the program tries to access memory it’s not allowed to access.  Buffer overrun: This is where the volume of data exceeds the storage capacity of the memory buffer, causing the program to crash. Before getting into the details of each ownership rule, it’s important to understand the distinctions between copy, move, and clone. copy: A type with a fixed size (particularly primitive types) can be stored on the stack and popped off when its scope ends, and may be quickly and easily copied to create a new, independent variable if another part of the code requires the same value in a different scope. Because copying stack memory is cheap and fast, primitive types with a fixed-size are said to have copy semantics. It cheaply creates a perfect replica (a duplicate).  It’s worth noting that primitive types with fixed-size implement the copy trait to make copies. let x =  hello ;let y = x;println!( {} , x) // helloprintln!( {} , y) // hello In Rust, there are two kinds of strings: String (heap allocated, and growable) and &amp;str (fixed size, and can’t be mutated). Because x is stored on the stack, copying its value to produce another copy for y is easier. This is not the case for a value that is stored on the heap. This is how the stack frame looks:                    Figure 4: Both x and y have their own data.       Duplicating data increases program runtime and memory consumption. Therefore, copying isn’t a good fit for large chunks of data. move: In Rust terminology, “move” means the ownership of the memory is transferred to another owner. Consider the case of complex types that are stored on the heap. let s1 = String::from( hello );let s2 = s1;We might assume that the second line (i. e. let s2 = s1;) would make a copy of the value in s1 and bind it to s2. But this is not the case. Take a look at the below one to see what’s happening to String under the hood. A String is made up of three parts, which are stored on the stack. The actual contents (hello, in this case) are stored on the heap.  Pointer - points to the memory that holds the contents of the string.  Length - it’s how much memory, in bytes, the contents of the String is currently using.  Capacity - it’s the total amount of memory, in bytes, that the String has received from the allocator. To put it in other words, the metadata is kept on the stack while the actual data is kept on the heap.                    Figure 5: The stack holds the metadata while the heap holds the actual contents.       When we assign s1 to s2, the String metadata is copied, meaning we copy the pointer, the length, and the capacity that are on the stack. We do not copy the data on the heap that the pointer refers to. The data representation in memory looks like the one below:                    Figure 6: Variable s2 gets a copy of the pointer, length, and capacity of s1.       It’s worth noting that the representation does not look like the one below, which is what memory would look like if Rust copied the heap data as well. If Rust performed this, the s2 = s1 operation could be extremely slow in terms of runtime performance if the heap data were large.                    Figure 7: If Rust copied the heap data, another possibility for what let s2 = s1 might do is data replication. However, Rust does not copy by default.       Note that when complex types are no longer in scope, Rust will call the drop function to explicitly deallocate heap memory. However, both data pointers in Figure 6 are pointing to the same location, which is not how Rust works. We will get into the details shortly. As previously stated, when we assign s1 to s2, variable s2 receives a copy of s1’s metadata (pointer, length, and capacity). But what happens to s1 once it’s been assigned to s2? Rust no longer considers s1 to be valid. Yes, you read that correctly. Let’s think about this let s2 = s1 assignment for a moment. Consider what happens if Rust still considers s1 as valid after this assignment. When s2 and s1 go out of scope, they will both try to free the same memory. Uh-oh, that’s not good. This is referred to as a double free error, and it is one of the memory safety bugs. Memory corruption can result from freeing memory twice, posing a security risk. To ensure memory safety, Rust considered s1 invalid after the line let s2 = s1. Therefore, when s1 is no longer in scope, Rust does not need to release anything. Examine what happens if we try to use s1 after s2 has been created. let s1 = String::from( hello );let s2 = s1;println!( {}, world! , s1); // Won't compile. We'll get an error. We’ll get an error like the one below because Rust prevents you from using the invalidated reference: $ cargo run  Compiling playground v0. 0. 1 (/playground)error[E0382]: borrow of moved value: `s1` --&gt; src/main. rs:6:28 |3 |   let s1 = String::from( hello ); |     -- move occurs because `s1` has type `String`, which does not implement the `Copy` trait4 |   let s2 = s1; |       -- value moved here5 | 6 |   println!( {}, world! , s1); |              ^^ value borrowed here after move | = note: this error originates in the macro `$crate::format_args_nl` (in Nightly builds, run with -Z macro-backtrace for more info)For more information about this error, try `rustc --explain E0382`. As Rust “moved” s1’s ownership of the memory to s2 after the line let s2 = s1, it considered s1 invalid. Here is the memory representation after s1 has been invalidated:                    Figure 8: Memory representation after s1 has been invalidated.       When only s2 remains valid, it alone will free the memory when it goes out of scope. As a result, the potential for a double free error is eliminated in Rust. That’s wonderful! clone: If we do want to deeply copy the heap data of the String, not just the stack data, we can use a method called clone. Here’s an example of how to use the clone method: let s1 = String::from( hello );let s2 = s1. clone();println!( s1 = {}, s2 = {} , s1, s2);When using the clone method, the heap data does get copied into s2. This works perfectly and produces the following behaviour:                    Figure 9: When using the clone method, the heap data does get copied into s2.       The use of the clone method has serious consequences; it not only copies the data, but it also does not synchronize any changes between the two. In general, clones should be planned carefully and with full awareness of the consequences. By now, we should be able to distinguish between copy, move, and clone. Let’s look at each ownership rule in more detail now. Ownership rule 1: Each value has a variable called its owner. It implies that all values are owned by variables. In the example below, variable s owns the pointer to our string, and in the second line, variable x owns a value 1. let s = String::from( Rule 1 );let n = 1;Ownership rule 2: There can only be one owner of a value at a given time. One can have many pets, but when it comes to the ownership model, there is only one value at any given moment :-)                    Designed by Freepik.       Let’s look at the example using primitives, which are fixed-size known at compile time. let x = 10;let y = x;let z = x;We have taken 10 and assigned it to x; in other words, x owns 10. Then we’re taking x and assigning it to y and we’re also assigning it to z. We know that there can only be one owner at a given time, but we’re not getting any errors here. So what’s going on here is that the compiler is making copies of x every time we assign it to a new variable. The stack frame for this would be as follows: x = 10, y = 10 and z = 10. This, however, does not appear to be the case as this: x = 10, y = x, and z = x. As we know, x is the sole owner of this value 10, and neither y nor z can own this value.                    Figure 10: Compiler made copies of x to both y and z.       Because copying stack memory is cheap and fast, primitive types with a fixed-size are said to have copy semantics, whereas complex types move ownership, as previously stated. Thus, in this case, the compiler makes the copies. At this point, the behaviour of variable binding is similar to that of other programming languages. To illustrate the rules of ownership, we need a complex data type. Let’s look at data that is stored on the heap and see how Rust understands when to clean it up; the String type is an excellent example for this use case. We’ll focus on String’s ownership-related behaviour; these principles, however, also apply to other complex data types. The complex type, as we know, manages data on the heap, and its contents are unknown at compile time. Let’s look at the same example we have seen before: let s1 = String::from( hello );let s2 = s1;println!( {}, world! , s1); // Won't compile. We'll get an error.  In the case of String type, the size might expand and be stored on the heap. This means:    At runtime, the memory must be requested from the memory allocator (let’s call it first part).   When we’re done using our String, we need to return (release) this memory back to the allocator (let’s call it second part).   We (developers) took care of the first part: when we call String::from, its implementation requests the memory it needs. This part is almost common across programming languages.  However, the second part is different. In languages with a garbage collector (GC), the GC keeps track of and cleans up memory that is no longer in use, and we don’t have to worry about it. In languages without a garbage collector, it’s our responsibility to identify when memory is no longer needed and call for it to be explicitly released. It has always been a challenging programming task to do this correctly:    We will waste memory if we forget.   We will have an invalid variable if we do it too early.   We will get a bug if we do it twice.   Rust handles memory deallocation in a novel way to make our lives easier: the memory is automatically returned once the variable that owns it goes out of scope. Let’s back to business. In Rust, for complex types, operations like assigning a value to a variable, passing it to a function, or returning it from a function don’t copy the value: they move it. To put it simply, complex types move ownership.  When complex types are no longer in scope, Rust will call the drop function to explicitly deallocate heap memory. Ownership rule 3: When the owner goes out of scope, the value will be dropped. Consider the preceding case again: let s1 = String::from( hello );let s2 = s1;println!( {}, world! , s1); // Won't compile. The value of s1 has already been dropped. The value of s1 has dropped after s1 is assigned to s2 (in the let s2 = s1 assignment statement). Thus, s1 is no longer valid after this assignment. Here is the memory representation after s1 has been dropped:                    Figure 11: Memory representation after s1 has been dropped.       How ownership moves: There are three ways to transfer ownership from one variable to another in a Rust program:  Assigning the value of one variable to another variable (it was already discussed).  Passing value to a function.  Returning from a function. Passing value to a function: Passing a value to a function has semantics that are similar to assigning a value to a variable. Just like assignment, passing a variable to a function causes it to move or copy. Take a look at this example, which shows both the copy and move use cases: fn main() {  let s = String::from( hello ); // s comes into scope  move_ownership(s);       // s's value moves into the function. . .                   // so it's no longer valid from this 																		// point forward  let x = 5;           // x comes into scope  makes_copy(x);         // x would move into the function                  // It follows copy semantics since it's 																		// primitive, so we use x afterward} // Here, x goes out of scope, then s. But because s's value was moved, nothing // special happens. fn move_ownership(some_string: String) { // some_string comes into scope  println!( {} , some_string);} // Here, some_string goes out of scope and `drop` is called.  // The occupied memory is freed. fn makes_copy(some_integer: i32) { // some_integer comes into scope  println!( {} , some_integer);} // Here, some_integer goes out of scope. Nothing special happens. If we tried to use s after the call to move_ownership, Rust would throw a compile-time error. Returning from a function: Returning values can also transfer ownership. The example below shows a function that returns a value, with annotations identical to those in the previous example. fn main() {  let s1 = gives_ownership();     // gives_ownership moves its return                    // value into s1  let s2 = String::from( hello );   // s2 comes into scope  let s3 = takes_and_gives_back(s2); // s2 is moved into                    // takes_and_gives_back, which also                    // moves its return value into s3} // Here, s3 goes out of scope and is dropped. s2 was moved, so nothing // happens. s1 goes out of scope and is dropped. fn gives_ownership() -&gt; String {       // gives_ownership will move its                       // return value into the function                       // that calls it  let some_string = String::from( yours ); // some_string comes into scope  some_string               // some_string is returned and                       // moves out to the calling                       // function}// This function takes a String and returns itfn takes_and_gives_back(a_string: String) -&gt; String { // a_string comes into                           // scope  a_string // a_string is returned and moves out to the calling function}The ownership of a variable always follows the same pattern: a value is moved when it is assigned to another variable. Unless ownership of the data has been moved to another variable, when a variable that includes data on the heap goes out of scope, the value will be cleaned away by drop. Hopefully, this gives us a basic understanding of what an ownership model is and how it influences the way Rust handles values, such as assigning them to one another and passing them into and out of functions. Hold on. One more thing… Rust’s ownership model, as with all good things, does have certain drawbacks. We quickly realize certain inconveniences once we begin working on Rust. We may have observed that taking ownership and then returning ownership with each function is a little inconvenient.                    Designed by Freepik.       It’s annoying that everything we pass into a function must be returned if we want to use it again, in addition to any other data returned by that function. What if we want a function to use a value without taking ownership of it? Consider the following example. The below code will result in an error because variable, v can no longer be used by the main function (in println!) that initially owned it once the ownership is transferred to the print_vector function. fn main() {  let v = vec![10,20,30];  print_vector(v);  println!( {} , v[0]); // this line gives us an error}fn print_vector(x: Vec&lt;i32&gt;) {  println!( Inside print_vector function {:?} ,x);}Tracking ownership may seem easy enough, but it can get complicated when we start to deal with large and complex programs. So we need a way to transfer values without transferring ownership, which is where the concept of borrowing comes into play. BorrowingBorrowing, in its literal sense, refers to receiving something with the promise of returning it. In the context of Rust, borrowing is a way of accessing value without claiming ownership of it, as it must be returned to its owner at some point.                    Designed by Freepik.       When we borrow a value, we reference its memory address with the &amp; operator. A &amp; is called a reference. The references themselves are nothing special—under the hood, they’re just addresses. For those familiar with C pointers, a reference is a pointer to memory that contains a value that belongs to (aka owned by) another variable. It’s worth noting that a reference can’t be null in Rust. In fact, a reference is a pointer; it’s the most basic type of pointer. There is just one type of pointer in most languages, but Rust has different kinds of pointers, rather than just one. Pointers and their various kinds are a different topic that will be discussed separately. To put it simply, Rust refers to creating a reference to some value as borrowing the value, which must eventually return to its owner. Let’s look at a simple example below: let x = 5;let y = &amp;x;println!( Value y={} , y);println!( Address of y={:p} , y);println!( Deref of y={} , *y);The above produces the following output: Value y=5Address of y=0x7fff6c0f131cDeref of y=5Here, the y variable borrows the number owned by variable x, while x still owns the value. We call y a reference to x. The borrow ends when y goes out of scope, and because y does not own the value, it is not destroyed. To borrow a value, take a reference by the &amp; operator. The p formatting, {:p} output as a memory location presented as hexadecimal.  Dereference: In the above code, “” (i. e. , an asterisk) is a *dereference operator that operates on a reference variable. This dereferencing operator allows us to get the value stored in the memory address of a pointer. Let’s look at how a function can use a value without taking ownership through borrowing: fn main() {  let v = vec![10,20,30];  print_vector(&amp;v);  println!( {} , v[0]); // can access v here as references can't move the value}fn print_vector(x: &amp;Vec&lt;i32&gt;) {  println!( Inside print_vector function {:?} , x);}We are passing a reference (&amp;v) (aka pass-by-reference) to the print_vector function rather than transferring the ownership (i. e. , pass-by-value). As a result, after calling the print_vector function in the main function, we can access v. Following the pointer to the value with the dereference operator: As stated previously, a reference is a kind of pointer, and a pointer may be thought of as an arrow pointing to a value stored elsewhere. Consider the below example: let x = 5;let y = &amp;x;assert_eq!(5, x);assert_eq!(5, *y);In the above code, we create a reference to an i32 type value and then use the dereference operator to follow the reference to the data. The variable x holds an i32 type value, 5. We set y equal to a reference to x. This is how the stack memory appears:                    Stack memory representation.       We can assert that x is equal to 5. However, if we want to make an assertion on the value in y, we must follow the reference to the value it’s referring to using *y (hence dereference here). Once we dereference y, we have access to the integer value that y is pointing to, which we can compare to 5. If we tried to write assert_eq!(5, y); instead, we would get this compilation error: error[E0277]: can't compare `{integer}` with `&amp;{integer}` --&gt; src/main. rs:11:5  |11 |   assert_eq!(5, y);  |   ^^^^^^^^^^^^^^^^ no implementation for `{integer} == &amp;{integer}`Because they’re different types, comparing a number and a reference to a number isn’t permitted. Hence, we must use the dereference operator to follow the reference to the value it’s pointing to. References are immutable by default: Like variable, a reference is immutable by default—it can be made mutable with mut, but only if its owner is also mutable: let mut x = 5;let y = &amp;mut x; Immutable references are also known as shared references, whereas mutable references are also known as exclusive references. Consider the below case. We’re granting read-only access to references since we’re using the &amp; operator instead of &amp;mut. Even if the source n is mutable, ref_to_n, and another_ref_to_n are not, as they are read-only n borrows. let mut n = 10;let ref_to_n = &amp;n;let another_ref_to_n = &amp;n;Borrow checker will give the below error: error[E0596]: cannot borrow `x` as mutable, as it is not declared as mutable --&gt; src/main. rs:4:9 |3 | let x = 5; |   - help: consider changing this to be mutable: `mut x`4 | let y = &amp;mut x; |     ^^^^^^ cannot borrow as mutableBorrowing rules: One could question why a borrowing would not always be preferred over a move. If that’s the case, why does Rust even have move semantic, and why doesn’t it borrow by default? The reason is that borrowing a value in Rust is not always possible. Borrowing is only permitted in certain cases. Borrowing has its own set of rules, which the borrow checker strictly enforces during compile time. These rules were put in place to prevent data races. They are as follows:  The scope of the borrower cannot outlast the scope of the original owner.  There can be multiple immutable references, but only one mutable reference.  Owners can have immutable or mutable references, but not both at the same time.  All references must be valid (can’t be null). Reference must not outlive the owner: A reference’s scope must be contained within the scope of the owner of the value. Otherwise, the reference may refer to a freed value, resulting in a use-after-free error. let x;{   let y = 0;  x = &amp;y;}println!( {} , x);The above program tries to dereference x after the owner y goes out of scope. Rust prevents this use-after-free error. Many immutable references, but only one mutable reference allowed: We can have as many immutable references (aka shared references) to a particular piece of data at a time, but only one mutable reference (aka exclusive reference) allowed at a moment. This rules exists to eliminate data races. When two references point to the same memory location at the same time, at least one of them is writing, and their actions are not synchronized, this is known as a data race. We may have as many immutable references as we like because they don’t change the data. Borrowing, on the other hand, restricts us to just keeping one mutable reference (&amp;mut) at a time to prevent the possibility of data races at compile time. Let’s look at this one: fn main() {  let mut s = String::from( hello );  let r1 = &amp;mut s;  let r2 = &amp;mut s;  println!( {}, {} , r1, r2);}The above code that attempts to create two mutable references (r1 and r2) to s will fail: error[E0499]: cannot borrow `s` as mutable more than once at a time --&gt; src/main. rs:6:14 |5 |   let r1 = &amp;mut s; |       ------ first mutable borrow occurs here6 |   let r2 = &amp;mut s; |       ^^^^^^ second mutable borrow occurs here7 | 8 |   println!( {}, {} , r1, r2); |            -- first borrow later used hereClosing remarksHopefully, this clarifies the concepts of ownership and borrowing. I also briefly touched on borrow checker, the backbone of ownership and borrowing. As I mentioned at the beginning, ownership is a novel idea that might be difficult to comprehend at first, even for seasoned developers, but gets easier and easier the more you work on it. This is just a rundown of how memory safety is enforced in Rust. I attempted to make this post as easy to understand as possible while yet providing enough information to grasp the concepts. For more details on Rust’s ownership feature, check out their online documentation. Rust is a great choice when performance matters and it solves pain points that bother many other languages, resulting in a significant step forward with a steep learning curve. For the sixth year in a row, Rust has been Stack Overflow’s most loved language, implying that many people who have had the chance to use it have fallen in love with it. The Rust community continues to grow. According to Rust Survey 2021 Results: The year 2021 was undoubtedly one of the most momentous in Rust’s history. It saw the founding of the Rust Foundation, the 2021 edition, and a larger community than ever before. Rust appears to be on a strong road as we head into the future. Happy learning!                Designed by Freepik.    "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'><small><span class='body'>No results found!</span></li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-primary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><small><span class='body'>"+ body +"</span><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'><small><span class='body'>No results found!</span></li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});