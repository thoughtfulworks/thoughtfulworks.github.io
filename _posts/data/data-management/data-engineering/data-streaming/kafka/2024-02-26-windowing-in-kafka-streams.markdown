---
layout: post
title:  "Windowing in Kafka Streams"
kicker: "Kafka Streams"
summary: "Windowing refers to the process of dividing a continuous stream of data into discrete segments or windows, based on time. These windows then serve as the basis for applying computational operations, such as aggregations or transformations, on the data contained within them."
image: assets/images/posts-cover-images/windowing-in-kafka-streams.webp
image-credits: "Image generated by <b>DALL-E</b>."
imageshadow: true
author: senthil
date: 2024-02-26 00:00:01 +0530
tags: ["kafka-streams", "kafka", "windowing"]
categories: kafka-streams
featured: true
hidden: false
toc: true
---

# Overview

Windowing in Kafka Streams is a powerful feature that allows us to manage and analyze data that enters a Kafka cluster in a time-sequenced order. It enables the grouping of records based on **time intervals**, or **windows**, which makes temporal operations on data streams easier. The term "temporal operations" refers to operations that are based on time. Having said that, windowing refers to the process of dividing a continuous stream of data into discrete segments or "windows", based on time. This feature is particularly useful for applications that require real-time analytics, aggregations, or other time-based processing on continuous streams of data. 

By segmenting the data temporally, windowing allows for more granular and time-sensitive analysis of streaming data. For example, calculating the average number of website visits per minute, detecting patterns of activity within a certain time frame, or summarizing financial transactions on an hourly basis.


## Types of windows supported in Kafka streams

Kafka Streams supports various types of windowing operations to group events by time boundaries, allowing for time-sensitive processing of streaming data:

- **Tumbling windows**
- **Hopping windows**
- **Session windows**
- **Sliding windows**

Before we go further into explaining all these types of windows, let's understand some of the core concepts:

## Window size 

The duration of each window. All events within this period are aggregated together.

## Advance interval (Hop size) 

The amount by which each window advances. If the advance interval is less than the window size, the windows will overlap.

## Grace period

The "grace period" refers to a configurable time window that allows for late-arriving records to be included in windowed aggregations. In other words, the grace period specifies how much time after a window has technically expired (based on its defined duration) the system will continue to accept records for that window. This feature is particularly useful for dealing with out-of-order data, which is common in distributed systems due to network delays or other factors. By setting a grace period, you can accommodate these late-arriving records to ensure they are processed and included in the correct aggregation window, rather than being discarded or processed as part of a later window.

For example, if we have a 1-minute tumbling window with a grace period of 30 seconds, records that arrive up to 30 seconds after the end of each 1-minute window can still be included in the corresponding windowed computation. After the grace period has elapsed, the window is closed, and no further records for that window are processed.

The grace period is set via the `Materialized` or Windows configuration when defining windowed operations in Kafka Streams. It's a crucial setting for ensuring data completeness and accuracy in real-time streaming applications, especially when handling data that may not arrive in perfect chronological order.

```java
// Tumbling window
countStream.groupByKey()
	.windowedBy(TimeWindows.ofSizeAndGrace(Duration.ofMinutes(1)
		,Duration.ofSeconds(30)))  // Using 30 seconds for the grace period
	.count(Materialized.as("Tumbling-window-counting-store"))

// Session window
countStream.groupByKey()
	.windowedBy(SessionWindows.ofInactivityGapAndGrace(Duration.ofMinutes(1),
		Duration.ofSeconds(30)))  // Using 30 seconds for the grace period
	.count(Materialized.as("Session-window-counting-store"))
```

---

# Tumbling windows

Fixed-size, non-overlapping intervals of time. They are useful for cases where we want to perform an operation on non-overlapping segments of data.

**Use case**: A good use case for tumbling windows is inventory tracking because we only want the unique number of items sold per window.

## Example

Let's say we want to determine the average temperature every *ten minutes* from a stream of temperature signals from a sensor. To guarantee that every temperature reading is contained in *a single 10-minute window*, tumbling windows are used in this situation. Averages are then computed for these non-overlapping periods.

Let's say the sensor sends readings at minutes: **`1, 3, 7, 11, 15, 18, 22, 25`**. Using 10-minute tumbling windows, the readings would be grouped and averaged as follows:

- **Window 1** (0-10 minutes): Includes readings at minutes **`1, 3, 7`**. The average of these readings is calculated.
- **Window 2** (10-20 minutes): Includes readings at minutes **`11, 15, 18`**. The average of these readings is calculated.
- **Window 3** (20-30 minutes): Includes readings at minutes **`22, 25`**. The average of these readings is calculated.

Each reading is accounted for in exactly one window, with no overlap between windows.

## Code example

Here's how we might set up a tumbling window in Kafka Streams for aggregating temperature readings:

```java
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.*;

import java.time.Duration;

public class TumblingWindowExample {
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, Double> temperatureReadings = builder.stream("temperature-sensor");

        TimeWindows tumblingWindows = TimeWindows.of(Duration.ofMinutes(10)); // Define a 10-minute tumbling window

        KTable<Windowed<String>, Double> averageTemperatures = temperatureReadings
            .groupByKey() // Assuming the key is the sensor ID
            .windowedBy(tumblingWindows)
            .aggregate(
                () -> 0.0, // Initializer
                (aggKey, newValue, aggValue) -> (aggValue + newValue) / 2, // Aggregator
                Materialized.as("average-temperatures-store") // State store name
            );

        // Further processing or output the results to a topic
    }
}
```

# Hopping windows

Fixed-size, overlapping intervals of time. They allow for operations over windows that move forward by a smaller step than the window size, providing overlapping data aggregates. Literally, "hop" means "lightly jump." This particular window is called hopping because each window moves forward by a fixed period, which is less than or equal to the size of the window itself. A hopping window is defined by a window size and the size of the time block at which it advances. This results in windows that overlap with each other, providing a way to analyze data points that are close in time across multiple windows. 

## Example

Imagine we are processing a stream of page views with a hopping window. We want to count the number of page views in windows of 5 minutes, but we want to update the count every minute. In this scenario:

- **Window Size**: 5 minutes
- **Advance Interval (Hop Size)**: 1 minute

This configuration means that every minute, a new window starts, but each window covers data from the past 5 minutes. Therefore, at any given time, there are multiple overlapping windows active.

Let's say our events are timestamped as follows (in minutes): **`1, 2, 3, 4, 5, 6, 7, 8, 9, 10`**.

- **Window 1** covers minutes **`1-5`** and sees events at **`1, 2, 3, 4, 5`**.
- **Window 2** (starting at minute 2) covers **`2-6`** and sees events at **`2, 3, 4, 5, 6`**.
- ...
- **Window 6** (starting at minute 6) covers **`6-10`** and sees events at **`6, 7, 8, 9, 10`**.

Notice how each window overlaps with the next, sharing four minutes of data with it.

## Code example

Here’s how we might define a hopping window in Kafka Streams for the above scenario:

```java
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.*;

import java.time.Duration;

public class HoppingWindowExample {
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> pageViews = builder.stream("page-views");

        TimeWindows hoppingWindows = TimeWindows.of(Duration.ofMinutes(5)) // Window size
                                                 .advanceBy(Duration.ofMinutes(1)); // Advance interval

        KTable<Windowed<String>, Long> pageViewCounts = pageViews
            .groupBy((key, value) -> value) // Group by page view event value (or some logic)
            .windowedBy(hoppingWindows)
            .count(Materialized.as("page-view-counts-store")); // Store name for the state store

        // Further processing...
    }
}
```

In this example, `pageViews` is a stream of page view events, possibly with a user ID as the key and some page identifier as the value. The events are grouped by page (or some other logic) and then counted within each window defined by `hoppingWindows`. The count operation will create a new `KTable` holding the counts of page views for each 5-minute window, updated every minute.

Hopping windows are particularly useful for analyses where we want more frequent updates but also need to consider events over a longer period. They provide a balance between real-time and batch processing, making them suitable for a wide range of time-sensitive aggregation tasks.

# Session windows

Dynamically sized windows based on periods of activity. Session windows do not overlap. In other words, its size is *not based on time but on behavior instead*. It capture periods of activity separated by gaps of inactivity, making them particularly suited for scenarios where activity occurs in bursts.

Session windows define a period of inactivity, and they expand in size as new events occur within that period. Unlike tumbling or hopping windows, which are based on fixed time intervals, session windows dynamically adjust their size based on the presence or absence of events. This capability allows for more flexible and meaningful aggregation of events that naturally occur in sessions or bursts[^1].

## Dynamic Duration 

The size of a session window is determined by the timestamps of the events it contains. Imagine we start watching events (like messages or transactions) coming in one after another. A session window keeps track of these events starting from the first one we see. Now, as long as events keep coming without taking too long of a break between them, they're all considered part of the same session. But if there's a pause — meaning no events come in for a while longer than a specific time we've decided on in advance — then we say the current session window is over. 

Once a new event comes in after that break, it starts a new session window. This way, each session window groups together a series of events that are closely related in time, with breaks indicating the end of one group and the start of another.

## Inactivity gap

This is a configurable duration that defines how much time must pass without any events before a new session window is started, i.e., before new messages start arraying. If a subsequent event arrives within the inactivity gap, it is included in the current session window, potentially extending its duration.

## No overlap for a given key

The key is a crucial concept in session windows. It's what Kafka Streams uses to group events into sessions. Each event in a Kafka stream has a key-value pair, and the key is used to identify which events are related. When processing streams, Kafka Streams uses these keys to organize data into windows.

For session windows, events with the same key are considered part of the same user or entity's activity, allowing the system to create windows that accurately reflect periods of activity for each unique entity represented by the key.

Each key in our data represents a unique identifier for a group of related events. For example, if we're processing user activity events, the key might be the User ID. Kafka Streams ensures that for any given key, session windows do not overlap. This means that each session window for a specific key is distinct and separated by a period of inactivity that exceeds the defined gap. There won't be two session windows for the same key that cover overlapping time periods.

## Vary in length

Since session windows are defined by periods of activity (events coming in) and inactivity (no events for a specified gap), they can vary in length. The duration of a session window is determined by how long the activity lasts before a period of inactivity that's long enough to end the session. This means session windows can be short for brief bursts of activity or long for extended periods of activity, and this length can differ significantly from one session to another, even for the same key.

## Example

Consider an online gaming platform where players log in, perform various activities (play games, chat, etc.), and then log out. We want to analyze the duration and frequency of individual gaming sessions.

Let's say we have the following events (login actions) for two players over time:

- **Player A**: Events at **`10:00`**, **`10:05`**, and **`10:20`**.
- **Player B**: Events at **`10:30`** and **`11:15`**.

Assuming an inactivity gap of 10 minutes:

- **Player A** would have a single session window from **`10:00`** to **`10:20`** because the events are within 10 minutes of each other.
- **Player B** would have two separate session windows (**`10:30`** to **`10:30`** and **`11:15`** to **`11:15`**) because the gap between events exceeds the 10-minute inactivity gap.

## Code example

```java
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.*;

import java.time.Duration;

public class SessionWindowExample {
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, String> playerActions = builder.stream("player-actions");

        Duration inactivityGap = Duration.ofMinutes(10); // Define inactivity gap for session windows

        KTable<Windowed<String>, Long> sessionCounts = playerActions
            .groupByKey() // Assuming the key is the player ID
            .windowedBy(SessionWindows.with(inactivityGap)) // Apply session windowing
            .count(Materialized.as("session-counts-store")); // Count events in each session

        // Further processing or output the results to a topic
    }
}
```

- **`playerActions`** is a **`KStream`** representing player actions, keyed by player ID.
- **`groupByKey`** groups the actions by player ID, preparing them for windowed aggregation.
- **`windowedBy(SessionWindows.with(inactivityGap))`** applies session windowing based on the specified inactivity gap.
- The **`count`** method aggregates the number of actions within each session window for each player.

Session windows are particularly useful for analyzing user behavior, session-based analytics, and any scenario where activity patterns are bursty or discontinuous. By dynamically adjusting window sizes based on actual activity, session windows offer a detailed view of event patterns compared to fixed-size windows.

# Sliding windows

Sliding windows are defined by a fixed duration and a sliding interval. Like the session window, they can continue to grow in size because they are based on behavior. In other words, they allow for the aggregation of events within a specific time frame that "slides" over the data stream as time progresses. 

Imagine a river is flowing by us, and we want to examine the water quality, but instead of checking the entire river at once, we decide to look at just a portion of the water that flows by us within a specific time frame. Let's say we choose to examine the water every 5 minutes, but instead of starting a new check every 5 minutes on the dot, we start a new check every minute, each time including the water from the last 5 minutes.

In the context of Kafka Streams and sliding windows, the "river" is our stream of data (events), and the "water quality check" is the aggregation or analysis we want to perform on that data. A sliding window is like the moving 5-minute period we use to check the water quality. As time moves forward, this 5-minute window "slides" along with the flow of the river (or the flow of our data). This means we're constantly updating our analysis or aggregation based on the most recent 5 minutes of data, starting a new analysis every minute (or whatever our chosen interval is).

Put simply, we're continuously summarizing or analyzing the most recent chunk of data, and this chunk moves forward over time, constantly capturing a fresh set of data based on the window's duration and the sliding interval we've defined. This approach allows us to get a continuous, up-to-date view of what's happening in our data stream, making it very useful for real-time monitoring and analysis.

## Example

Consider a scenario where we're analyzing temperature readings from a sensor, and we want to calculate the average temperature over the last 5 minutes for every reading. This means for each temperature reading, we look back 5 minutes from its timestamp and include all readings in that interval for our calculation.

Let's say we have temperature readings timestamped as follows (in minutes): **`1, 2, 3, 4, 5, 6, 7, 8, 9, 10`**.

Using a 5-minute sliding window, for the reading at minute **`6`**, we would include readings from minutes **`2, 3, 4, 5, 6`** in the average calculation. For the reading at minute **`7`**, the window slides to include readings from **`3, 4, 5, 6, 7`**, and so on.

## Code example

Here's how we might set up a sliding window in Kafka Streams for calculating average temperatures:

```java
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.*;

import java.time.Duration;

public class SlidingWindowExample {
    public static void main(String[] args) {
        StreamsBuilder builder = new StreamsBuilder();
        KStream<String, Double> temperatureReadings = builder.stream("temperature-sensor");

        // Define a sliding window of 5 minutes
        SlidingWindows slidingWindows = SlidingWindows.ofTimeDifferenceWithNoGrace(Duration.ofMinutes(5));

        KTable<Windowed<String>, Double> averageTemperatures = temperatureReadings
            .groupByKey() // Assuming the key is the sensor ID
            .windowedBy(slidingWindows)
            .aggregate(
                () -> 0.0, // Initializer for the aggregation
                (key, newValue, aggValue) -> aggValue + newValue, // Adder
                (key, leftValue, rightValue) -> leftValue + rightValue, // Subtracter for out-of-window values, if needed
                Materialized.<String, Double, WindowStore<Bytes, byte[]>>as("average-temperature-store")
                    .withValueSerde(Serdes.Double()) // Specify the SerDe explicitly for the aggregated value
            )
            .mapValues(total -> total / 5); // Calculate the average

        // Further processing or output the results to a topic
    }
}
```

- **`temperatureReadings`** is a **`KStream`** representing the stream of temperature readings, with each reading possibly keyed by the sensor ID.
- The **`groupByKey`** method groups the readings by sensor ID, preparing them for aggregation.
- **`windowedBy(slidingWindows)`** applies the sliding window definition to the grouped stream.
- The **`aggregate`** method computes the sum of temperatures in each window, and then we calculate the average by dividing by the count of readings (simplified here for demonstration; in practice, we'd dynamically count readings within each window).

Sliding windows are ideal for continuous calculations that need to be updated with each new event, providing a more granular and immediate view of data trends over time.

---

[^1]: **Bursts**: In the context of Kafka Streams, the term "bursts" typically refers to sudden increases in the volume of data messages being processed by the stream. These bursts can occur due to various reasons, such as a rapid flood of events from producers, periodic increases in activity (e.g., during specific hours of the day or days of the week), or backlog processing after downtime or maintenance periods.